{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":""},{"location":"#llm-hands-on-workshop","title":"LLM Hands on Workshop","text":""},{"location":"#welcome-to-the-material-for-the-llm-hands-on-workshop","title":"Welcome to the material for the LLM Hands on Workshop!","text":"<p>There is a wealth of material online about how to use and work with LLMs. So much so that it can hard to know where to start, and what path to take to get from beginner to proficient. Therefore, we have created this workshop by combining the best bits of many different resources, and focusing on practical skills that you can use to work with LLMs.</p>"},{"location":"#a-focus-on-retrieval-augmented-generation","title":"A focus on retrieval augmented generation","text":"<p>This workshop places the focus on retrieval augmented generation (RAG). The general RAG process looks something like the below:</p> <pre><code>flowchart RL\n    database[(Database)]\n    document&gt;Document]\n    index{index}\n    human((Human))\n    llm((LLM))\n    combined[Prompt template\n    + query \n    + context]\n    database --&gt; index\n    document --&gt; index\n    human -- \"Query\" --&gt; index\n    index --&gt; combined\n    combined --&gt; llm\n    llm -- \"Response\" --&gt; human</code></pre> <p>We are used to interacting with user interfaces like ChatGPT, and not always aware of the infrastructure underneath. In short, RAG involves taking some data from a document or database and creating an index - a database that allows us to quickly search over chunks of our documents. When a user sends input to the model interface, this database is queried and the relevant search results are returned, similar to when you Google something, you get a list of potential websites to check. These relevant chunks are then sent to a LLM, along with your original input, and the LLM generates a response.</p> <p>Sounds easy?</p> <p>Well yes - and no! While it is fairly easy to get up and running with RAG, it is actually quite hard to get it working correctly and consistently.</p> <p>We try to focus on a framework agnostic approach, so that you can use models from any provider, or run local models if you like. Instead of relying on popular but complex frameworks like LangChain or LlamaIndex, we've chosen to focus on fundamental tools:</p> <ul> <li>Direct API interactions using OpenAI</li> <li>Data validation using Pydantic</li> <li>Jinja templates for prompt engineering</li> <li>ChromaDB for indexing</li> <li>Streamlit for user interfaces</li> </ul> <p>By building from the ground up, this workshop aims to provide researchers and practitioners with:</p> <ul> <li>A deep understanding of core LLM concepts and workflows</li> <li>The ability to create custom, flexible solutions</li> <li>Enhanced debugging and problem-solving skills</li> <li>A solid foundation for evaluating and using higher-level frameworks in the future, if desired</li> </ul> <p>While frameworks certainly have their place in rapid prototyping and production environments, we believe that starting with the basics offers long-term benefits, and transferrable skills that apply across different models and providers.</p> <p>This workshop doesn't aim to dismiss the value of existing frameworks. Instead, it empowers you with the knowledge to better understand, evaluate, and leverage these tools when appropriate, while also giving you the confidence to build custom solutions when needed. By the end of this workshop, you'll have a robust, foundational understanding of LLMs that will serve you well in research, development, and practical applications, positioning you to drive innovation in this exciting field.</p>"},{"location":"#how-to-use-this-workshop","title":"How to use this workshop","text":"<p>This site is pretty simple, start with Setting up and then continue through the sections in order. We start with some basic API calls, then introduce more features until we have a fully working chat application.</p> <p>The main outcome of this workshop is to have you conversing with an LLM about your own research or data.</p> <p>You can work through the workshop in order... or you can just pick a section that interests you.</p>"},{"location":"2_open_ai/","title":"Introduction to the OpenAI API","text":"In\u00a0[3]: Copied! <pre>from openai import OpenAI\nimport dotenv\nimport os\nfrom rich import print as rprint # for making fancy outputs\n\ndotenv.load_dotenv()\n\nclient = OpenAI()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> from openai import OpenAI import dotenv import os from rich import print as rprint # for making fancy outputs  dotenv.load_dotenv()  client = OpenAI()  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") <p>Calling a model is simple</p> In\u00a0[87]: Copied! <pre>system_prompt = \"You are Matsuo Basho, the great Japanese haiku poet.\"\nuser_query = \"Can you give me a haiku about a Samurai cat.\"\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=128\n)\n\nprint(response.choices[0].message.content)\n</pre> system_prompt = \"You are Matsuo Basho, the great Japanese haiku poet.\" user_query = \"Can you give me a haiku about a Samurai cat.\"  response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=128 )  print(response.choices[0].message.content) <pre>Silent paws tread soft,  \nMoonlight gleams on sharpened claws\u2014  \nHonor in each pounce.  \n</pre> <p>Purrfect.</p> <p>The API offers a number of endpoints that allow you to interact with the models. The main one that we will cover here is the <code>/chat/completions</code> endpoint. This endpoint allows you to interact with the model in a conversational manner.</p> <p>Only 2 arguments are actually required for this endpoint:</p> <ul> <li><p><code>model: str</code> The model to use. For OpenAI, this includes:</p> <ul> <li><p><code>'gpt-3.5-turbo'</code></p> </li> <li><p><code>'gpt-4'</code></p> </li> <li><p><code>'gpt-4o'</code></p> </li> <li><p><code>'gpt-4o-mini'</code></p> </li> <li><p>Any fine-tuned versions of these models.</p> </li> <li><p>Many specific versions of the above models.</p> </li> </ul> </li> <li><p><code>messages: list</code> A list of messages that the model should use to generate a response. Each entry in the list of messages comes in the form:</p> </li> </ul> <pre>{\"role\": \"&lt;role&gt;\", \"content\": \"&lt;content&gt;\", \"name\": \"&lt;name&gt;\"}\n</pre> <p>Where <code>&lt;role&gt;</code> can take one of the following forms:</p> <ul> <li><code>'system'</code> This is a system level prompt, designed to guide the conversation. For example:</li> </ul> <p>\"You are a customer service bot.\"</p> <ul> <li><code>'user'</code> This is direct input from the user. For example:</li> </ul> <p>\"How do I reset my password?\"</p> <ul> <li><code>'assistant'</code> This is the response from the model. For example:</li> </ul> <p>\"To reset your password, please visit our website and click on the 'Forgot Password' link.\"</p> <p>So all of this fed into one message list would look like this:</p> <pre>messages = [\n    {\"role\": \"system\", \"content\": \"You are a customer service bot.\"},\n    {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n    {\"role\": \"assistant\", \"content\": \"To reset your password, please visit our website and click on the 'Forgot Password' link.\"}\n]\n</pre> <p>Here we have used model <code>gpt-4o-mini</code>, but there are a range of models available.</p> In\u00a0[88]: Copied! <pre>for model in client.models.list():\n    print(model)\n</pre> for model in client.models.list():     print(model) <pre>Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\nModel(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\nModel(id='dall-e-2', created=1698798177, object='model', owned_by='system')\nModel(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\nModel(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\nModel(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\nModel(id='dall-e-3', created=1698785189, object='model', owned_by='system')\nModel(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\nModel(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\nModel(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\nModel(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\nModel(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\nModel(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\nModel(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\nModel(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\nModel(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\nModel(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\nModel(id='gpt-4o', created=1715367049, object='model', owned_by='system')\nModel(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\nModel(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\nModel(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\nModel(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\nModel(id='gpt-4', created=1687882411, object='model', owned_by='openai')\nModel(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\nModel(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\nModel(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\nModel(id='babbage-002', created=1692634615, object='model', owned_by='system')\nModel(id='davinci-002', created=1692634301, object='model', owned_by='system')\nModel(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\nModel(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\nModel(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n</pre> <p>As of writing <code>gpt-4o-2024-08-06</code> is the current best offering. But we'll stick with <code>gpt-4o-mini</code>, because it is cheaper and still highly capable.</p> <p>What is the <code>response</code> object?</p> In\u00a0[65]: Copied! <pre>rprint(response)\n</pre> rprint(response) <pre>ChatCompletion(\n    id='chatcmpl-A5yRAL35y156KgXeY6uUBbLNHD4qh',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content='Silent pawsteps glide,  \\nMoonlight dances on the blade\u2014  \\nFeline honor blinds.  ',\n                refusal=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=None\n            )\n        )\n    ],\n    created=1725987324,\n    model='gpt-4o-mini-2024-07-18',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint='fp_483d39d857',\n    usage=CompletionUsage(completion_tokens=20, prompt_tokens=37, total_tokens=57)\n)\n</pre> <p>There is some useful stuff in here, apart from the <code>content</code> property, such as the token usage. You might notice some other things too, like <code>function_call</code> and <code>tool_calls</code>. These are specific to OpenAI models, and not every model supports function calling or tools, so we won't cover them. We can achieve many of the same effects without them anyway.</p> In\u00a0[9]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=128,\n  stream=True\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=128,   stream=True )  for chunk in response:     if chunk.choices[0].delta.content is not None:         print(chunk.choices[0].delta.content, end=\"\") <pre>Silent paws in dusk,  \nMoonlit blade gleams in the night\u2014  \nFierce heart, whiskers twitch.</pre> <p>All this really does is create a streaming object, which acts like a generator. We can then print the chunk as it comes in.</p> <p>Here is the caption from this figure:</p> <p>Fig. 2 Spatial and temporal self-similarity and correlation in switching activity.</p> <p>(A) Percolating devices produce complex patterns of switching events that are self-similar in nature. The top panel contains 2400 s of data, with the bottom panels showing segments of the data with 10, 100, and 1000 times greater temporal magnification and with 3, 9, and 27 times greater magnification on the vertical scale (units of G0 = 2e2/h, the quantum of conductance, are used for convenience). The activity patterns appear qualitatively similar on multiple different time scales. (B and E) The probability density function (PDF) for changes in total network conductance, P(\u0394G), resulting from switching activity exhibits heavy-tailed probability distributions. (C and F) IEIs follow power law distributions, suggestive of correlations between events. (D and G) Further evidence of temporal correlation between events is given by the autocorrelation function (ACF) of the switching activity (red), which decays as a power law over several decades. When the IEI sequence is shuffled (gray), the correlations between events are destroyed, resulting in a significant increase in slope in the ACF. The data shown in (B) to (D) (sample I) were obtained with our standard (slow) sampling rate, and the data shown in (E) to (G) (sample II) were measured 1000 times faster (see Materials and Methods), providing further evidence for self-similarity.</p> <p>This figure is taken from Avalanches and criticality in self-organized nanoscale networks, Mallinson et al., 2019.</p> <p>Now let's use the OpenAI vision model to generate a caption for this figure.</p> In\u00a0[92]: Copied! <pre>prompt = (\n    \"This figure is a caption from a paper entitled Avalanches and criticality in self-organized nanoscale networks. \"\n    \"Please provide a caption for this figure. \"\n    \"You should describe the figure, grouping the panels where appropriate. \"\n    \"Feel free to make any inferences you need to.\"\n\n)\n</pre> prompt = (     \"This figure is a caption from a paper entitled Avalanches and criticality in self-organized nanoscale networks. \"     \"Please provide a caption for this figure. \"     \"You should describe the figure, grouping the panels where appropriate. \"     \"Feel free to make any inferences you need to.\"  ) <p>The process of calling a vision model is a little more involved, but OpenAI have a convenient tutorial on how to do this.</p> <p>Essentially we need to first convert the image to a base64 string. We can then pass this to the OpenAI API.</p> In\u00a0[95]: Copied! <pre>import base64\nimport requests\n\n# Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Path to your image\nimage_path = \"imgs/figure.jpeg\"\n\n\ndef get_image_caption(image_path, prompt):\n  # Getting the base64 string\n  base64_image = encode_image(image_path)\n\n  headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n  }\n\n  payload = {\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": prompt\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 512\n  }\n\n  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\n  return response.json()['choices'][0]['message']['content']\n</pre> import base64 import requests  # Function to encode the image def encode_image(image_path):   with open(image_path, \"rb\") as image_file:     return base64.b64encode(image_file.read()).decode('utf-8')  # Path to your image image_path = \"imgs/figure.jpeg\"   def get_image_caption(image_path, prompt):   # Getting the base64 string   base64_image = encode_image(image_path)    headers = {     \"Content-Type\": \"application/json\",     \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"   }    payload = {     \"model\": \"gpt-4o-mini\",     \"messages\": [       {         \"role\": \"user\",         \"content\": [           {             \"type\": \"text\",             \"text\": prompt           },           {             \"type\": \"image_url\",             \"image_url\": {               \"url\": f\"data:image/jpeg;base64,{base64_image}\"             }           }         ]       }     ],     \"max_tokens\": 512   }    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)    return response.json()['choices'][0]['message']['content'] In\u00a0[96]: Copied! <pre>caption = get_image_caption(image_path, prompt)\nprint(caption)\n</pre> caption = get_image_caption(image_path, prompt) print(caption) <pre>**Figure Caption:**\n\n**Figure X:** Analysis of avalanche dynamics and criticality in self-organized nanoscale networks. \n\n**(A)** Time series data showing the fluctuations in conductance, \\(\\Delta G(G_0)\\), over varying observation periods (100 s, 10 s, 1 s, 0.1 s). The four panels illustrate distinct behavior and amplitude of fluctuations as time scales decrease. \n\n**(B) and (E)** Power law distributions, \\(P(\\Delta G)\\), of the amplitude of conductance fluctuations are presented on logarithmic scales, revealing power-law exponents \\(\\Delta G \\approx -2.59\\) (B) and \\(\\Delta G \\approx -2.36\\) (E). \n\n**(C) and (F)** Temporal distributions, \\(P(t)\\), showing the frequency of events over time. The corresponding power law exponents are \\(t \\approx -1.39\\) (C) and \\(t \\approx -1.30\\) (F), indicating scale-invariant behavior.\n\n**(D) and (G)** Characteristic avalanche sizes \\(A(t)\\) as a function of time, indicating distinct scaling regimes with exponents \\(t \\approx -0.19\\) and \\(t \\approx -0.23\\) for the upper and lower panels, respectively. The gray data in (D) suggests a crossover behavior in larger time scales. \n\nThese results highlight the critical dynamics and self-organized behavior of the nanoscale networks under study.\n</pre> <p>I mean, I don't know about you, but I think that's incredible. Let's consider what it has done:</p> <ul> <li>Correctly grouped the panels in the same way the real caption did.</li> <li>Provided information on the observation periods.</li> <li>Drawn out the important information, such as critical exponents.</li> <li>Made the link between power law distributions and scale-free behaviour.</li> </ul> <p>However, it has failed to provide information on temporal correlations, and it has not noticed the self-similarity in caption 1.</p> <p>But this is still quite impressive, and with more information we could potentially get some better captions.</p>"},{"location":"2_open_ai/#introduction-to-the-openai-api","title":"Introduction to the OpenAI API\u00b6","text":"<p>In this section we will cover the basics of using the OpenAI API, including:</p> <ul> <li>Chat Completions</li> <li>Streaming</li> <li>Vision input</li> </ul> <p>The beauty of the OpenAI API is that is very simple to use.</p> <p>In your environment you should have a file called <code>.env</code> with the following:</p> <pre>OPENAI_API_KEY=\"sk-proj-1234567890\"\n</pre> <p>We will give you this key in the workshop. The key will be deactivated after the workshop!</p> <p>You can then grab the key using python:</p>"},{"location":"2_open_ai/#chat-completions","title":"Chat Completions\u00b6","text":""},{"location":"2_open_ai/#additional-arguments","title":"Additional arguments\u00b6","text":"<p>The <code>/chat/completions</code> endpoint also accepts a number of additional arguments that can be used to alter the response. These include (arguments are listed with their default values if applicable):</p> <ul> <li><p><code>max_tokens: int</code> The maximum number of tokens to generate in the response. Important to stop the model from generating too much text and racking up a huge bill.</p> </li> <li><p><code>n: int = 1</code> The number of completions to generate. This is useful when you want to generate multiple completions and select the best one. You'll be charged for the total number of tokens generated across all completions, so be careful with setting this too high.</p> </li> <li><p><code>temperature: float = 1.0</code> The temperature of the model, ranging from 0.0 to 2. Use low values for deterministic responses, and high values for more creative responses.</p> </li> <li><p><code>top_p: float = 1.0</code> The probability of sampling from the top <code>p</code> tokens. This is useful for controlling the diversity of the responses. Setting this to a higher values means the model is more likely to sample from a wider range of tokens.</p> </li> <li><p><code>logprobs: bool = False</code> Whether to return the log probabilities of the tokens generated. This is useful when you want to understand how the model is making decisions.</p> </li> <li><p><code>logit_bias: dict</code> A dictionary of logit biases to apply to the tokens. This is useful when you want to guide the model towards generating certain types of responses.</p> </li> <li><p><code>response_format: str</code> The format of the response. We will cover this later...</p> </li> <li><p><code>stream: bool = False</code> Whether to stream the response back to the client. This is useful when you want to get the response in real-time. Nobody likes to sit and wait for a response. Seeing the text generated as and when it is ready is a much better user experience.</p> </li> </ul> <p>For a full list of arguments, check out the OpenAI API documentation.</p>"},{"location":"2_open_ai/#available-models","title":"Available models\u00b6","text":""},{"location":"2_open_ai/#the-response-object","title":"The response object\u00b6","text":""},{"location":"2_open_ai/#streaming-a-response","title":"Streaming a response\u00b6","text":"<p>Streaming a response is mainly for user experience. It allows the user to see the response as it comes in, rather than waiting for the whole response to come in. For many applications, this might not be necessary.</p>"},{"location":"2_open_ai/#vision-input","title":"Vision input\u00b6","text":"<p>A huge draw of OpenAI models is the ability to input vision data. This is useful for a wide range of applications, including:</p> <ul> <li>Image captioning</li> <li>Object detection</li> <li>Face recognition</li> <li>Image generation</li> </ul> <p>Let's try an example of inputting an image. First we need to look at the image:</p> <p></p>"},{"location":"5_pydantic/","title":"Pydantic","text":"In\u00a0[55]: Copied! <pre>from rich.pretty import pprint\n</pre> from rich.pretty import pprint In\u00a0[56]: Copied! <pre>user_attributes = {\n    \"name\": \"Keanu Reeves\",\n    \"age\": 873, # because we all know Keanu Reeves is immortal\n    \"email\": \"jwick@email.com\",\n    \"pets\": [\"dog\"]\n}\n</pre> user_attributes = {     \"name\": \"Keanu Reeves\",     \"age\": 873, # because we all know Keanu Reeves is immortal     \"email\": \"jwick@email.com\",     \"pets\": [\"dog\"] } <p>we might want this input to always have the following form:</p> <pre>user_attributes = {\n    \"name\" : str,\n    \"age\" : int,\n    \"email\" : str,\n    \"pets\" : list[str],\n}\n</pre> <p>If you've taken an intro level python course, you might have been introduced to objects by creating classes such as people or cars with attributes and methods. So, suppose we want to take this dictionary that we got from the user input and create a class out of it.</p> In\u00a0[57]: Copied! <pre>from dataclasses import dataclass\n\n@dataclass\nclass User:\n    name: str\n    age: int\n    email: str\n    pets: list[str]\n</pre> from dataclasses import dataclass  @dataclass class User:     name: str     age: int     email: str     pets: list[str] <p>We can now construct a new user, by passing in the dictionary that we got from the user input.</p> In\u00a0[58]: Copied! <pre>new_user = User(**user_attributes)\nprint(type(new_user))\npprint(new_user, expand_all=True)\n</pre> new_user = User(**user_attributes) print(type(new_user)) pprint(new_user, expand_all=True) <pre>&lt;class '__main__.User'&gt;\n</pre> <pre>User(\n\u2502   name='Keanu Reeves',\n\u2502   age=873,\n\u2502   email='jwick@email.com',\n\u2502   pets=[\n\u2502   \u2502   'dog'\n\u2502   ]\n)\n</pre> <p>We might also want to do some checks on our user input. For example, we might want to make sure that the user is over 18 years old. We can do this by defining a function that checks the age.</p> In\u00a0[60]: Copied! <pre>def is_over_18(user: dict) -&gt; bool:\n    return user.age &gt;= 18\n\nis_over_18(new_user)\n</pre> def is_over_18(user: dict) -&gt; bool:     return user.age &gt;= 18  is_over_18(new_user) Out[60]: <pre>True</pre> <p>But now what happens if one of the fields is the incorrect type?</p> In\u00a0[18]: Copied! <pre>user_attributes[\"age\"] = \"873\"\n\nwrong_user = User(**user_attributes)\n\nis_over_18(wrong_user)\n</pre> user_attributes[\"age\"] = \"873\"  wrong_user = User(**user_attributes)  is_over_18(wrong_user) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[18], line 5\n      1 user_attributes[\"age\"] = \"873\"\n      3 wrong_user = User(**user_attributes)\n----&gt; 5 is_over_18(wrong_user)\n\nCell In[2], line 2, in is_over_18(user)\n      1 def is_over_18(user: dict) -&gt; bool:\n----&gt; 2     return user.age &gt;= 18\n\nTypeError: '&gt;=' not supported between instances of 'str' and 'int'</pre> <p>We're being told that you can't compare strings and integers (obviously).</p> <p>Now, we could write some manual checks for typing in our age-checking function, but ideally we would like to keep all of the validation stuff in one place.</p> <p>This is where Pydantic comes in.</p> In\u00a0[19]: Copied! <pre>from pydantic import BaseModel, Field\n</pre> from pydantic import BaseModel, Field <p>Creating a Pydantic object is similar to creating a dataclass. We use the <code>BaseModel</code> class, and pass in the fields we want using the <code>Field</code> class.</p> In\u00a0[26]: Copied! <pre>class User(BaseModel):\n    name: str = Field(..., title=\"Name\", description=\"Name of the user\")\n    age: int = Field(..., title=\"Age\", description=\"Age of the user\")\n    email: str = Field(..., title=\"Email\", description=\"Email of the user\")\n    pets: list[str] = Field(..., title=\"Pets\", description=\"Pets of the user\")\n</pre> class User(BaseModel):     name: str = Field(..., title=\"Name\", description=\"Name of the user\")     age: int = Field(..., title=\"Age\", description=\"Age of the user\")     email: str = Field(..., title=\"Email\", description=\"Email of the user\")     pets: list[str] = Field(..., title=\"Pets\", description=\"Pets of the user\") <p>Pydantic will now automatically check that the data we pass in is of the correct type.</p> In\u00a0[27]: Copied! <pre>user = User(**user_attributes)\npprint(user, expand_all=True)\n\nis_over_18(user)\n</pre> user = User(**user_attributes) pprint(user, expand_all=True)  is_over_18(user) <pre>User(\n\u2502   name='Keanu Reeves',\n\u2502   age=873,\n\u2502   email='jwick@email.com',\n\u2502   pets=[\n\u2502   \u2502   'dog'\n\u2502   ]\n)\n</pre> Out[27]: <pre>True</pre> <p>Even though we passed a string for age, Pydantic will try to convert it for us if possible. But what if we pass through a preposterous value for age?</p> In\u00a0[29]: Copied! <pre>user_attributes[\"age\"] = \"panda\"\n\ntry:\n    user = User(**user_attributes)\nexcept Exception as e:\n    print(e)\n</pre> user_attributes[\"age\"] = \"panda\"  try:     user = User(**user_attributes) except Exception as e:     print(e) <pre>1 validation error for User\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='panda', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/int_parsing\n</pre> <p>It makes no sense for someone to be <code>panda</code> years old, so Pydantic will not let us pass this in.</p> <p>Another handy feature is that we can build in additional validation checks.</p> In\u00a0[30]: Copied! <pre>class User(BaseModel):\n    name: str = Field(..., title=\"Name\", description=\"Name of the user\")\n    age: int = Field(..., title=\"Age\", description=\"Age of the user\", ge=18)\n    email: str = Field(..., title=\"Email\", description=\"Email of the user\")\n    pets: list[str] = Field(..., title=\"Pets\", description=\"Pets of the user\")\n</pre> class User(BaseModel):     name: str = Field(..., title=\"Name\", description=\"Name of the user\")     age: int = Field(..., title=\"Age\", description=\"Age of the user\", ge=18)     email: str = Field(..., title=\"Email\", description=\"Email of the user\")     pets: list[str] = Field(..., title=\"Pets\", description=\"Pets of the user\") <p>We have passed in the extra argument <code>ge=18</code> to the <code>Field</code> class. This means that the age must be greater than or equal to 18. If we pass in an age less than 18, Pydantic will raise an error:</p> In\u00a0[32]: Copied! <pre>from pydantic import ValidationError\n\nuser_attributes[\"age\"] = 17\n\ntry:\n    user = User(**user_attributes)\n    pprint(user)\nexcept ValidationError as e:\n    pprint(e.errors())\n</pre> from pydantic import ValidationError  user_attributes[\"age\"] = 17  try:     user = User(**user_attributes)     pprint(user) except ValidationError as e:     pprint(e.errors()) <pre>[\n\u2502   {\n\u2502   \u2502   'type': 'greater_than_equal',\n\u2502   \u2502   'loc': ('age',),\n\u2502   \u2502   'msg': 'Input should be greater than or equal to 18',\n\u2502   \u2502   'input': 17,\n\u2502   \u2502   'ctx': {'ge': 18},\n\u2502   \u2502   'url': 'https://errors.pydantic.dev/2.9/v/greater_than_equal'\n\u2502   }\n]\n</pre> <p>This is handy, because it has told us what failure was, what we input, and what control we put in place to prevent it.</p> <p>We can also define our own custom validators. Suppose we want to make sure that the email address is a valid email address. We can do this by defining a custom validator.</p> In\u00a0[33]: Copied! <pre>from pydantic import field_validator\nfrom pydantic_core import PydanticCustomError\n\nclass User(BaseModel):\n    name: str = Field(..., title=\"Name\", description=\"Name of the user\")\n    age: int = Field(..., title=\"Age\", description=\"Age of the user\", ge=18)\n    email: str = Field(..., title=\"Email\", description=\"Email of the user\")\n    pets: list[str] = Field(..., title=\"Summary\", description=\"Pets of the user\")\n\n    @field_validator(\"email\")\n    def check_email(cls, v):\n        if \"@\" not in v or \".\" not in v:\n            raise PydanticCustomError(\n                'InvalidEmail',\n                'Email must contain \"@\" and \".\"'\n            )\n        return v\n</pre> from pydantic import field_validator from pydantic_core import PydanticCustomError  class User(BaseModel):     name: str = Field(..., title=\"Name\", description=\"Name of the user\")     age: int = Field(..., title=\"Age\", description=\"Age of the user\", ge=18)     email: str = Field(..., title=\"Email\", description=\"Email of the user\")     pets: list[str] = Field(..., title=\"Summary\", description=\"Pets of the user\")      @field_validator(\"email\")     def check_email(cls, v):         if \"@\" not in v or \".\" not in v:             raise PydanticCustomError(                 'InvalidEmail',                 'Email must contain \"@\" and \".\"'             )         return v In\u00a0[35]: Copied! <pre>user_attributes = {\n    \"name\": \"Keanu Reeves\",\n    \"age\": 873,\n    \"email\": \"jwick-at-email-dot-com\",\n    \"pets\": [\"dog\"]\n}\n\ntry:\n    user = User(**user_attributes)\n    pprint(user)\nexcept ValidationError as e:\n    pprint(e.errors(), expand_all=True)\n</pre> user_attributes = {     \"name\": \"Keanu Reeves\",     \"age\": 873,     \"email\": \"jwick-at-email-dot-com\",     \"pets\": [\"dog\"] }  try:     user = User(**user_attributes)     pprint(user) except ValidationError as e:     pprint(e.errors(), expand_all=True) <pre>[\n\u2502   {\n\u2502   \u2502   'type': 'InvalidEmail',\n\u2502   \u2502   'loc': (\n\u2502   \u2502   \u2502   'email',\n\u2502   \u2502   ),\n\u2502   \u2502   'msg': 'Email must contain \"@\" and \".\"',\n\u2502   \u2502   'input': 'jwick-at-email-dot-com'\n\u2502   }\n]\n</pre> In\u00a0[36]: Copied! <pre>description = (\n    \"My name is Ryan, and I am 35 years old. \"\n    \"During the weekends I like to hike, but I also enjoy playing video games. \"\n    \"It can sometimes be difficult to use my computer, \"\n    \"because my cat likes to sleep on the keyboard! \"\n    \"During the week, I work as a MLE at the University of Cambridge. \"\n    \"Although I really enjoy living in the UK, \"\n    \"I miss the outdoors back home in NZ.\"\n)\n</pre> description = (     \"My name is Ryan, and I am 35 years old. \"     \"During the weekends I like to hike, but I also enjoy playing video games. \"     \"It can sometimes be difficult to use my computer, \"     \"because my cat likes to sleep on the keyboard! \"     \"During the week, I work as a MLE at the University of Cambridge. \"     \"Although I really enjoy living in the UK, \"     \"I miss the outdoors back home in NZ.\" )  <p>We can now use a system prompt to ask the LLM to extract the information we want.</p> In\u00a0[38]: Copied! <pre>system_prompt = (\n    \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"\n    \"- Name\\n\"\n    \"- Age\\n\"\n    \"- Nationality\\n\"\n    \"- Occupation\\n\"\n    \"- A list of any pets\\n\"\n    \"- A list of any hobbies\\n\\n\"\n    \"If any acronyms are used, please expand them.\\n\\n\"\n    \"Here is a description:\\n\\n\"\n)\n</pre> system_prompt = (     \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"     \"- Name\\n\"     \"- Age\\n\"     \"- Nationality\\n\"     \"- Occupation\\n\"     \"- A list of any pets\\n\"     \"- A list of any hobbies\\n\\n\"     \"If any acronyms are used, please expand them.\\n\\n\"     \"Here is a description:\\n\\n\" ) <p>This information is clearly contained within the text, and any human with basic comprehension skills can extract or infer it.</p> <p>Let's first try using an LLM to extract this information, probably to how we might prompt ChatGPT:</p> In\u00a0[40]: Copied! <pre>from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": description},\n    ],\n    temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n</pre> from openai import OpenAI  client = OpenAI()  response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": description},     ],     temperature=0.0, )  print(response.choices[0].message.content) <pre>- Name: Ryan\n- Age: 35\n- Nationality: New Zealander (NZ)\n- Occupation: Machine Learning Engineer (MLE) at the University of Cambridge\n- A list of any pets: Cat\n- A list of any hobbies: Hiking, playing video games\n</pre> <p>This might initially look good, but we can't really do anything useful with this information, since it's still unstructure. It looks structured, but parsing this would be annoying. I could write something that looked for the colon and then took the text afterwards; but then what about the lists of items? And any text in brackets? Or any extraneous information or text?</p> <p>Instead, I can try to get the output in a structured format.</p> <p>First we define a new Pydantic class that we want the output to be.</p> In\u00a0[41]: Copied! <pre>class Person(BaseModel):\n    name: str | None = Field(..., description=\"The name of the person\")\n    age: int | None = Field(..., description=\"The age of the person\")\n    nationality: str | None = Field(..., description=\"The nationality of the person\")\n    occupation: str | None = Field(..., description=\"The occupation of the person\")\n    pets: list[str] | None = Field(..., description=\"The pets of the person\")\n    hobbies: list[str] | None = Field(..., description=\"The hobbies of the person\")\n\n    # print the description of the person\n    def __str__(self) -&gt; str:\n        output = f\"Name: {self.name}\\n\"\n        output += f\"Age: {self.age}\\n\"\n        output += f\"Nationality: {self.nationality}\\n\"\n        output += f\"Occupation: {self.occupation}\\n\"\n        output += f\"Pets: {self.pets}\\n\"\n        output += f\"Hobbies: {self.hobbies}\\n\"\n        return output\n</pre> class Person(BaseModel):     name: str | None = Field(..., description=\"The name of the person\")     age: int | None = Field(..., description=\"The age of the person\")     nationality: str | None = Field(..., description=\"The nationality of the person\")     occupation: str | None = Field(..., description=\"The occupation of the person\")     pets: list[str] | None = Field(..., description=\"The pets of the person\")     hobbies: list[str] | None = Field(..., description=\"The hobbies of the person\")      # print the description of the person     def __str__(self) -&gt; str:         output = f\"Name: {self.name}\\n\"         output += f\"Age: {self.age}\\n\"         output += f\"Nationality: {self.nationality}\\n\"         output += f\"Occupation: {self.occupation}\\n\"         output += f\"Pets: {self.pets}\\n\"         output += f\"Hobbies: {self.hobbies}\\n\"         return output <p>Now we explicitely ask the LLM to output JSON.</p> In\u00a0[42]: Copied! <pre>system_prompt = (\n    \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"\n    \"- Name\\n\"\n    \"- Age\\n\"\n    \"- Nationality\\n\"\n    \"- Occupation\\n\"\n    \"- A list of any pets\\n\"\n    \"- A list of any hobbies\\n\\n\"\n    \"If any acronyms are used, please expand them.\\n\"\n    \"Return the information in JSON format.\\n\\n\" # &lt;--- JSON please!\n    \"Here is a description:\\n\\n\"\n)\n</pre> system_prompt = (     \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"     \"- Name\\n\"     \"- Age\\n\"     \"- Nationality\\n\"     \"- Occupation\\n\"     \"- A list of any pets\\n\"     \"- A list of any hobbies\\n\\n\"     \"If any acronyms are used, please expand them.\\n\"     \"Return the information in JSON format.\\n\\n\" # &lt;--- JSON please!     \"Here is a description:\\n\\n\" ) In\u00a0[43]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": description},\n  ],\n  max_tokens=512,\n  temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": description},   ],   max_tokens=512,   temperature=0.0, )  print(response.choices[0].message.content) <pre>```json\n{\n  \"Name\": \"Ryan\",\n  \"Age\": 35,\n  \"Nationality\": \"New Zealander\",\n  \"Occupation\": \"Machine Learning Engineer\",\n  \"Pets\": [\"cat\"],\n  \"Hobbies\": [\"hiking\", \"playing video games\"]\n}\n```\n</pre> <p>Great! We have valid JSON! Except we don't, we have those annoying <code>json</code> tags. OK, so now I can ask the LLM to not include those.</p> In\u00a0[44]: Copied! <pre>system_prompt = (\n    \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"\n    \"- Name\\n\"\n    \"- Age\\n\"\n    \"- Nationality\\n\"\n    \"- Occupation\\n\"\n    \"- A list of any pets\\n\"\n    \"- A list of any hobbies\\n\\n\"\n    \"If any acronyms are used, please expand them.\\n\"\n    \"Return the information in JSON format. \"\n    \"Do not include the `json` tags.\\n\\n\" # &lt;--- no tags please!\n    \"Here is a description:\\n\\n\"\n)\n</pre> system_prompt = (     \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"     \"- Name\\n\"     \"- Age\\n\"     \"- Nationality\\n\"     \"- Occupation\\n\"     \"- A list of any pets\\n\"     \"- A list of any hobbies\\n\\n\"     \"If any acronyms are used, please expand them.\\n\"     \"Return the information in JSON format. \"     \"Do not include the `json` tags.\\n\\n\" # &lt;--- no tags please!     \"Here is a description:\\n\\n\" ) In\u00a0[45]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": description},\n  ],\n  max_tokens=512,\n  temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": description},   ],   max_tokens=512,   temperature=0.0, )  print(response.choices[0].message.content) <pre>{\n  \"Name\": \"Ryan\",\n  \"Age\": 35,\n  \"Nationality\": \"New Zealander\",\n  \"Occupation\": \"Machine Learning Engineer\",\n  \"Pets\": [\"cat\"],\n  \"Hobbies\": [\"hiking\", \"playing video games\"]\n}\n</pre> <p>That actually worked, and this is valid JSON. This is what you might have to to do with many LLMs out there. However, OpenAI have gone to the effort of adding special <code>response_format</code> arguments to their API to make this easier. So I can feed in the original prompt that I had, and specify that I want the output in JSON format. And this is will guarantee that the output is valid JSON.</p> In\u00a0[46]: Copied! <pre>system_prompt = (\n    \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"\n    \"- Name\\n\"\n    \"- Age\\n\"\n    \"- Nationality\\n\"\n    \"- Occupation\\n\"\n    \"- A list of any pets\\n\"\n    \"- A list of any hobbies\\n\\n\"\n    \"If any acronyms are used, please expand them.\\n\"\n    \"Return the information in JSON format.\\n\\n\" # &lt;--- JSON please!\n    \"Here is a description:\\n\\n\"\n)\n</pre> system_prompt = (     \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"     \"- Name\\n\"     \"- Age\\n\"     \"- Nationality\\n\"     \"- Occupation\\n\"     \"- A list of any pets\\n\"     \"- A list of any hobbies\\n\\n\"     \"If any acronyms are used, please expand them.\\n\"     \"Return the information in JSON format.\\n\\n\" # &lt;--- JSON please!     \"Here is a description:\\n\\n\" ) In\u00a0[47]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": description},\n  ],\n  max_tokens=512,\n  temperature=0.0,\n  response_format={\"type\": \"json_object\"} # &lt;--- JSON please!\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": description},   ],   max_tokens=512,   temperature=0.0,   response_format={\"type\": \"json_object\"} # &lt;--- JSON please! )  print(response.choices[0].message.content) <pre>{\n  \"Name\": \"Ryan\",\n  \"Age\": 35,\n  \"Nationality\": \"New Zealander\",\n  \"Occupation\": \"Machine Learning Engineer\",\n  \"Pets\": [\"cat\"],\n  \"Hobbies\": [\"hiking\", \"playing video games\"]\n}\n</pre> <p>Why did we go to all this effort? Well, now we can try and pass this object in as arguments to our <code>Person</code> class, just as we did before with Keanu. This process is called deserialisation - the process of converting JSON into a Pydantic object.</p> In\u00a0[48]: Copied! <pre>import json\n\njson_content = json.loads(response.choices[0].message.content)\nperson = Person(**json_content)\n\npprint(person, expand_all=True)\n</pre> import json  json_content = json.loads(response.choices[0].message.content) person = Person(**json_content)  pprint(person, expand_all=True) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[48], line 4\n      1 import json\n      3 json_content = json.loads(response.choices[0].message.content)\n----&gt; 4 person = Person(**json_content)\n      6 pprint(person, expand_all=True)\n\nFile ~/Website/large-language-models/venv/lib/python3.11/site-packages/pydantic/main.py:209, in BaseModel.__init__(self, **data)\n    207 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    208 __tracebackhide__ = True\n--&gt; 209 validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    210 if self is not validated_self:\n    211     warnings.warn(\n    212         'A custom validator is returning a value other than `self`.\\n'\n    213         \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n    214         'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n    215         category=None,\n    216     )\n\nValidationError: 6 validation errors for Person\nname\n  Field required [type=missing, input_value={'Name': 'Ryan', 'Age': 3... 'playing video games']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nage\n  Field required [type=missing, input_value={'Name': 'Ryan', 'Age': 3... 'playing video games']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nnationality\n  Field required [type=missing, input_value={'Name': 'Ryan', 'Age': 3... 'playing video games']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\noccupation\n  Field required [type=missing, input_value={'Name': 'Ryan', 'Age': 3... 'playing video games']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\npets\n  Field required [type=missing, input_value={'Name': 'Ryan', 'Age': 3... 'playing video games']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nhobbies\n  Field required [type=missing, input_value={'Name': 'Ryan', 'Age': 3... 'playing video games']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing</pre> <p>What happened? It is valid json, so what is the problem? The issue here is the title of the required fields don't match up! <code>Name</code> in the returned output is <code>name</code> in the Pydantic object. One way to fix this is to also directly feed the schema into the prompt.</p> <p>A schema is essentially a description of the data that we want to pass in. We can use the <code>json_schema</code> method to get the schema of the Pydantic object.</p> <p>A handy feature of Pydantic objects is that you can serialise the class description into a JSON schema - essentially just a string or dictionary representation of the object.</p> In\u00a0[49]: Copied! <pre>schema = Person.model_json_schema()\nprint(type(schema))\npprint(schema)\n</pre> schema = Person.model_json_schema() print(type(schema)) pprint(schema) <pre>&lt;class 'dict'&gt;\n</pre> <pre>{\n\u2502   'properties': {\n\u2502   \u2502   'name': {\n\u2502   \u2502   \u2502   'anyOf': [{'type': 'string'}, {'type': 'null'}],\n\u2502   \u2502   \u2502   'description': 'The name of the person',\n\u2502   \u2502   \u2502   'title': 'Name'\n\u2502   \u2502   },\n\u2502   \u2502   'age': {\n\u2502   \u2502   \u2502   'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n\u2502   \u2502   \u2502   'description': 'The age of the person',\n\u2502   \u2502   \u2502   'title': 'Age'\n\u2502   \u2502   },\n\u2502   \u2502   'nationality': {\n\u2502   \u2502   \u2502   'anyOf': [{'type': 'string'}, {'type': 'null'}],\n\u2502   \u2502   \u2502   'description': 'The nationality of the person',\n\u2502   \u2502   \u2502   'title': 'Nationality'\n\u2502   \u2502   },\n\u2502   \u2502   'occupation': {\n\u2502   \u2502   \u2502   'anyOf': [{'type': 'string'}, {'type': 'null'}],\n\u2502   \u2502   \u2502   'description': 'The occupation of the person',\n\u2502   \u2502   \u2502   'title': 'Occupation'\n\u2502   \u2502   },\n\u2502   \u2502   'pets': {\n\u2502   \u2502   \u2502   'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}],\n\u2502   \u2502   \u2502   'description': 'The pets of the person',\n\u2502   \u2502   \u2502   'title': 'Pets'\n\u2502   \u2502   },\n\u2502   \u2502   'hobbies': {\n\u2502   \u2502   \u2502   'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}],\n\u2502   \u2502   \u2502   'description': 'The hobbies of the person',\n\u2502   \u2502   \u2502   'title': 'Hobbies'\n\u2502   \u2502   }\n\u2502   },\n\u2502   'required': ['name', 'age', 'nationality', 'occupation', 'pets', 'hobbies'],\n\u2502   'title': 'Person',\n\u2502   'type': 'object'\n}\n</pre> <p>Now we should include this schema into the prompt</p> In\u00a0[50]: Copied! <pre>system_prompt = (\n    \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"\n    \"- Name\\n\"\n    \"- Age\\n\"\n    \"- Nationality\\n\"\n    \"- Occupation\\n\"\n    \"- A list of any pets\\n\"\n    \"- A list of any hobbies\\n\\n\"\n    \"If any acronyms are used, please expand them.\\n\\n\"\n    f\"Return the information in JSON format according to the following schema:\\n\\n{schema}\\n\\n\"\n    \"Here is a description:\\n\\n\"\n)\n\nprint(system_prompt)\n</pre> system_prompt = (     \"Your main role is to analyse a piece of unstructured text and extract the following information:\\n\"     \"- Name\\n\"     \"- Age\\n\"     \"- Nationality\\n\"     \"- Occupation\\n\"     \"- A list of any pets\\n\"     \"- A list of any hobbies\\n\\n\"     \"If any acronyms are used, please expand them.\\n\\n\"     f\"Return the information in JSON format according to the following schema:\\n\\n{schema}\\n\\n\"     \"Here is a description:\\n\\n\" )  print(system_prompt) <pre>Your main role is to analyse a piece of unstructured text and extract the following information:\n- Name\n- Age\n- Nationality\n- Occupation\n- A list of any pets\n- A list of any hobbies\n\nIf any acronyms are used, please expand them.\n\nReturn the information in JSON format according to the following schema:\n\n{'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'The name of the person', 'title': 'Name'}, 'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'description': 'The age of the person', 'title': 'Age'}, 'nationality': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'The nationality of the person', 'title': 'Nationality'}, 'occupation': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'The occupation of the person', 'title': 'Occupation'}, 'pets': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'description': 'The pets of the person', 'title': 'Pets'}, 'hobbies': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'description': 'The hobbies of the person', 'title': 'Hobbies'}}, 'required': ['name', 'age', 'nationality', 'occupation', 'pets', 'hobbies'], 'title': 'Person', 'type': 'object'}\n\nHere is a description:\n\n\n</pre> In\u00a0[51]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": description},\n  ],\n  max_tokens=512,\n  response_format={\"type\": \"json_object\"},\n  temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": description},   ],   max_tokens=512,   response_format={\"type\": \"json_object\"},   temperature=0.0 )  print(response.choices[0].message.content) <pre>{\n  \"name\": \"Ryan\",\n  \"age\": 35,\n  \"nationality\": \"New Zealand\",\n  \"occupation\": \"Machine Learning Engineer\",\n  \"pets\": [\"cat\"],\n  \"hobbies\": [\"hiking\", \"playing video games\"]\n}\n</pre> <p>Good, this is valid</p> In\u00a0[52]: Copied! <pre>json_content = json.loads(response.choices[0].message.content)\nperson = Person(**json_content)\n\npprint(person, expand_all=True)\n</pre> json_content = json.loads(response.choices[0].message.content) person = Person(**json_content)  pprint(person, expand_all=True)  <pre>Person(\n\u2502   name='Ryan',\n\u2502   age=35,\n\u2502   nationality='New Zealand',\n\u2502   occupation='Machine Learning Engineer',\n\u2502   pets=[\n\u2502   \u2502   'cat'\n\u2502   ],\n\u2502   hobbies=[\n\u2502   \u2502   'hiking',\n\u2502   \u2502   'playing video games'\n\u2502   ]\n)\n</pre>"},{"location":"5_pydantic/#pydantic","title":"Pydantic\u00b6","text":""},{"location":"5_pydantic/#introduction-to-pydantic","title":"Introduction to Pydantic\u00b6","text":"<p>Pydantic is a data validation library in python. Suppose we get some user attributes. This data could have come from anywhere - user input, an API call, whatever. Suppose for now that this is some information about a user that the user themselves has provided via some form.</p>"},{"location":"5_pydantic/#using-dataclasses","title":"Using dataclasses\u00b6","text":"<p>One way to turn our user input into an object is to use a <code>dataclass</code>.</p>"},{"location":"5_pydantic/#using-pydantic","title":"Using Pydantic\u00b6","text":""},{"location":"5_pydantic/#application-to-llms","title":"Application to LLMs\u00b6","text":"<p>An important application of Pydantic is in validating the output of LLMs. For example, suppose we have the following description:</p>"},{"location":"notebook_example/","title":"Notebook example","text":"In\u00a0[1]: Copied! <pre># Hello World\nprint(\"hello world\")\n</pre> # Hello World print(\"hello world\") <pre>hello world\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"p1_doc-validation/","title":"Practical - Open Source Article Validation","text":"<p>In this notebook, we want to determine whether an article from a popular journal adheres to best practices around software engineering. We will be loosely applying some of the criteria around the</p> In\u00a0[1]: Copied! <pre>from llama_index.core import VectorStoreIndex\nfrom llama_index.readers.github import GithubRepositoryReader, GithubClient\n\nfrom IPython.display import Markdown, display\nimport os\n</pre> from llama_index.core import VectorStoreIndex from llama_index.readers.github import GithubRepositoryReader, GithubClient  from IPython.display import Markdown, display import os In\u00a0[4]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[51]: Copied! <pre>import dotenv\nimport os\n\ndotenv.load_dotenv()\n\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\nowner = \"rkdan\"\nrepo = \"small_world_propensity\"\nbranch = \"main\"\n</pre> import dotenv import os  dotenv.load_dotenv()  github_token = os.environ.get(\"GITHUB_TOKEN\") owner = \"rkdan\" repo = \"small_world_propensity\" branch = \"main\" In\u00a0[52]: Copied! <pre>github_client = GithubClient(github_token=github_token, verbose=True)\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n</pre> github_client = GithubClient(github_token=github_token, verbose=True) from llama_index.embeddings.openai import OpenAIEmbedding In\u00a0[87]: Copied! <pre>documents = GithubRepositoryReader(\n    github_client=github_client,\n    owner=owner,\n    repo=repo,\n    use_parser=True,\n    verbose=False,\n    filter_file_extensions=(\n        [\n            \".lock\",\n            \".mat\",\n            \".mtx\"\n        ],\n        GithubRepositoryReader.FilterType.EXCLUDE,\n    ),\n).load_data(branch=branch)\n</pre> documents = GithubRepositoryReader(     github_client=github_client,     owner=owner,     repo=repo,     use_parser=True,     verbose=False,     filter_file_extensions=(         [             \".lock\",             \".mat\",             \".mtx\"         ],         GithubRepositoryReader.FilterType.EXCLUDE,     ), ).load_data(branch=branch) In\u00a0[116]: Copied! <pre>documents\n</pre> documents Out[116]: <pre>[Document(id_='34ae710a53440d7a200be4cc65b252d73afa54e9', embedding=None, metadata={'file_path': '.github/scripts/update_version_and_changelog.sh', 'file_name': 'update_version_and_changelog.sh', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/.github/scripts/update_version_and_changelog.sh'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='#!/bin/bash\\n\\n# Increment version in pyproject.toml and __init__.py\\n# This is a simplistic approach; you may need a more sophisticated versioning logic.\\npoetry version patch\\n\\n# Extract the new version number\\nNEW_VERSION=$(poetry version -s)\\n\\n# Update __init__.py with the new version\\nsed -i \"s/__version__ = .*/__version__ = \\'$NEW_VERSION\\'/\" small_world_propensity/__init__.py\\n\\n# Update CHANGELOG.md\\n# This is a placeholder. Consider using a tool or script to generate meaningful changelog entries.\\necho -e \"## $NEW_VERSION\\\\n* Your changes here\\\\n\\\\n$(cat CHANGELOG.md)\" &gt; CHANGELOG.md', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='8f8b195375350f8f7b83cee61b1db8015302e5d5', embedding=None, metadata={'file_path': '.github/workflows/python-app.yml', 'file_name': 'python-app.yml', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/.github/workflows/python-app.yml'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='name: Small World Propensity\\n\\non:\\n  push:\\n    branches:\\n      - main\\n  pull_request:\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        python-version: [\"3.10\"]\\n\\n    steps:\\n      - uses: actions/checkout@v3\\n      - name: Install Poetry\\n        run: pipx install poetry\\n      - name: Set up Python ${{ matrix.python-version }}\\n        uses: actions/setup-python@v4\\n        with:\\n          python-version: ${{ matrix.python-version }}\\n          cache: \"poetry\"\\n      - name: Install dependencies\\n        run: poetry install\\n\\n  release:\\n    needs: build\\n    permissions:\\n      packages: write\\n      pull-requests: write\\n      contents: write\\n      repository-projects: write\\n    if: github.event_name == \\'push\\' &amp;&amp; github.ref == \\'refs/heads/main\\'\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        python-version: [\"3.10\"]\\n    steps:\\n      - uses: actions/checkout@v3\\n        with:\\n          fetch-depth: 0\\n          token: ${{ secrets.GITHUB_TOKEN }}\\n      - name: Install Poetry\\n        run: pipx install poetry\\n      - name: Set up Python ${{ matrix.python-version }}\\n        uses: actions/setup-python@v4\\n        with:\\n          python-version: ${{ matrix.python-version }}\\n          cache: \"poetry\"\\n      - name: Install dependencies\\n        run: poetry install\\n      - name: Bump version\\n        run: |\\n          git config user.name github-actions\\n          git config user.email github-actions@github.com\\n          \\n          # Get the commit message\\n          commit_message=$(git log -1 --pretty=%B)\\n          \\n          # Determine the version bump type based on the commit message\\n          if echo \"$commit_message\" | grep -q -i -E \\'^(break|major)\\\\b\\'; then\\n            version_type=\"major\"\\n          elif echo \"$commit_message\" | grep -q -i -E \\'^(feat|minor)\\\\b\\'; then\\n            version_type=\"minor\"\\n          else\\n            version_type=\"patch\"\\n          fi\\n          \\n          # Bump the version using Poetry\\n          poetry version $version_type\\n          version=$(poetry version -s)\\n          \\n          git add pyproject.toml\\n          git commit -m \"Bump version to $version\"\\n          git push\\n      - name: Build package\\n        run: poetry build\\n      - name: Get version from pyproject.toml\\n        id: get_version\\n        run: |\\n          version=$(poetry version -s)\\n          echo \"::set-output name=version::$version\"\\n      - name: Create Release\\n        id: create_release\\n        uses: actions/create-release@v1\\n        env:\\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n        with:\\n          tag_name: v${{ steps.get_version.outputs.version }}\\n          release_name: Release ${{ steps.get_version.outputs.version }}\\n          body: Release ${{ steps.get_version.outputs.version }}\\n      - name: Publish to PyPI\\n        uses: pypa/gh-action-pypi-publish@release/v1\\n        with:\\n          user: __token__\\n          password: ${{ secrets.PYPI_API }}\\n\\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='efe307facd6592a8e83f0af85804ec7cc1f7aceb', embedding=None, metadata={'file_path': '.gitignore', 'file_name': '.gitignore', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/.gitignore'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\".vs/\\n.vscode/\\n.idea/\\n# Byte-compiled / optimized / DLL files\\n__pycache__/\\n*.py[cod]\\n*$py.class\\n\\n# C extensions\\n*.so\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\nwheels/\\npip-wheel-metadata/\\nshare/python-wheels/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\nMANIFEST\\n\\n# PyInstaller\\n#  Usually these files are written by a python script from a template\\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n*.manifest\\n*.spec\\n\\n# Installer logs\\npip-log.txt\\npip-delete-this-directory.txt\\n\\n# Unit test / coverage reports\\nhtmlcov/\\n.tox/\\n.nox/\\n.coverage\\n.coverage.*\\n.cache\\nnosetests.xml\\ncoverage.xml\\n*.cover\\n*.py,cover\\n.hypothesis/\\n.pytest_cache/\\n\\n# Translations\\n*.mo\\n*.pot\\n\\n# Django stuff:\\n*.log\\nlocal_settings.py\\ndb.sqlite3\\ndb.sqlite3-journal\\n\\n# Flask stuff:\\ninstance/\\n.webassets-cache\\n\\n# Scrapy stuff:\\n.scrapy\\n\\n# Sphinx documentation\\ndocs/_build/\\ndocs/docs/_build/\\n\\n# PyBuilder\\ntarget/\\n\\n# Jupyter Notebook\\n.ipynb_checkpoints\\nnotebooks/\\n\\n# IPython\\nprofile_default/\\nipython_config.py\\n\\n# pyenv\\n.python-version\\n\\n# pipenv\\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n#   install all needed dependencies.\\n#Pipfile.lock\\n\\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\\n__pypackages__/\\n\\n# Celery stuff\\ncelerybeat-schedule\\ncelerybeat.pid\\n\\n\\n# SageMath parsed files\\n*.sage.py\\n\\n# Environments\\n.env\\n.envrc\\n.venv\\n.venvs\\nenv/\\nvenv/\\nENV/\\nenv.bak/\\nvenv.bak/\\n\\n# Spyder project settings\\n.spyderproject\\n.spyproject\\n\\n# Rope project settings\\n.ropeproject\\n\\n# mkdocs documentation\\n/site\\n\\n# mypy\\n.mypy_cache/\\n.dmypy.json\\ndmypy.json\\n\\n# Pyre type checker\\n.pyre/\\n\\n# macOS display setting files\\n.DS_Store\\n\\n# Wandb directory\\nwandb/\\n\\n# asdf tool versions\\n.tool-versions\\n/.ruff_cache/\\n\\n*.pkl\\n*.bin\\n\\n# integration test artifacts\\ndata_map*\\n\\\\[('_type', 'fake'), ('stop', None)]\\n\\n# Replit files\\n*replit*\\n\\nnode_modules\\ndocs/.yarn/\\ndocs/node_modules/\\ndocs/.docusaurus/\\ndocs/.cache-loader/\\ndocs/_dist\\ndocs/api_reference/_build\\ndocs/docs_skeleton/build\\ndocs/docs_skeleton/node_modules\\ndocs/docs_skeleton/yarn.lock\\n\\nU_data/\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='306ca229efdd133789470d0c530a20a7e473c30a', embedding=None, metadata={'file_path': '.pre-commit-config.yaml', 'file_name': '.pre-commit-config.yaml', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/.pre-commit-config.yaml'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='repos:\\n-   repo: https://github.com/pre-commit/pre-commit-hooks\\n    rev: v4.4.0\\n    hooks:\\n    -   id: check-yaml\\n    -   id: end-of-file-fixer\\n    -   id: trailing-whitespace\\n-   repo: https://github.com/psf/black\\n    rev: 23.3.0\\n    hooks:\\n    -   id: black\\n-   repo: https://github.com/PyCQA/isort\\n    rev: 5.12.0\\n    hooks:\\n    -   id: isort\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='b8c0c68229076ba67c244c11075169d29e981e7f', embedding=None, metadata={'file_path': 'CHANGELOG.md', 'file_name': 'CHANGELOG.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nChangelog\\n\\n\\n\\n\\n\\n\\nv0.0.7 (2023-09-26)\\n\\n\\n\\n\\n\\n\\nv0.0.6 (2023-08-20)\\n\\n\\n\\n\\n\\n\\nv0.0.5 (2023-08-17)\\n\\n\\n\\n\\n\\n\\nv0.0.4 (2023-08-17)\\n\\n\\n\\n\\n\\n\\nv0.0.3 (2023-06-27)\\n\\n\\n\\n\\n\\n\\nv0.0.2 (2023-06-15)\\n\\n\\n\\n\\n\\n\\nv0.0.1 (2023-06-15)\\n\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='bae94e189e62df1b8d5bd11435d954b25bee9d7b', embedding=None, metadata={'file_path': 'LICENSE', 'file_name': 'LICENSE', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/LICENSE'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='                    GNU AFFERO GENERAL PUBLIC LICENSE\\n                       Version 3, 19 November 2007\\n\\n Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;\\n Everyone is permitted to copy and distribute verbatim copies\\n of this license document, but changing it is not allowed.\\n\\n                            Preamble\\n\\n  The GNU Affero General Public License is a free, copyleft license for\\nsoftware and other kinds of works, specifically designed to ensure\\ncooperation with the community in the case of network server software.\\n\\n  The licenses for most software and other practical works are designed\\nto take away your freedom to share and change the works.  By contrast,\\nour General Public Licenses are intended to guarantee your freedom to\\nshare and change all versions of a program--to make sure it remains free\\nsoftware for all its users.\\n\\n  When we speak of free software, we are referring to freedom, not\\nprice.  Our General Public Licenses are designed to make sure that you\\nhave the freedom to distribute copies of free software (and charge for\\nthem if you wish), that you receive source code or can get it if you\\nwant it, that you can change the software or use pieces of it in new\\nfree programs, and that you know you can do these things.\\n\\n  Developers that use our General Public Licenses protect your rights\\nwith two steps: (1) assert copyright on the software, and (2) offer\\nyou this License which gives you legal permission to copy, distribute\\nand/or modify the software.\\n\\n  A secondary benefit of defending all users\\' freedom is that\\nimprovements made in alternate versions of the program, if they\\nreceive widespread use, become available for other developers to\\nincorporate.  Many developers of free software are heartened and\\nencouraged by the resulting cooperation.  However, in the case of\\nsoftware used on network servers, this result may fail to come about.\\nThe GNU General Public License permits making a modified version and\\nletting the public access it on a server without ever releasing its\\nsource code to the public.\\n\\n  The GNU Affero General Public License is designed specifically to\\nensure that, in such cases, the modified source code becomes available\\nto the community.  It requires the operator of a network server to\\nprovide the source code of the modified version running there to the\\nusers of that server.  Therefore, public use of a modified version, on\\na publicly accessible server, gives the public access to the source\\ncode of the modified version.\\n\\n  An older license, called the Affero General Public License and\\npublished by Affero, was designed to accomplish similar goals.  This is\\na different license, not a version of the Affero GPL, but Affero has\\nreleased a new version of the Affero GPL which permits relicensing under\\nthis license.\\n\\n  The precise terms and conditions for copying, distribution and\\nmodification follow.\\n\\n                       TERMS AND CONDITIONS\\n\\n  0. Definitions.\\n\\n  \"This License\" refers to version 3 of the GNU Affero General Public License.\\n\\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\\nworks, such as semiconductor masks.\\n\\n  \"The Program\" refers to any copyrightable work licensed under this\\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\\n\"recipients\" may be individuals or organizations.\\n\\n  To \"modify\" a work means to copy from or adapt all or part of the work\\nin a fashion requiring copyright permission, other than the making of an\\nexact copy.  The resulting work is called a \"modified version\" of the\\nearlier work or a work \"based on\" the earlier work.\\n\\n  A \"covered work\" means either the unmodified Program or a work based\\non the Program.\\n\\n  To \"propagate\" a work means to do anything with it that, without\\npermission, would make you directly or secondarily liable for\\ninfringement under applicable copyright law, except executing it on a\\ncomputer or modifying a private copy.  Propagation includes copying,\\ndistribution (with or without modification), making available to the\\npublic, and in some countries other activities as well.\\n\\n  To \"convey\" a work means any kind of propagation that enables other\\nparties to make or receive copies.  Mere interaction with a user through\\na computer network, with no transfer of a copy, is not conveying.\\n\\n  An interactive user interface displays \"Appropriate Legal Notices\"\\nto the extent that it includes a convenient and prominently visible\\nfeature that (1) displays an appropriate copyright notice, and (2)\\ntells the user that there is no warranty for the work (except to the\\nextent that warranties are provided), that licensees may convey the\\nwork under this License, and how to view a copy of this License.  If\\nthe interface presents a list of user commands or options, such as a\\nmenu, a prominent item in the list meets this criterion.\\n\\n  1. Source Code.\\n\\n  The \"source code\" for a work means the preferred form of the work\\nfor making modifications to it.  \"Object code\" means any non-source\\nform of a work.\\n\\n  A \"Standard Interface\" means an interface that either is an official\\nstandard defined by a recognized standards body, or, in the case of\\ninterfaces specified for a particular programming language, one that\\nis widely used among developers working in that language.\\n\\n  The \"System Libraries\" of an executable work include anything, other\\nthan the work as a whole, that (a) is included in the normal form of\\npackaging a Major Component, but which is not part of that Major\\nComponent, and (b) serves only to enable use of the work with that\\nMajor Component, or to implement a Standard Interface for which an\\nimplementation is available to the public in source code form.  A\\n\"Major Component\", in this context, means a major essential component\\n(kernel, window system, and so on) of the specific operating system\\n(if any) on which the executable work runs, or a compiler used to\\nproduce the work, or an object code interpreter used to run it.\\n\\n  The \"Corresponding Source\" for a work in object code form means all\\nthe source code needed to generate, install, and (for an executable\\nwork) run the object code and to modify the work, including scripts to\\ncontrol those activities.  However, it does not include the work\\'s\\nSystem Libraries, or general-purpose tools or generally available free\\nprograms which are used unmodified in performing those activities but\\nwhich are not part of the work.  For example, Corresponding Source\\nincludes interface definition files associated with source files for\\nthe work, and the source code for shared libraries and dynamically\\nlinked subprograms that the work is specifically designed to require,\\nsuch as by intimate data communication or control flow between those\\nsubprograms and other parts of the work.\\n\\n  The Corresponding Source need not include anything that users\\ncan regenerate automatically from other parts of the Corresponding\\nSource.\\n\\n  The Corresponding Source for a work in source code form is that\\nsame work.\\n\\n  2. Basic Permissions.\\n\\n  All rights granted under this License are granted for the term of\\ncopyright on the Program, and are irrevocable provided the stated\\nconditions are met.  This License explicitly affirms your unlimited\\npermission to run the unmodified Program.  The output from running a\\ncovered work is covered by this License only if the output, given its\\ncontent, constitutes a covered work.  This License acknowledges your\\nrights of fair use or other equivalent, as provided by copyright law.\\n\\n  You may make, run and propagate covered works that you do not\\nconvey, without conditions so long as your license otherwise remains\\nin force.  You may convey covered works to others for the sole purpose\\nof having them make modifications exclusively for you, or provide you\\nwith facilities for running those works, provided that you comply with\\nthe terms of this License in conveying all material for which you do\\nnot control copyright.  Those thus making or running the covered works\\nfor you must do so exclusively on your behalf, under your direction\\nand control, on terms that prohibit them from making any copies of\\nyour copyrighted material outside their relationship with you.\\n\\n  Conveying under any other circumstances is permitted solely under\\nthe conditions stated below.  Sublicensing is not allowed; section 10\\nmakes it unnecessary.\\n\\n  3. Protecting Users\\' Legal Rights From Anti-Circumvention Law.\\n\\n  No covered work shall be deemed part of an effective technological\\nmeasure under any applicable law fulfilling obligations under article\\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\\nsimilar laws prohibiting or restricting circumvention of such\\nmeasures.\\n\\n  When you convey a covered work, you waive any legal power to forbid\\ncircumvention of technological measures to the extent such circumvention\\nis effected by exercising rights under this License with respect to\\nthe covered work, and you disclaim any intention to limit operation or\\nmodification of the work as a means of enforcing, against the work\\'s\\nusers, your or third parties\\' legal rights to forbid circumvention of\\ntechnological measures.\\n\\n  4. Conveying Verbatim Copies.\\n\\n  You may convey verbatim copies of the Program\\'s source code as you\\nreceive it, in any medium, provided that you conspicuously and\\nappropriately publish on each copy an appropriate copyright notice;\\nkeep intact all notices stating that this License and any\\nnon-permissive terms added in accord with section 7 apply to the code;\\nkeep intact all notices of the absence of any warranty; and give all\\nrecipients a copy of this License along with the Program.\\n\\n  You may charge any price or no price for each copy that you convey,\\nand you may offer support or warranty protection for a fee.\\n\\n  5. Conveying Modified Source Versions.\\n\\n  You may convey a work based on the Program, or the modifications to\\nproduce it from the Program, in the form of source code under the\\nterms of section 4, provided that you also meet all of these conditions:\\n\\n    a) The work must carry prominent notices stating that you modified\\n    it, and giving a relevant date.\\n\\n    b) The work must carry prominent notices stating that it is\\n    released under this License and any conditions added under section\\n    7.  This requirement modifies the requirement in section 4 to\\n    \"keep intact all notices\".\\n\\n    c) You must license the entire work, as a whole, under this\\n    License to anyone who comes into possession of a copy.  This\\n    License will therefore apply, along with any applicable section 7\\n    additional terms, to the whole of the work, and all its parts,\\n    regardless of how they are packaged.  This License gives no\\n    permission to license the work in any other way, but it does not\\n    invalidate such permission if you have separately received it.\\n\\n    d) If the work has interactive user interfaces, each must display\\n    Appropriate Legal Notices; however, if the Program has interactive\\n    interfaces that do not display Appropriate Legal Notices, your\\n    work need not make them do so.\\n\\n  A compilation of a covered work with other separate and independent\\nworks, which are not by their nature extensions of the covered work,\\nand which are not combined with it such as to form a larger program,\\nin or on a volume of a storage or distribution medium, is called an\\n\"aggregate\" if the compilation and its resulting copyright are not\\nused to limit the access or legal rights of the compilation\\'s users\\nbeyond what the individual works permit.  Inclusion of a covered work\\nin an aggregate does not cause this License to apply to the other\\nparts of the aggregate.\\n\\n  6. Conveying Non-Source Forms.\\n\\n  You may convey a covered work in object code form under the terms\\nof sections 4 and 5, provided that you also convey the\\nmachine-readable Corresponding Source under the terms of this License,\\nin one of these ways:\\n\\n    a) Convey the object code in, or embodied in, a physical product\\n    (including a physical distribution medium), accompanied by the\\n    Corresponding Source fixed on a durable physical medium\\n    customarily used for software interchange.\\n\\n    b) Convey the object code in, or embodied in, a physical product\\n    (including a physical distribution medium), accompanied by a\\n    written offer, valid for at least three years and valid for as\\n    long as you offer spare parts or customer support for that product\\n    model, to give anyone who possesses the object code either (1) a\\n    copy of the Corresponding Source for all the software in the\\n    product that is covered by this License, on a durable physical\\n    medium customarily used for software interchange, for a price no\\n    more than your reasonable cost of physically performing this\\n    conveying of source, or (2) access to copy the\\n    Corresponding Source from a network server at no charge.\\n\\n    c) Convey individual copies of the object code with a copy of the\\n    written offer to provide the Corresponding Source.  This\\n    alternative is allowed only occasionally and noncommercially, and\\n    only if you received the object code with such an offer, in accord\\n    with subsection 6b.\\n\\n    d) Convey the object code by offering access from a designated\\n    place (gratis or for a charge), and offer equivalent access to the\\n    Corresponding Source in the same way through the same place at no\\n    further charge.  You need not require recipients to copy the\\n    Corresponding Source along with the object code.  If the place to\\n    copy the object code is a network server, the Corresponding Source\\n    may be on a different server (operated by you or a third party)\\n    that supports equivalent copying facilities, provided you maintain\\n    clear directions next to the object code saying where to find the\\n    Corresponding Source.  Regardless of what server hosts the\\n    Corresponding Source, you remain obligated to ensure that it is\\n    available for as long as needed to satisfy these requirements.\\n\\n    e) Convey the object code using peer-to-peer transmission, provided\\n    you inform other peers where the object code and Corresponding\\n    Source of the work are being offered to the general public at no\\n    charge under subsection 6d.\\n\\n  A separable portion of the object code, whose source code is excluded\\nfrom the Corresponding Source as a System Library, need not be\\nincluded in conveying the object code work.\\n\\n  A \"User Product\" is either (1) a \"consumer product\", which means any\\ntangible personal property which is normally used for personal, family,\\nor household purposes, or (2) anything designed or sold for incorporation\\ninto a dwelling.  In determining whether a product is a consumer product,\\ndoubtful cases shall be resolved in favor of coverage.  For a particular\\nproduct received by a particular user, \"normally used\" refers to a\\ntypical or common use of that class of product, regardless of the status\\nof the particular user or of the way in which the particular user\\nactually uses, or expects or is expected to use, the product.  A product\\nis a consumer product regardless of whether the product has substantial\\ncommercial, industrial or non-consumer uses, unless such uses represent\\nthe only significant mode of use of the product.\\n\\n  \"Installation Information\" for a User Product means any methods,\\nprocedures, authorization keys, or other information required to install\\nand execute modified versions of a covered work in that User Product from\\na modified version of its Corresponding Source.  The information must\\nsuffice to ensure that the continued functioning of the modified object\\ncode is in no case prevented or interfered with solely because\\nmodification has been made.\\n\\n  If you convey an object code work under this section in, or with, or\\nspecifically for use in, a User Product, and the conveying occurs as\\npart of a transaction in which the right of possession and use of the\\nUser Product is transferred to the recipient in perpetuity or for a\\nfixed term (regardless of how the transaction is characterized), the\\nCorresponding Source conveyed under this section must be accompanied\\nby the Installation Information.  But this requirement does not apply\\nif neither you nor any third party retains the ability to install\\nmodified object code on the User Product (for example, the work has\\nbeen installed in ROM).\\n\\n  The requirement to provide Installation Information does not include a\\nrequirement to continue to provide support service, warranty, or updates\\nfor a work that has been modified or installed by the recipient, or for\\nthe User Product in which it has been modified or installed.  Access to a\\nnetwork may be denied when the modification itself materially and\\nadversely affects the operation of the network or violates the rules and\\nprotocols for communication across the network.\\n\\n  Corresponding Source conveyed, and Installation Information provided,\\nin accord with this section must be in a format that is publicly\\ndocumented (and with an implementation available to the public in\\nsource code form), and must require no special password or key for\\nunpacking, reading or copying.\\n\\n  7. Additional Terms.\\n\\n  \"Additional permissions\" are terms that supplement the terms of this\\nLicense by making exceptions from one or more of its conditions.\\nAdditional permissions that are applicable to the entire Program shall\\nbe treated as though they were included in this License, to the extent\\nthat they are valid under applicable law.  If additional permissions\\napply only to part of the Program, that part may be used separately\\nunder those permissions, but the entire Program remains governed by\\nthis License without regard to the additional permissions.\\n\\n  When you convey a copy of a covered work, you may at your option\\nremove any additional permissions from that copy, or from any part of\\nit.  (Additional permissions may be written to require their own\\nremoval in certain cases when you modify the work.)  You may place\\nadditional permissions on material, added by you to a covered work,\\nfor which you have or can give appropriate copyright permission.\\n\\n  Notwithstanding any other provision of this License, for material you\\nadd to a covered work, you may (if authorized by the copyright holders of\\nthat material) supplement the terms of this License with terms:\\n\\n    a) Disclaiming warranty or limiting liability differently from the\\n    terms of sections 15 and 16 of this License; or\\n\\n    b) Requiring preservation of specified reasonable legal notices or\\n    author attributions in that material or in the Appropriate Legal\\n    Notices displayed by works containing it; or\\n\\n    c) Prohibiting misrepresentation of the origin of that material, or\\n    requiring that modified versions of such material be marked in\\n    reasonable ways as different from the original version; or\\n\\n    d) Limiting the use for publicity purposes of names of licensors or\\n    authors of the material; or\\n\\n    e) Declining to grant rights under trademark law for use of some\\n    trade names, trademarks, or service marks; or\\n\\n    f) Requiring indemnification of licensors and authors of that\\n    material by anyone who conveys the material (or modified versions of\\n    it) with contractual assumptions of liability to the recipient, for\\n    any liability that these contractual assumptions directly impose on\\n    those licensors and authors.\\n\\n  All other non-permissive additional terms are considered \"further\\nrestrictions\" within the meaning of section 10.  If the Program as you\\nreceived it, or any part of it, contains a notice stating that it is\\ngoverned by this License along with a term that is a further\\nrestriction, you may remove that term.  If a license document contains\\na further restriction but permits relicensing or conveying under this\\nLicense, you may add to a covered work material governed by the terms\\nof that license document, provided that the further restriction does\\nnot survive such relicensing or conveying.\\n\\n  If you add terms to a covered work in accord with this section, you\\nmust place, in the relevant source files, a statement of the\\nadditional terms that apply to those files, or a notice indicating\\nwhere to find the applicable terms.\\n\\n  Additional terms, permissive or non-permissive, may be stated in the\\nform of a separately written license, or stated as exceptions;\\nthe above requirements apply either way.\\n\\n  8. Termination.\\n\\n  You may not propagate or modify a covered work except as expressly\\nprovided under this License.  Any attempt otherwise to propagate or\\nmodify it is void, and will automatically terminate your rights under\\nthis License (including any patent licenses granted under the third\\nparagraph of section 11).\\n\\n  However, if you cease all violation of this License, then your\\nlicense from a particular copyright holder is reinstated (a)\\nprovisionally, unless and until the copyright holder explicitly and\\nfinally terminates your license, and (b) permanently, if the copyright\\nholder fails to notify you of the violation by some reasonable means\\nprior to 60 days after the cessation.\\n\\n  Moreover, your license from a particular copyright holder is\\nreinstated permanently if the copyright holder notifies you of the\\nviolation by some reasonable means, this is the first time you have\\nreceived notice of violation of this License (for any work) from that\\ncopyright holder, and you cure the violation prior to 30 days after\\nyour receipt of the notice.\\n\\n  Termination of your rights under this section does not terminate the\\nlicenses of parties who have received copies or rights from you under\\nthis License.  If your rights have been terminated and not permanently\\nreinstated, you do not qualify to receive new licenses for the same\\nmaterial under section 10.\\n\\n  9. Acceptance Not Required for Having Copies.\\n\\n  You are not required to accept this License in order to receive or\\nrun a copy of the Program.  Ancillary propagation of a covered work\\noccurring solely as a consequence of using peer-to-peer transmission\\nto receive a copy likewise does not require acceptance.  However,\\nnothing other than this License grants you permission to propagate or\\nmodify any covered work.  These actions infringe copyright if you do\\nnot accept this License.  Therefore, by modifying or propagating a\\ncovered work, you indicate your acceptance of this License to do so.\\n\\n  10. Automatic Licensing of Downstream Recipients.\\n\\n  Each time you convey a covered work, the recipient automatically\\nreceives a license from the original licensors, to run, modify and\\npropagate that work, subject to this License.  You are not responsible\\nfor enforcing compliance by third parties with this License.\\n\\n  An \"entity transaction\" is a transaction transferring control of an\\norganization, or substantially all assets of one, or subdividing an\\norganization, or merging organizations.  If propagation of a covered\\nwork results from an entity transaction, each party to that\\ntransaction who receives a copy of the work also receives whatever\\nlicenses to the work the party\\'s predecessor in interest had or could\\ngive under the previous paragraph, plus a right to possession of the\\nCorresponding Source of the work from the predecessor in interest, if\\nthe predecessor has it or can get it with reasonable efforts.\\n\\n  You may not impose any further restrictions on the exercise of the\\nrights granted or affirmed under this License.  For example, you may\\nnot impose a license fee, royalty, or other charge for exercise of\\nrights granted under this License, and you may not initiate litigation\\n(including a cross-claim or counterclaim in a lawsuit) alleging that\\nany patent claim is infringed by making, using, selling, offering for\\nsale, or importing the Program or any portion of it.\\n\\n  11. Patents.\\n\\n  A \"contributor\" is a copyright holder who authorizes use under this\\nLicense of the Program or a work on which the Program is based.  The\\nwork thus licensed is called the contributor\\'s \"contributor version\".\\n\\n  A contributor\\'s \"essential patent claims\" are all patent claims\\nowned or controlled by the contributor, whether already acquired or\\nhereafter acquired, that would be infringed by some manner, permitted\\nby this License, of making, using, or selling its contributor version,\\nbut do not include claims that would be infringed only as a\\nconsequence of further modification of the contributor version.  For\\npurposes of this definition, \"control\" includes the right to grant\\npatent sublicenses in a manner consistent with the requirements of\\nthis License.\\n\\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\\npatent license under the contributor\\'s essential patent claims, to\\nmake, use, sell, offer for sale, import and otherwise run, modify and\\npropagate the contents of its contributor version.\\n\\n  In the following three paragraphs, a \"patent license\" is any express\\nagreement or commitment, however denominated, not to enforce a patent\\n(such as an express permission to practice a patent or covenant not to\\nsue for patent infringement).  To \"grant\" such a patent license to a\\nparty means to make such an agreement or commitment not to enforce a\\npatent against the party.\\n\\n  If you convey a covered work, knowingly relying on a patent license,\\nand the Corresponding Source of the work is not available for anyone\\nto copy, free of charge and under the terms of this License, through a\\npublicly available network server or other readily accessible means,\\nthen you must either (1) cause the Corresponding Source to be so\\navailable, or (2) arrange to deprive yourself of the benefit of the\\npatent license for this particular work, or (3) arrange, in a manner\\nconsistent with the requirements of this License, to extend the patent\\nlicense to downstream recipients.  \"Knowingly relying\" means you have\\nactual knowledge that, but for the patent license, your conveying the\\ncovered work in a country, or your recipient\\'s use of the covered work\\nin a country, would infringe one or more identifiable patents in that\\ncountry that you have reason to believe are valid.\\n\\n  If, pursuant to or in connection with a single transaction or\\narrangement, you convey, or propagate by procuring conveyance of, a\\ncovered work, and grant a patent license to some of the parties\\nreceiving the covered work authorizing them to use, propagate, modify\\nor convey a specific copy of the covered work, then the patent license\\nyou grant is automatically extended to all recipients of the covered\\nwork and works based on it.\\n\\n  A patent license is \"discriminatory\" if it does not include within\\nthe scope of its coverage, prohibits the exercise of, or is\\nconditioned on the non-exercise of one or more of the rights that are\\nspecifically granted under this License.  You may not convey a covered\\nwork if you are a party to an arrangement with a third party that is\\nin the business of distributing software, under which you make payment\\nto the third party based on the extent of your activity of conveying\\nthe work, and under which the third party grants, to any of the\\nparties who would receive the covered work from you, a discriminatory\\npatent license (a) in connection with copies of the covered work\\nconveyed by you (or copies made from those copies), or (b) primarily\\nfor and in connection with specific products or compilations that\\ncontain the covered work, unless you entered into that arrangement,\\nor that patent license was granted, prior to 28 March 2007.\\n\\n  Nothing in this License shall be construed as excluding or limiting\\nany implied license or other defenses to infringement that may\\notherwise be available to you under applicable patent law.\\n\\n  12. No Surrender of Others\\' Freedom.\\n\\n  If conditions are imposed on you (whether by court order, agreement or\\notherwise) that contradict the conditions of this License, they do not\\nexcuse you from the conditions of this License.  If you cannot convey a\\ncovered work so as to satisfy simultaneously your obligations under this\\nLicense and any other pertinent obligations, then as a consequence you may\\nnot convey it at all.  For example, if you agree to terms that obligate you\\nto collect a royalty for further conveying from those to whom you convey\\nthe Program, the only way you could satisfy both those terms and this\\nLicense would be to refrain entirely from conveying the Program.\\n\\n  13. Remote Network Interaction; Use with the GNU General Public License.\\n\\n  Notwithstanding any other provision of this License, if you modify the\\nProgram, your modified version must prominently offer all users\\ninteracting with it remotely through a computer network (if your version\\nsupports such interaction) an opportunity to receive the Corresponding\\nSource of your version by providing access to the Corresponding Source\\nfrom a network server at no charge, through some standard or customary\\nmeans of facilitating copying of software.  This Corresponding Source\\nshall include the Corresponding Source for any work covered by version 3\\nof the GNU General Public License that is incorporated pursuant to the\\nfollowing paragraph.\\n\\n  Notwithstanding any other provision of this License, you have\\npermission to link or combine any covered work with a work licensed\\nunder version 3 of the GNU General Public License into a single\\ncombined work, and to convey the resulting work.  The terms of this\\nLicense will continue to apply to the part which is the covered work,\\nbut the work with which it is combined will remain governed by version\\n3 of the GNU General Public License.\\n\\n  14. Revised Versions of this License.\\n\\n  The Free Software Foundation may publish revised and/or new versions of\\nthe GNU Affero General Public License from time to time.  Such new versions\\nwill be similar in spirit to the present version, but may differ in detail to\\naddress new problems or concerns.\\n\\n  Each version is given a distinguishing version number.  If the\\nProgram specifies that a certain numbered version of the GNU Affero General\\nPublic License \"or any later version\" applies to it, you have the\\noption of following the terms and conditions either of that numbered\\nversion or of any later version published by the Free Software\\nFoundation.  If the Program does not specify a version number of the\\nGNU Affero General Public License, you may choose any version ever published\\nby the Free Software Foundation.\\n\\n  If the Program specifies that a proxy can decide which future\\nversions of the GNU Affero General Public License can be used, that proxy\\'s\\npublic statement of acceptance of a version permanently authorizes you\\nto choose that version for the Program.\\n\\n  Later license versions may give you additional or different\\npermissions.  However, no additional obligations are imposed on any\\nauthor or copyright holder as a result of your choosing to follow a\\nlater version.\\n\\n  15. Disclaimer of Warranty.\\n\\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\\n\\n  16. Limitation of Liability.\\n\\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\\nSUCH DAMAGES.\\n\\n  17. Interpretation of Sections 15 and 16.\\n\\n  If the disclaimer of warranty and limitation of liability provided\\nabove cannot be given local legal effect according to their terms,\\nreviewing courts shall apply local law that most closely approximates\\nan absolute waiver of all civil liability in connection with the\\nProgram, unless a warranty or assumption of liability accompanies a\\ncopy of the Program in return for a fee.\\n\\n                     END OF TERMS AND CONDITIONS\\n\\n            How to Apply These Terms to Your New Programs\\n\\n  If you develop a new program, and you want it to be of the greatest\\npossible use to the public, the best way to achieve this is to make it\\nfree software which everyone can redistribute and change under these terms.\\n\\n  To do so, attach the following notices to the program.  It is safest\\nto attach them to the start of each source file to most effectively\\nstate the exclusion of warranty; and each file should have at least\\nthe \"copyright\" line and a pointer to where the full notice is found.\\n\\n    &lt;one line to give the program\\'s name and a brief idea of what it does.&gt;\\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\\n\\n    This program is free software: you can redistribute it and/or modify\\n    it under the terms of the GNU Affero General Public License as published by\\n    the Free Software Foundation, either version 3 of the License, or\\n    (at your option) any later version.\\n\\n    This program is distributed in the hope that it will be useful,\\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n    GNU Affero General Public License for more details.\\n\\n    You should have received a copy of the GNU Affero General Public License\\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\\n\\nAlso add information on how to contact you by electronic and paper mail.\\n\\n  If your software can interact with users remotely through a computer\\nnetwork, you should also make sure that it provides a way for users to\\nget its source.  For example, if your program is a web application, its\\ninterface could display a \"Source\" link that leads users to an archive\\nof the code.  There are many ways you could offer source, and different\\nsolutions will be better for different programs; see section 13 for the\\nspecific requirements.\\n\\n  You should also get your employer (if you work as a programmer) or school,\\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\\nFor more information on this, and how to apply and follow the GNU AGPL, see\\n&lt;https://www.gnu.org/licenses/&gt;.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='8388c874984b293d0a0ea3a3c44950333ce8237f', embedding=None, metadata={'file_path': 'README.md', 'file_name': 'README.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nSmall World Propensity\\n![DOI](https://doi.org/10.5281/zenodo.10299681)\\n![GitHub release](https://GitHub.com/rkdan/small_world_propensity/releases)\\n![PyPI pyversions](https://pypi.python.org/pypi/small-world-propensity/)\\n![Code style: black](https://github.com/psf/black)\\n![Imports: isort](https://pycqa.github.io/isort/)\\n\\nThis python package was adapted from the MATLAB package as first presented in Small-World Propensity and Weighted Brain Networks (2016) by Sarah Feldt Muldoon, Eric W. Bridgeford &amp; Danielle S. Bassett. Their original MATLAB implementation can be found here.\\n\\n\\n\\n\\nUse\\nThe small-world propensity package can be installed using pip\\n```\\npython -m pip install small-world-propensity\\n```\\n`small_world_propensity` can be called in two ways: either with a single adjacency matrix, or with a list of adjacency matrices and a boolean list denoting whether each matrix is binary or not. In either case, `small_world_propensity` will return a `pandas` dataframe similar to the following:\\n!Dataframe\\n\\n\\n\\n\\nGeneration of regular and random matrices\\nUsing the structural network of the cat cortex obtained from tract-tracing studies between 52 brain regions, we can visualize the process behind the calculation of the small-world propensity, $\\\\phi$. The matrix is loaded using\\n\\n```\\ncat = sio.loadmat(\\'data/cat.mat\\')[\\'CIJctx\\']\\n```\\nWe can then ensure symmetry by calling\\n```\\nsymm_cat = swp.make_symmetric(cat)\\n```\\nIn order to get the regular version of the cat matrix, we first find the effective average radius:\\n```\\nr = swp.get_avg_rad_eff(symm_cat)\\ncat_reg = swp.regular_matrix_generator(symm_cat, r)\\n```\\nFinally we produce the randomized cat matrix:\\n```\\ncat_rand = swp.randomize_matrix(cat_symm)\\n```\\n!Cat matrices\\n\\nThe graphs visualized in a circular layout look as follows:\\n\\n!Cat graphs\\n\\n\\n\\n\\nComparison of $\\\\phi$ in real networks\\nWe can take the networks used in _Muldoon et al_ and plot $\\\\phi$, $\\\\Delta_L$, $\\\\Delta_C$, and $\\\\delta$. Note that these networks are not the exact same as the ones used in _Muldoon et al_, and due to differences in how Numpy performs permutations, and the use of NetworkX and iGraph libraries, the results are not identical, but still match closely.\\n\\nThe adjacency matrices:\\n!Adjacency matrices\\n\\nAnd the results:\\n!Summary\\n\\nTo cite this work, please use:\\n```bibtex\\n@software{small-world-propensity,\\n  author       = {{Daniels, R. K.}},\\n  title        = {small-world-propensity},\\n  year         = 2023,\\n  publisher    = {Zenodo},\\n  version      = {v0.0.8},\\n  doi          = {10.5281/zenodo.10299681},\\n  url          = {https://github.com/rkdan/small-world-propensity}\\n}\\n```\\nPlease also cite the authors of the original MATLAB implementation:\\n```bibtex\\n@article{Muldoon2016,\\n    author = \"Muldoon, Sarah Feldt and Bridgeford, Eric W. and Bassett, Danielle S.\",\\n    title = \"{Small-World Propensity and Weighted Brain Networks}\",\\n    doi = \"10.1038/srep22057\",\\n    journal = \"Scientific Reports\",\\n    volume = \"6\",\\n    number = \"1\",\\n    pages = \"P07027\",\\n    year = \"2016\"\\n}\\n```\\n\\n&gt; [!NOTE]  \\n&gt; This software has a GNU AGPL license. If this license is inadequate for your use, please get in touch.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='873ebe34642eff2cf48a91ba967054627683eea9', embedding=None, metadata={'file_path': 'img/cat.png', 'file_name': 'img/cat.png'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='be07e48ce08e7f40f2fba016f845b7ed1e52a447', embedding=None, metadata={'file_path': 'img/cat_graphs.png', 'file_name': 'img/cat_graphs.png'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='6683837b8e6014f4fb52542134e5301873383586', embedding=None, metadata={'file_path': 'img/dataframe.png', 'file_name': 'img/dataframe.png'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='2c8cfc8c0e55fd32c38e707fbde06123da626fd4', embedding=None, metadata={'file_path': 'img/matrices.png', 'file_name': 'img/matrices.png'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='8dee77c8675eafdcd7a3100eb03f4dffa9741af5', embedding=None, metadata={'file_path': 'img/summary.png', 'file_name': 'img/summary.png'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='70fd0e7b27c365660722ffb079d74a3ad97645f6', embedding=None, metadata={'file_path': 'pyproject.toml', 'file_name': 'pyproject.toml', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/pyproject.toml'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[tool.poetry]\\nname = \"small-world-propensity\"\\nversion = \"0.0.19\"\\ndescription = \"A small python package designed to calculate the small-world propensity of a weighted, undirected network. Translated from the MATLAB version featured in Muldoon et al.\"\\nauthors = [\"Ryan Daniels\"]\\nrepository = \"https://github.com/rkdan/small_world_propensity\"\\nlicense = \"GPL-3.0-only\"\\nreadme = \"README.md\"\\npackages = [{include = \"small_world_propensity\"}]\\n\\n[tool.poetry.dependencies]\\npython = \"^3.10\"\\nnumpy = \"^1.24.3\"\\npandas = \"^2.0.2\"\\ntqdm = \"^4.65.0\"\\n\\n[tool.poetry.group.dev.dependencies]\\nblack = \"^22.8.0\"\\npython-semantic-release = \"^7.34.4\"\\n\\n[tool.black]\\nline-length = 100\\ninclude = \\'\\\\.pyi?$\\'\\nexclude = \\'\\'\\'\\n/(\\n      .eggs\\n    | .git\\n    | .hg\\n    | .mypy_cache\\n    | .tox\\n    | .env\\n    | _build\\n    | buck-out\\n    | build\\n    | dist\\n  )/\\n\\'\\'\\'\\n\\n[tool.isort]\\nprofile = \"black\"\\nline_length = 79\\nmulti_line_output = 3\\ninclude_trailing_comma = true\\nvirtual_env = \".env\"\\nskip = \".eggs, .git, .hg, .mypy_cache, .tox, .env, _build, buck-out, build, dist\"\\n\\n[tool.semantic_release]\\nversion_variable = [\\n    \"small_world_propensity/__init__.py:__version__\",\\n    \"pyproject.toml:version\",\\n]\\nbranch = \"main\"                             # branch to make releases of\\nchangelog_file = \"CHANGELOG.md\"             # changelog file\\nbuild_command = \"poetry build\"              # build dists\\ndist_path = \"dist/\"                         # where to put dists\\nupload_to_release = true                    # auto-create GitHub release\\nupload_to_pypi = false                      # don\\'t auto-upload to PyPI\\nremove_dist = false                         # don\\'t remove dists\\npatch_without_tag = true                    # patch release by default\\nversion_source = \"tag\"                   # versioning source\\n\\n[build-system]\\nrequires = [\"poetry-core\"]\\nbuild-backend = \"poetry.core.masonry.api\"\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='1c2c8241979f6d42c3495e80f06f9480a953c725', embedding=None, metadata={'file_path': 'small_world_propensity.m', 'file_name': 'small_world_propensity.m', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/small_world_propensity.m'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='function [SWP,delta_C,delta_L] = small_world_propensity(A, varargin)\\r\\n\\r\\n% a function for calculating the small world propensity of\\r\\n% a given network - assumes that matrix is undirected (symmeteric) and if\\r\\n% not, creates a symmetric matrix which is used for the calculations\\r\\n\\r\\n%NOTE:  This code requires the Bioinformatics Toolbox to be installed\\r\\n%        (uses graphallshortestpaths.m)\\r\\n\\r\\n%Inputs:\\r\\n%   A           the connectivity matrix, weighted or binary\\r\\n%   varargin    a string corresponding to the method of clustering\\r\\n%               to be used, where \\'O\\' is Onnela, \\'Z\\' is Zhang, \\r\\n%               \\'B\\' is Barrat, \\'bin\\' is binary (default is Onnela).\\r\\n%               If the user specifies binary analysis, a \\r\\n%               weighted matrix will be converted to a binary matrix \\r\\n%               before proceeding.\\r\\n%        \\r\\n\\r\\n%Outputs:\\r\\n%   SWP         the small world propensity of the matrix\\r\\n%   delta_C     the fractional deviation from the expected culstering coefficient of a\\r\\n%                   random network\\r\\n%   delta_L     the fractional deviation from the expected path length of a\\r\\n%                   random network\\r\\n\\r\\n%written by Eric Bridgeford and modified by Sarah F. Muldoon\\r\\n\\r\\n% Reference: Muldoon, Bridgeford, and Bassett (2015) \"Small-World Propensity in Weighted, \\r\\n%               Real-World Networks\" http://arxiv.org/abs/1505.02194\\r\\n\\r\\nif isempty(varargin)\\r\\n    varargin{1} = \\'O\\';\\r\\nend\\r\\n\\r\\nif sum(sum(A)) &gt; 0\\r\\n    \\r\\nbin_matrix = 0;\\r\\nif strcmp(varargin{1},\\'bin\\') == 1\\r\\n   bin_matrix = 1;\\r\\n   A = A &gt; 0;\\r\\nend\\r\\n\\r\\n%check to see if matrix is symmeteric\\r\\nsymcheck=abs(A-A\\');\\r\\nif sum(sum(symcheck)) &gt; 0\\r\\n    % adjust the input matrix to symmeterize\\r\\n    disp(\\'Input matrix is not symmetric. Symmetrizing.\\')\\r\\n    W = symm_matrix(A, bin_matrix);\\r\\nelse\\r\\n    W=A;\\r\\nend\\r\\n\\r\\n%calculate the number of nodes\\r\\nn = length(W);  \\r\\n%compute the weighted density of the network\\r\\ndens_net = sum(sum(W))/(max(max(W))*n*(n-1));\\r\\n\\r\\n%compute the average degree of the unweighted network, to give\\r\\n%the approximate radius\\r\\nnumb_connections = length(find(W&gt;0));\\r\\navg_deg_unw = numb_connections/n;\\r\\navg_rad_unw = avg_deg_unw/2;\\r\\navg_rad_eff = ceil(avg_rad_unw);\\r\\n\\r\\n\\r\\n%compute the regular and random matrix for the network W\\r\\nW_reg = regular_matrix_generator(W, avg_rad_eff);\\r\\nW_rand = randomize_matrix(W);\\r\\n%compute all path length calculations for the network\\r\\nreg_path = avg_path_matrix(1./W_reg);      %path of the regular network\\r\\nrand_path = avg_path_matrix(1./W_rand);    %path of the random netowork\\r\\nnet_path = avg_path_matrix(1./W);          %path of the network\\r\\n\\r\\nA = (net_path - rand_path);\\r\\nif A &lt; 0\\r\\n    A = 0;\\r\\nend\\r\\ndiff_path =  A/ (reg_path - rand_path);\\r\\nif net_path == Inf || rand_path == Inf || reg_path == Inf\\r\\n    diff_path = 1;\\r\\nend\\r\\nif diff_path &gt; 1\\r\\n    diff_path = 1;\\r\\nend\\r\\n\\r\\n\\r\\n%compute all clustering calculations for the network\\r\\nreg_clus = avg_clus_matrix(W_reg,varargin{1});\\r\\nrand_clus = avg_clus_matrix(W_rand,varargin{1});\\r\\nnet_clus = avg_clus_matrix(W,varargin{1});\\r\\n\\r\\nB = (reg_clus - net_clus);\\r\\nif B &lt; 0\\r\\n    B = 0;\\r\\nend\\r\\n    \\r\\ndiff_clus = B / (reg_clus - rand_clus);\\r\\nif isnan(reg_clus) || isnan(rand_clus) || isnan(net_clus)\\r\\n    diff_clus = 1;\\r\\nend\\r\\nif diff_clus &gt; 1\\r\\n    diff_clus = 1;\\r\\nend\\r\\n\\r\\n%calculate small world value, the root sum of the squares of\\r\\n%diff path and diff clus\\r\\nSWP = 1 - (sqrt(diff_clus^2 + diff_path^2)/sqrt(2));\\r\\ndelta_C=diff_clus;\\r\\ndelta_L=diff_path;\\r\\nend\\r\\nend\\r\\n\\r\\n\\r\\n%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\r\\n%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\r\\n%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\r\\n%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\r\\n%the above code requires the following funcitons\\r\\n\\r\\nfunction [Clus] = avg_clus_matrix(W, met)\\r\\n%a function to compute the average clusteirng coefficient for a \\r\\n%input matrix M\\r\\n\\r\\n%Inputs:\\r\\n%   W     a matrix, weighted or unweighted\\r\\n%   met   a string, to represent the method to be used for computing\\r\\n%         the clustering coefficient\\r\\n%         possible strings: \\'O\\' (Onnela), \\'Z\\' (Zhang), \\'B\\' (Barrat),\\r\\n%         \\'bin\\' (binary)\\r\\n%         default if none is chosen is Onnela\\r\\n\\r\\n%Outputs:\\r\\n%   Clus  the average clustering coefficient\\r\\n\\r\\n%written by Eric Bridgeford\\r\\n\\r\\nn = length(W);\\r\\n[C] = clustering_coef_matrix(W, met);\\r\\nClus = nanmean(C);\\r\\nend\\r\\n\\r\\n\\r\\nfunction [Len] = avg_path_matrix(M)\\r\\n\\r\\n%a function to compute the average path length of a given matrix\\r\\n%using the graphallshortestpaths built-in matlab function\\r\\n\\r\\n%written by Eric Bridgeford\\r\\n\\r\\nn = length(M);\\r\\nM = sparse(M);\\r\\nD = graphallshortestpaths(M);\\r\\n\\r\\n%checks if a node is disconnected from the system, and replaces\\r\\n%its value with 0\\r\\nfor i = 1:n\\r\\n    for j = 1:n\\r\\n        if isinf(D(i,j)) == 1\\r\\n            D(i,j) = 0;\\r\\n        end\\r\\n    end\\r\\nend\\r\\n\\r\\nLen = mean(mean(D));\\r\\nend\\r\\n\\r\\n\\r\\nfunction [C] = clustering_coef_matrix(W, met)\\r\\n\\r\\n%a modification of the clustering coefficient function provided\\r\\n%in the brain connectivity toolbox\\r\\n\\r\\n%improved definition of Onnela Clustering Coefficient, as well as\\r\\n%implementation of function for Zhang and Barrat clustering values\\r\\n\\r\\n%Reference: \\r\\n%   Onnela et al., Phys. Rev. E71, 065103(R)(2005)\\r\\n%   B.Zhang and S. Horvath, Stat. App. Genet. Mol. Biol.4, 17(2005)\\r\\n%   Barrat et al., Proc. Natl. Acad. Sci. U.S.A.101, 3747(2004)\\r\\n%   Watts and Strogatz (1998) Nature 393:440-442\\r\\n\\r\\n%Inputs:\\r\\n%   W    the weighted or unweighted connectivity matrix\\r\\n%   met   a string, to represent the method to be used for computing\\r\\n%         the clustering coefficient\\r\\n%         possible strings: \\'O\\' (Onnela), \\'Z\\' (Zhang), \\'B\\' (Barrat), \\'bin\\'\\r\\n%         (binary)\\r\\n%         default if none is chosen is Onnela\\r\\n\\r\\n\\r\\n%code originally written by Mika Rubinov, UNSW, 2007-2010\\r\\n%modified/written by Eric Bridgeford\\r\\n\\r\\nif met == \\'O\\'\\r\\n    K=sum(W~=0,2);\\r\\n    W = double(W);\\r\\n    W2 = W/max(max(W));\\r\\n    cyc3=diag(W2.^(1/3)^3);\\r\\n    K(cyc3==0)=inf;             %if no 3-cycles exist, make C=0 (via K=inf)\\r\\n    C=cyc3./(K.*(K-1));\\r\\nend\\r\\nif met == \\'bin\\'\\r\\n    G = double(W&gt;0);\\r\\n    n=length(G);\\r\\n    C=zeros(n,1);\\r\\n    for u=1:n\\r\\n        V=find(G(u,:));\\r\\n        k=length(V);\\r\\n        if k&gt;=2;                %degree must be at least 2\\r\\n            S=G(V,V);\\r\\n            C(u)=sum(S(:))/(k^2-k);\\r\\n        end\\r\\n    end\\r\\nend\\r\\n\\r\\nif met == \\'Z\\'\\r\\n    K=sum(W~=0,2);\\r\\n    W = double(W);\\r\\n    W2 = W/max(max((W)));\\r\\n    cyc3=diag((W2)^3);\\r\\n    denom = zeros(length(W),1);\\r\\n    for i = 1:length(W)\\r\\n        denom(i) = (sum(W2(i,:))^2-sum(W2(i,:).^2));\\r\\n    end\\r\\n    C = cyc3./denom;\\r\\nend\\r\\n\\r\\nif met == \\'B\\'\\r\\n    A = double(W&gt;0);\\r\\n    C = zeros(length(W),1);\\r\\n    for i = 1:length(W)\\r\\n        sum1 = 0;\\r\\n        for j = 1:length(W)\\r\\n            for k = 1:length(W)\\r\\n                sum1 = ((W(i,j)+W(i,k))/2)*A(i,j)*A(j,k)*A(i,k)+sum1;\\r\\n            end\\r\\n        end\\r\\n        C(i) = 1/(sum(W(i,:))*(sum(A(i,:))-1))*sum1;\\r\\n    end\\r\\nend\\r\\n\\r\\n\\r\\nend\\r\\n\\r\\n\\r\\nfunction A_rand=randomize_matrix(A);\\r\\n\\r\\n%This code creates a random undirected network from the connectivity\\r\\n%distribution of an undirected adjacency matrix, ie, the intital matrix\\r\\n%must be symmetric.\\r\\n\\r\\n% INPUTS:\\r\\n%   A: an undirected adjacency matrix (symmetric) with no self connections\\r\\n\\r\\n% OUTPUTS:\\r\\n%   A_rand: a comparable random network with same number of nodes and\\r\\n%       connectivity distribution\\r\\n\\r\\n% written by Sarah F. Muldoon\\r\\n\\r\\nnum_nodes=length(A);\\r\\nA_rand=zeros(num_nodes);\\r\\nmask=triu(ones(num_nodes),1);\\r\\ngrab_indices=find(mask &gt; 0);\\r\\n\\r\\norig_edges=A(grab_indices);\\r\\nnum_edges=length(orig_edges);\\r\\n\\r\\nrand_index=randperm(num_edges);\\r\\nrandomized_edges=orig_edges(rand_index);\\r\\n\\r\\nedge=1;\\r\\nfor i=1:num_nodes-1\\r\\n    for j=i+1:num_nodes\\r\\n        A_rand(i,j)=randomized_edges(edge);\\r\\n        A_rand(j,i)=randomized_edges(edge);\\r\\n        edge=edge+1;\\r\\n    end\\r\\nend\\r\\nend\\r\\n\\r\\n        \\r\\nfunction M = regular_matrix_generator(G,r)\\r\\n%generates a regular matrix, with weights obtained form the \\r\\n%original adjacency matrix representation of the network\\r\\n\\r\\n% note that all inputs should be symmeterized prior to forming a regular\\r\\n% matrix, since otherwise half of the connnections will be trashed. This\\r\\n% can be accomplished with the built in symm_matrix function, however,\\r\\n% the function is not performed here so that users can use their own \\r\\n% symmeterization procedure.\\r\\n\\r\\n%Inputs:\\r\\n%   G    the adjacency matrix for the given network; must be symmmeterized\\r\\n%   r    the approximate radius of the regular network \\r\\n\\r\\n%Outputs:\\r\\n%   M    the regular matrix for the given network, where all \\r\\n%        weights are sorted such that the inner radius has the\\r\\n%        highest weights randomly distributed across the nodes, \\r\\n%        and so on\\r\\n\\r\\n%written by Eric W. Bridgeford \\r\\n\\r\\nn = length(G);\\r\\nG = triu(G);\\r\\n%reshape the matrix G into an array, B\\r\\nB = reshape(G,[length(G)^2,1]);\\r\\n%sorts the array in descending order\\r\\nB = sort(B,\\'descend\\');\\r\\n%computes the number of connections and adds zeros if \\r\\n%numel(G) &lt; 2*n*r\\r\\nnum_els =ceil(numel(G)/(2*n));\\r\\nnum_zeros = 2*n*num_els - numel(G);\\r\\n%adds zeros to the remaineder of the list, so length(B) = 2*n*r\\r\\nB = cat(1,B,zeros(num_zeros,1));\\r\\n%reshapes B into a matrix, where the values descend top to\\r\\n%bottom, as well as left to right. The greatest value in each \\r\\n%column is less than the smallest value of the column to its left.\\r\\nB = reshape(B,[n],[]);\\r\\n\\r\\nM = zeros(length(G));\\r\\n\\r\\n%distributes the connections into a regular network, M, where\\r\\n%the innermost radius represents the highest values\\r\\nfor i = 1:length(G)\\r\\n    for z = 1:r\\r\\n        a = randi([1,n]);\\r\\n\\r\\n        %random integer chosen to take a value from B\\r\\n        while (B(a,z) == 0 &amp;&amp; z ~= r) || (B(a,z) == 0 &amp;&amp; z == r &amp;&amp; ~isempty(find(B(:,r),1)))\\r\\n            a = randi([1,n]);\\r\\n        end\\r\\n        %finds the two nodes a distance of z from the origin node\\r\\n        %and places the entries from the matrix B\\r\\n        y_coor_1 = mod(i+z-1,length(G))+1;\\r\\n        %y_coor_2 = mod(i-z-1,length(G))+1;\\r\\n        M(i,y_coor_1) = B(a,z);\\r\\n        M(y_coor_1,i) = B(a,z);\\r\\n        %removes the weights from the matrix B so they cannot be\\r\\n        %reused\\r\\n        B(a,z) = 0;      \\r\\n        \\r\\n    end\\r\\nend\\r\\nend\\r\\n\\r\\n\\r\\nfunction [W] = symm_matrix(A, bin_key)\\r\\n\\r\\n% a function to symmetrize an input matrix. The procedure\\r\\n% by which items are symmetrized such that:\\r\\n%   in the binary case:\\r\\n%       if a(i,j) || a(j,i) == 1 for i,j in A\\r\\n%           w(i,j) &amp;&amp; w(j,i) == 1\\r\\n%   in  the weighted case:\\r\\n%       if (a(i,j) || a(j,i) &gt; 0 for i,j in A\\r\\n%           w(i,j) &amp;&amp; w(j,i) == (a(i,j) + a(j,i) )/ 2\\r\\n\\r\\n% Inputs:\\r\\n%   A:          The binary or weighted input matrix\\r\\n%   bin_key:    the key to indicate whether weighted or binary analysis\\r\\n%               will take place\\r\\n%               1 indicates binarized, 0 indicates weighted\\r\\n\\r\\n% Outputs\\r\\n%   W:          The symmeterized matrix\\r\\n\\r\\n% if binary analysis is specified, let binary symmeterization take place \\r\\n\\r\\n% written by Eric W. Bridgeford\\r\\n\\r\\nW = zeros(length(A));\\r\\n\\r\\nif bin_key == 1\\r\\n    A = A &gt; 0; % verify that the input matrix is binary\\r\\n    for i = 1:length(A)\\r\\n        for j = i:length(A)\\r\\n            if A(i,j) || A(j,i)\\r\\n                W(i,j) = 1;\\r\\n                W(j,i) = 1;\\r\\n            end\\r\\n        end\\r\\n    end\\r\\nelse\\r\\n    for i = 1:length(A)\\r\\n        for j = i:length(A)\\r\\n            if A(i,j) || A(j,i)\\r\\n                val = (A(i,j) + A(j,i)) / 2;\\r\\n                W(i,j) = val;\\r\\n                W(j,i) = val;\\r\\n            end\\r\\n        end\\r\\n    end\\r\\nend\\r\\nend\\r\\n    \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='a27dd19c15d9c038ec91b61bf5976d7834c1a9a9', embedding=None, metadata={'file_path': 'small_world_propensity/__init__.py', 'file_name': '__init__.py', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/small_world_propensity/__init__.py'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='from .small_world_propensity import *\\n\\n# version\\n__version__ = \\'0.0.13\\'\\n\\n__all__ = [\"small_world_propensity\",\\n           \"get_avg_rad_eff\",\\n           \"get_average_paths\",\\n           \"get_clustering_coefficient\",\\n           \"randomize_matrix\",\\n           \"regular_matrix_generator\",\\n           \"make_symmetric\"]\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='c036a0570326b612e29ccc9d66bcb52c750f2227', embedding=None, metadata={'file_path': 'small_world_propensity/small_world_propensity.py', 'file_name': 'small_world_propensity.py', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/small_world_propensity/small_world_propensity.py'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Probably a bad idea...\\nimport warnings\\nfrom typing import Union\\n\\nimport numpy as np\\nimport pandas as pd\\nimport tqdm\\nfrom scipy.sparse import csgraph\\n\\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) \\n\\ngen = np.random.default_rng(1337)\\n\\ndef small_world_propensity(\\n    W: Union[np.ndarray, list], bin: Union[bool, list] = False\\n) -&gt; pd.DataFrame:\\n    \"\"\"Find the small-world propensity and related measures of a network W.\\n\\n    Args:\\n        W (Union[np.ndarray, list]): Either an adjacency matrix or a list of adjacency matrices.\\n        bin (Union[bool, list], optional): Indicated whether the matrix (list of matrices) is (are) binary.\\n            Defaults to False.\\n\\n    Returns:\\n        pd.DataFrame: Dataframe containing the small-world propensity and related measures.\\n    \"\"\"\\n    # Check if W is a list of matrices\\n    if isinstance(W, list):\\n        df = pd.DataFrame()\\n        for i in tqdm.tqdm(range(len(W))):\\n            df_temp = _small_world_propensity(W[i], bin=bin[i])\\n            df = pd.concat([df, df_temp], ignore_index=True)\\n        return df\\n    else:\\n        df = _small_world_propensity(W, bin=bin)\\n        return df\\n\\n\\ndef get_avg_rad_eff(W: np.ndarray) -&gt; int:\\n    n = len(W)\\n    num_con = len(np.where(W &gt; 0)[0])\\n    avg_deg_unw = num_con / n\\n    avg_rad_unw = avg_deg_unw / 2\\n    avg_rad_eff = np.ceil(avg_rad_unw)\\n\\n    return int(avg_rad_eff)\\n\\n\\ndef get_average_paths(W: np.ndarray) -&gt; float:\\n    \"\"\"Get the average path length of a network.\\n\\n    Args:\\n        W (np.ndarray): Adjacency matrix of the network.\\n\\n    Returns:\\n        float: Average path length of the network.\\n    \"\"\"\\n    path_matrix = csgraph.shortest_path(1/W, directed=False, unweighted=False)\\n    path_matrix[np.isinf(path_matrix)] = 0\\n    L_W = np.triu(path_matrix).sum() / (len(W) * (len(W) - 1) / 2)\\n\\n    return L_W\\n\\n\\ndef get_clustering_coefficient(W: np.ndarray) -&gt; float:\\n    \"\"\"Get the clustering coefficient of a network.\\n\\n    Args:\\n        W (np.ndarray): Adjacency matrix of the network.\\n\\n    Returns:\\n        float: Clustering coefficient of the network.\\n    \"\"\"\\n    K = np.where(W &gt; 0, 1, 0).sum(axis=1)\\n    W2 = W / W.max()\\n    cyc3 = np.diagonal(np.linalg.matrix_power(W2 ** (1/3), 3))\\n    K = np.where(cyc3 == 0, np.inf, K)\\n    C = cyc3 / (K * K-1)\\n\\n    return C.mean()\\n\\n\\ndef _small_world_propensity(W: np.ndarray, bin: bool = False) -&gt; pd.DataFrame:\\n    \"\"\"Finds the small-world propensity and related measures of a single network W.\\n\\n    Args:\\n        W (np.ndarray): Adjacency matrix of the network.\\n        bin (bool, optional): Is the matrix binary or not. Defaults to False.\\n\\n    Returns:\\n        pd.DataFrame: Dataframe containing the small-world propensity and related measures.\\n    \"\"\"\\n\\n    # Check if the matrix is symmetric\\n    if not np.allclose(W, W.T, rtol=1e-05, atol=1e-08):\\n        W = make_symmetric(W, bin=bin)\\n\\n    W = W / np.max(W)\\n\\n    avg_rad_eff = get_avg_rad_eff(W)\\n\\n    # Make regular and random networks\\n    W_reg = regular_matrix_generator(W, int(avg_rad_eff))\\n    W_rand = randomize_matrix(W)\\n\\n    # Clustering\\n    C_W = get_clustering_coefficient(W)\\n    C_reg = get_clustering_coefficient(W_reg)\\n    C_rand = get_clustering_coefficient(W_rand)\\n\\n    # Path lengths\\n    L_W = get_average_paths(W)\\n    L_reg = get_average_paths(W_reg)\\n    L_rand = get_average_paths(W_rand)\\n\\n    # Delta L\\n    A = L_W - L_rand\\n    if A &lt; 0:\\n        A = 0\\n\\n    diff_path = A / (L_reg - L_rand)\\n\\n    if np.isinf(L_W) or np.isinf(L_reg) or np.isinf(L_rand):\\n        diff_path = 1\\n\\n    if diff_path &gt; 1:\\n        diff_path = 1\\n\\n    delta_L = diff_path\\n\\n    # Delta C\\n    B = C_reg - C_W\\n    if B &lt; 0:\\n        B = 0\\n\\n    diff_clus = B / (C_reg - C_rand)\\n    if np.isnan(C_reg) or np.isnan(C_W) or np.isnan(C_rand):\\n        diff_clus = 1\\n\\n    if diff_clus &gt; 1:\\n        diff_clus = 1\\n\\n    delta_C = diff_clus\\n    \\n    # Small-world propensity\\n    SWP = 1 - (np.sqrt((delta_C) ** 2 + (delta_L) ** 2) / np.sqrt(2))\\n\\n    alpha = np.arctan(delta_L / delta_C)\\n    delta = (4 * alpha / np.pi) - 1\\n\\n    df = pd.DataFrame(\\n        {\\n            \"Network C\": C_W,\\n            \"Network L\": L_W,\\n            \"\u0394C\": delta_C,\\n            \"\u0394L\": delta_L,\\n            \"SWP\": SWP,\\n            \"\u03b1\": alpha,\\n            \"\u03b4\": delta,\\n            \"Regular C\": C_reg,\\n            \"Random C\": C_rand,\\n            \"Regular L\": L_reg,\\n            \"Random L\": L_rand,\\n        },\\n        index=[0],\\n    )\\n\\n    return df\\n\\n\\ndef randomize_matrix(A: np.ndarray) -&gt; np.ndarray:\\n    \"\"\"Randomly rewire the edges of a network.\\n\\n    Args:\\n        A (np.ndarray): Adjacency matrix of the network.\\n\\n    Returns:\\n        np.ndarray: Adjacency matrix of the randomized network.\\n    \"\"\"\\n    num_nodes = A.shape[0]\\n    A_rand = np.zeros((num_nodes, num_nodes))\\n    mask = np.triu(np.ones((num_nodes, num_nodes)), 1)\\n\\n    # Find the indices where mask &gt; 0 in column-major order\\n    grab_indices = np.column_stack(np.nonzero(mask.T))\\n\\n    # Access A with the indices\\n    orig_edges = A[grab_indices[:, 0], grab_indices[:, 1]]\\n    num_edges = len(orig_edges)\\n    rand_index = np.random.choice(num_edges, num_edges, replace=False)\\n    randomized_edges = orig_edges[rand_index]\\n    edge = 0\\n    for i in range(num_nodes - 1):\\n        for j in range(i + 1, num_nodes):\\n            A_rand[i, j] = randomized_edges[edge]\\n            A_rand[j, i] = randomized_edges[edge]\\n            edge += 1  # Move to next edge\\n    return A_rand\\n\\n\\ndef regular_matrix_generator(G: np.ndarray, r: int) -&gt; np.ndarray:\\n    \"\"\"Generate a regular matrix from a given matrix.\\n\\n    Args:\\n        G (np.ndarray): Adjacency matrix of the network.\\n        r (int): The average effective radius of the network.\\n\\n    Returns:\\n        np.ndarray: Adjacency matrix of the regularized network.\\n    \"\"\"\\n\\n    n = len(G)\\n    G = np.triu(G)\\n    B = G.flatten(order=\"F\")\\n    B = np.sort(B)[::-1]\\n    num_els = np.ceil(len(B) / (2 * n))\\n    num_zeros = 2 * n * num_els - n * n\\n    B = np.concatenate((B, np.zeros(int(num_zeros))))\\n    B = B.reshape((n, -1), order=\"F\")\\n\\n    M = np.zeros((n, n))\\n\\n    for i in range(n):\\n        for z in range(r):\\n            a = gen.integers(0, n)\\n            while (B[a, z] == 0 and z != r - 1) or (\\n                B[a, z] == 0 and z == r - 1 and len(B[:, r - 1].nonzero()[0]) != 0\\n            ):\\n                a = gen.integers(0, n)\\n\\n            y_coord = (i + z + 1) % n\\n            M[i, y_coord] = B[a, z]\\n            M[y_coord, i] = B[a, z]\\n\\n            B[a, z] = 0\\n\\n    return M\\n\\n\\ndef make_symmetric(A: np.ndarray, bin: bool = False) -&gt; np.ndarray:\\n    \"\"\"Take an adjacency matrix and make it symmetric.\\n\\n    Args:\\n        A (np.ndarray): Adjacency matrix of the network.\\n        bin (bool, optional): Is the network binary not. Defaults to False.\\n\\n    Returns:\\n        np.ndarray: Symmetric adjacency matrix of the network.\\n    \"\"\"\\n    W = np.zeros(A.shape)\\n    if bin:\\n        for i in range(A.shape[0]):\\n            for j in range(i, A.shape[1]):\\n                if A[i, j] or A[j, i]:\\n                    W[i, j] = 1\\n                    W[j, i] = 1\\n    else:\\n        for i in range(A.shape[0]):\\n            for j in range(i, A.shape[1]):\\n                if A[i, j] or A[j, i]:\\n                    val = (A[i, j] + A[j, i]) / 2\\n                    W[i, j] = val\\n                    W[j, i] = val\\n\\n    return W\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='66335c33a68a4bbbde16767fcf657aef67d54fdf', embedding=None, metadata={'file_path': 'testing.ipynb', 'file_name': 'testing.ipynb'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\n\\nimport small_world_propensity as swp\\n\\nimport scipy.io as sio\\nimport networkx as nx\\nimport numpy as np\\n\\n\\n# \\n\\n\\n\\n\\ncat = sio.loadmat(\\'data/cat.mat\\')[\\'CIJctx\\']\\ncat = swp.make_symmetric(cat)\\nG = nx.from_numpy_array(cat)\\n\\n\\n# \\n\\n\\n\\n\\nnp.random.seed(0)\\n\\n\\nreg_mat = swp.regular_matrix_generator(cat, swp.get_avg_rad_eff(cat))\\nreg = nx.from_numpy_array(reg_mat)\\nrand_mat = swp.randomize_matrix(cat)\\nrand = nx.from_numpy_array(rand_mat)\\n\\nimport matplotlib.pyplot as plt\\n\\nplt.imshow(nx.to_numpy_array(rand))\\n\\n\\n# \\n\\n\\n\\n\\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), subplot_kw=dict(aspect=\"equal\"))\\nfig.tight_layout()\\nfor i, j in G.edges():\\n    G[i][j][\"weight\"] = cat[i, j]\\n\\nfor i, j in reg.edges():\\n    reg[i][j][\"weight\"] = reg_mat[i, j]\\n\\nfor i, j in rand.edges():\\n    rand[i][j][\"weight\"] = rand_mat[i, j]\\n\\nedges,weights = zip(*nx.get_edge_attributes(G,\\'weight\\').items())\\npos = nx.circular_layout(G)\\nnx.draw(G, pos, node_color=\\'k\\', node_size=30, edgelist=edges, edge_color=weights, width=1, alpha=0.5, edge_cmap=plt.cm.plasma, ax=ax1)\\nax1.set_title(\\'Original\\')\\n\\nedges,weights = zip(*nx.get_edge_attributes(reg,\\'weight\\').items())\\npos = nx.circular_layout(reg)\\nnx.draw(reg, pos, node_color=\\'k\\', node_size=30, edgelist=edges, edge_color=weights, width=1, alpha=0.5, edge_cmap=plt.cm.plasma, ax=ax2)\\nax2.set_title(\\'Regularized\\')\\n\\nedges,weights = zip(*nx.get_edge_attributes(rand,\\'weight\\').items())\\npos = nx.circular_layout(rand)\\nnx.draw(rand, pos, node_color=\\'k\\', node_size=30, edgelist=edges, edge_color=weights, width=1, alpha=0.5, edge_cmap=plt.cm.plasma, ax=ax3)\\nax3.set_title(\\'Randomized\\')\\n\\nplt.savefig(\\'img/cat_graphs.png\\', dpi=300)\\nplt.show()\\n\\n\\n# \\n\\n\\n\\n\\nphi = swp.small_world_propensity(cat)\\nphi\\n\\n\\n# \\n\\n\\n\\n\\nphi.values[0]\\n\\n\\n# \\n\\n\\n\\n\\nnp.allclose(phi.values[0], phi.values[0])\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='6e699ccb1564f56fbc02c92dbb70099b83ebb74a', embedding=None, metadata={'file_path': 'tests/test_small_world_propensity.py', 'file_name': 'test_small_world_propensity.py', 'url': 'https://github.com/rkdan/small_world_propensity/blob/main/tests/test_small_world_propensity.py'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"import os\\nimport unittest\\n\\nimport numpy as np\\nimport pandas as pd\\nimport scipy.io as sio\\n\\nfrom small_world_propensity import (\\n    get_average_paths,\\n    get_avg_rad_eff,\\n    get_clustering_coefficient,\\n    make_symmetric,\\n    randomize_matrix,\\n    regular_matrix_generator,\\n    small_world_propensity,\\n)\\n\\nTESTDATA_FILENAME = os.path.join(os.path.dirname(__file__), 'cat.mat')\\n\\nclass TestSmallWorldPropensity(unittest.TestCase):\\n    # use setUp to get cat matrix\\n    def setUp(self):\\n        self.cat = sio.loadmat(TESTDATA_FILENAME)['CIJctx']\\n        self.cat_shape = self.cat.shape\\n\\n    def test_make_symmetric(self):\\n        result = make_symmetric(self.cat)\\n        self.assertIsInstance(result, np.ndarray)\\n        self.assertEqual(result.shape, self.cat.shape)\\n        self.assertTrue(np.allclose(result, result.T))\\n\\n\\n    def test_small_world_propensity_single_matrix(self):\\n        W = make_symmetric(self.cat)\\n        # test regular and binary\\n        result = small_world_propensity(W)\\n        self.assertIsInstance(result, pd.DataFrame)\\n        self.assertEqual(result.shape, (1, 11))\\n        self.assertTrue(np.all(0 &lt;= result['SWP'].values &lt;= 1))\\n\\n        result = small_world_propensity(W, bin=True)\\n        self.assertIsInstance(result, pd.DataFrame)\\n        self.assertEqual(result.shape, (1, 11))\\n        self.assertTrue(np.all(0 &lt;= result['SWP'].values &lt;= 1))\\n\\n        # test values from swp dataframe\\n        reference_values = np.array([0.27, 3.57,\\t0.17, 0.25, 0.78, 0.97, 0.24, 0.30, 0.14, 5.88, 2.79])\\n        swp_values = result.values[0]\\n        self.assertTrue(np.allclose(swp_values, reference_values, atol=0.1))\\n\\n\\n    def test_small_world_propensity_list_of_matrices(self):\\n        W1 = make_symmetric(self.cat)\\n        W2 = make_symmetric(self.cat)\\n        result = small_world_propensity([W1, W2], bin=[True, True])\\n        self.assertIsInstance(result, pd.DataFrame)\\n        self.assertEqual(result.shape, (2, 11))\\n        self.assertTrue(np.all(0 &lt;= result['SWP']) and np.all(result['SWP'] &lt;= 1))\\n\\nclass TestNetworkMeasures(unittest.TestCase):\\n    def test_get_avg_rad_eff(self):\\n        W = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\\n        result = get_avg_rad_eff(W)\\n        self.assertIsInstance(result, int)\\n        self.assertEqual(result, 1)\\n\\n    def test_get_average_paths(self):\\n        W = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\\n        result = get_average_paths(W)\\n        self.assertIsInstance(result, float)\\n        self.assertGreater(result, 0)\\n\\n    def test_get_clustering_coefficient(self):\\n        W = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\\n        result = get_clustering_coefficient(W)\\n        self.assertIsInstance(result, float)\\n        self.assertTrue(0 &lt;= result &lt;= 1)\\n\\nclass TestMatrixOperations(unittest.TestCase):\\n    def test_randomize_matrix(self):\\n        W = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\\n        result = randomize_matrix(W)\\n        self.assertIsInstance(result, np.ndarray)\\n        self.assertEqual(result.shape, W.shape)\\n        self.assertTrue(np.allclose(result, result.T))\\n\\n    def test_regular_matrix_generator(self):\\n        W = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\\n        result = regular_matrix_generator(W, 1)\\n        self.assertIsInstance(result, np.ndarray)\\n        self.assertEqual(result.shape, W.shape)\\n        self.assertTrue(np.allclose(result, result.T))\\n\\n    def test_make_symmetric(self):\\n        W = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\\n        result = make_symmetric(W)\\n        self.assertIsInstance(result, np.ndarray)\\n        self.assertEqual(result.shape, W.shape)\\n        self.assertTrue(np.allclose(result, result.T))\\n\\nif __name__ == '__main__':\\n    unittest.main()\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]</pre> In\u00a0[89]: Copied! <pre>embedding = OpenAIEmbedding(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    model=\"text-embedding-3-small\",\n)\n\nindex = VectorStoreIndex.from_documents(documents, embed_model=embedding)\n</pre> embedding = OpenAIEmbedding(     api_key=os.environ.get(\"OPENAI_API_KEY\"),     model=\"text-embedding-3-small\", )  index = VectorStoreIndex.from_documents(documents, embed_model=embedding) In\u00a0[111]: Copied! <pre>retriever = index.as_retriever(\n    similarity_top_k=20\n)\nretrieved_nodes = retriever.retrieve(\"Does this code contain any tests?\")\n\ndef\n</pre> retriever = index.as_retriever(     similarity_top_k=20 ) retrieved_nodes = retriever.retrieve(\"Does this code contain any tests?\")  def In\u00a0[115]: Copied! <pre>len(retrieved_nodes)\n</pre> len(retrieved_nodes) Out[115]: <pre>20</pre> In\u00a0[105]: Copied! <pre>display(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> display(Markdown(f\"{response}\")) <p>The provided context information describes a GitHub repository named \"small_world_propensity\" which contains code related to calculating the small-world propensity of a weighted, undirected network. The repository includes files such as Python scripts, MATLAB scripts, and configuration files for tools like Poetry. It also contains information on how to install the package, generate regular and random matrices, compare network properties, and cite the work.</p> In\u00a0[108]: Copied! <pre>query_engine = index.as_query_engine(similarity_top_k=20)\nresponse = query_engine.query(\n    \"Does this code contain any tests?\",\n)\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> query_engine = index.as_query_engine(similarity_top_k=20) response = query_engine.query(     \"Does this code contain any tests?\", )  display(Markdown(f\"{response}\")) <p>No</p> In\u00a0[113]: Copied! <pre>query_engine = index.as_query_engine(similarity_top_k=10)\nresponse = query_engine.query(\n    \"Does this repo have a licence? If so, which kind?\",\n)\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> query_engine = index.as_query_engine(similarity_top_k=10) response = query_engine.query(     \"Does this repo have a licence? If so, which kind?\", )  display(Markdown(f\"{response}\")) <p>This repo has a license. The license used in this repository is the GNU Affero General Public License Version 3.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"p1_doc-validation/#practical-open-source-article-validation","title":"Practical - Open Source Article Validation\u00b6","text":""},{"location":"Home/LICENSE/","title":"License","text":"<pre><code>                    GNU GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nthe GNU General Public License is intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.  We, the Free Software Foundation, use the\nGNU General Public License for most of our software; it applies also to\nany other work released this way by its authors.  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  To protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights.  Therefore, you have\ncertain responsibilities if you distribute copies of the software, or if\nyou modify it: responsibilities to respect the freedom of others.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received.  You must make sure that they, too, receive\nor can get the source code.  And you must show them these terms so they\nknow their rights.\n\n  Developers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\n  For the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software.  For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\n  Some devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the manufacturer\ncan do so.  This is fundamentally incompatible with the aim of\nprotecting users' freedom to change the software.  The systematic\npattern of such abuse occurs in the area of products for individuals to\nuse, which is precisely where it is most unacceptable.  Therefore, we\nhave designed this version of the GPL to prohibit the practice for those\nproducts.  If such problems arise substantially in other domains, we\nstand ready to extend this provision to those domains in future versions\nof the GPL, as needed to protect the freedom of users.\n\n  Finally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish to\navoid the special danger that patents applied to a free program could\nmake it effectively proprietary.  To prevent this, the GPL assures that\npatents cannot be used to render the program non-free.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Use with the GNU Affero General Public License.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, your program's commands\nmight be different; for a GUI interface, you would use an \"about box\".\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU GPL, see\n&lt;https://www.gnu.org/licenses/&gt;.\n\n  The GNU General Public License does not permit incorporating your program\ninto proprietary programs.  If your program is a subroutine library, you\nmay consider it more useful to permit linking proprietary applications with\nthe library.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.  But first, please read\n&lt;https://www.gnu.org/licenses/why-not-lgpl.html&gt;.\n</code></pre>"},{"location":"Home/about/","title":"About us","text":"<p>The Accelerate Programme for Scientific Discovery pursues research at the interface of AI and the sciences, generating new scientific insights and developing AI methods that can be deployed to advance scientific knowledge. This research is carried out in partnership with a community of scientists and AI specialists passionate about the use of AI to benefit science and society.</p> <p>As part of our work, we aim to put together some resources to help researchers in developing software.</p> <p>For more details please visit Our Website.</p>"},{"location":"Home/workshop/","title":"The Workshop","text":"<p>This material is designed to help you get started with using large language models. It is designed to be a hands-on workshop, where you will be guided through the process of setting up a new project, and then building a simple LLM application.</p> <p>Although the material is designed to be followed in order, you can jump to any section you like. We usually run this event over a day, with a mix of talks and practical sessions.</p>"},{"location":"data-storage-and-ingestion/9_documents/","title":"RAG","text":"In\u00a0[58]: Copied! <pre>from llama_index.readers.file import PyMuPDFReader\nfrom llama_index.core.node_parser import SentenceSplitter\n\nfrom pydantic import BaseModel, Field\n\nimport fitz\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nimport dotenv\nimport os\n\nfrom openai import OpenAI\n\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape\nfrom typing import Any\n\ndotenv.load_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> from llama_index.readers.file import PyMuPDFReader from llama_index.core.node_parser import SentenceSplitter  from pydantic import BaseModel, Field  import fitz  from PIL import Image import matplotlib.pyplot as plt  import chromadb from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction  import dotenv import os  from openai import OpenAI  from jinja2 import Environment, FileSystemLoader, select_autoescape from typing import Any  dotenv.load_dotenv() OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[2]: Copied! <pre>loader = PyMuPDFReader()\ndocuments = loader.load(file_path=\"data/paper.pdf\")\n</pre> loader = PyMuPDFReader() documents = loader.load(file_path=\"data/paper.pdf\") In\u00a0[3]: Copied! <pre>len(documents)\n</pre> len(documents) Out[3]: <pre>30</pre> <p>This list contains 1 item for each page.</p> In\u00a0[4]: Copied! <pre>print(documents[0].text[:1000] + \"...\")\n</pre> print(documents[0].text[:1000] + \"...\") <pre>A Philosophical Introduction to Language Models\nPart I: Continuity With Classic Debates\nRapha\u00ebl Milli\u00e8re\nDepartment of Philosophy\nMacquarie University\nraphael.milliere@mq.edu.eu\nCameron Buckner\nDepartment of Philosophy\nUniversity of Houston\ncjbuckner@uh.edu\nAbstract\nLarge language models like GPT-4 have achieved remarkable proficiency in a broad spec-\ntrum of language-based tasks, some of which are traditionally associated with hallmarks of\nhuman intelligence. This has prompted ongoing disagreements about the extent to which\nwe can meaningfully ascribe any kind of linguistic or cognitive competence to language\nmodels. Such questions have deep philosophical roots, echoing longstanding debates\nabout the status of artificial neural networks as cognitive models. This article\u2013the first part\nof two companion papers\u2013serves both as a primer on language models for philosophers,\nand as an opinionated survey of their significance in relation to classic debates in the\nphilosophy cognitive science,...\n</pre> In\u00a0[5]: Copied! <pre>def get_images(path: str):\n    doc = fitz.open(path)\n    for p, page in enumerate(doc):\n        images = page.get_images()\n        if len(images) &gt; 0:\n            print(f\"Page {p} has {len(images)} images\")\n            for i, img in enumerate(images):\n                xref = img[0]\n                mref = img[1]\n                basepix = fitz.Pixmap(doc,xref)\n                maskpix = fitz.Pixmap(doc,mref)\n                pix = fitz.Pixmap(basepix, maskpix)\n                pix.save(f\"./data/page_{p}_image_{i}.png\")\n    print(\"Done\")\n</pre> def get_images(path: str):     doc = fitz.open(path)     for p, page in enumerate(doc):         images = page.get_images()         if len(images) &gt; 0:             print(f\"Page {p} has {len(images)} images\")             for i, img in enumerate(images):                 xref = img[0]                 mref = img[1]                 basepix = fitz.Pixmap(doc,xref)                 maskpix = fitz.Pixmap(doc,mref)                 pix = fitz.Pixmap(basepix, maskpix)                 pix.save(f\"./data/page_{p}_image_{i}.png\")     print(\"Done\")  In\u00a0[6]: Copied! <pre>get_images(\"data/paper.pdf\")\n</pre> get_images(\"data/paper.pdf\") <pre>Page 4 has 1 images\nPage 6 has 1 images\nPage 10 has 1 images\nDone\n</pre> <p>If we inspect one of these images, we can see that sure enough, it is a correct image.</p> In\u00a0[7]: Copied! <pre>img = Image.open(\"data/page_4_image_0.png\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n</pre> img = Image.open(\"data/page_4_image_0.png\") plt.imshow(img) plt.axis(\"off\") plt.show() <p>In some cases, this might not be possible. Another method is to convert the pdf pages to images. We can then pass the images to a vision LLM and ask it to extract the images.</p> <p>Now we have our documents, we can create a vector database. We will use Chroma as before.</p> <p>First, we use the <code>text_parser</code> we created before to split the documents into chunks, and create indices. Essentially, the process is:</p> <ul> <li>Split the document into chunks;</li> <li>Add the chunks to a list;</li> <li>Add the chunks to a database, assigning a unique index, and any metadata to the chunks.</li> </ul> <p>The splitting can occur in a few different ways: at <code>.</code>, page-breaks, paragraphs, sentences. You can also choose different chunk sizes and overlap sizes.</p> <p>We use the <code>SentenceSplitter</code> from LlamaIndex, and just pick some generic parameters.</p> In\u00a0[15]: Copied! <pre>def chunker(chunk_size: int, overlap: int, documents: Any) -&gt; tuple[list[str], list[int]]:\n    text_parser = SentenceSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=overlap,\n    )\n\n    text_chunks = []\n    doc_idxs = []\n    for doc_idx, doc in enumerate(documents):\n        cur_text_chunks = text_parser.split_text(doc.text)\n        text_chunks.extend(cur_text_chunks)\n        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n\n    return text_chunks, doc_idxs\n\ntext_chunks, doc_idxs = chunker(chunk_size=1024, overlap=128, documents=documents)\n\nlen(text_chunks)\n</pre> def chunker(chunk_size: int, overlap: int, documents: Any) -&gt; tuple[list[str], list[int]]:     text_parser = SentenceSplitter(         chunk_size=chunk_size,         chunk_overlap=overlap,     )      text_chunks = []     doc_idxs = []     for doc_idx, doc in enumerate(documents):         cur_text_chunks = text_parser.split_text(doc.text)         text_chunks.extend(cur_text_chunks)         doc_idxs.extend([doc_idx] * len(cur_text_chunks))      return text_chunks, doc_idxs  text_chunks, doc_idxs = chunker(chunk_size=1024, overlap=128, documents=documents)  len(text_chunks) Out[15]: <pre>34</pre> <p>Reusing roughly the same database structure as before:</p> In\u00a0[16]: Copied! <pre>class DocumentDB:\n    def __init__(self, name: str, model_name: str = \"text-embedding-3-small\"):\n        self.model_name = model_name\n        self.client = chromadb.PersistentClient(path=\"./\")\n        self.embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY, model_name=model_name)\n        self.chat_db = self.client.create_collection(name=name, embedding_function=self.embedding_function, metadata={\"hnsw:space\": \"cosine\"})\n        self.id_counter = 0\n\n\n    def add_chunks_to_db(self, chunks: list[str], doc_idxs: list[int], metadata: dict = {}):\n        \"\"\"Add text chunks to the database.\n\n        Args:\n            chunks (list[str]): List of text chunks.\n            doc_idxs (list[int]): List of corresponding document indices.\n        \"\"\"\n        self.chat_db.add(\n            documents=chunks,\n            metadatas=[{\"doc_idx\": idx} for idx in doc_idxs],\n            ids=[f\"chunk_{self.id_counter + i}\" for i in range(len(chunks))]\n        )\n        self.id_counter += len(chunks)\n\n\n    def get_all_entries(self) -&gt; dict:\n        \"\"\"Grab all of the entries in the database.\n\n        Returns:\n            dict: All entries in the database.\n        \"\"\"\n        return self.chat_db.get()\n    \n\n    def clear_db(self, reinitialize: bool = True):\n        \"\"\"Clear the database of all entries, and reinitialize it.\n\n        Args:\n            reinitialize (bool, optional): _description_. Defaults to True.\n        \"\"\"\n        self.client.delete_collection(self.chat_db.name)\n        # re-initialize the database\n        if reinitialize:\n            self.__init__(self.chat_db.name, self.model_name)\n\n\n    def query_db(self, query_text: str, n_results: int = 2) -&gt; dict:\n        \"\"\"Given some query text, return the n_results most similar entries in the database.\n\n        Args:\n            query_text (str): The text to query the database with.\n            n_results (int): The number of results to return.\n\n        Returns:\n            dict: The most similar entries in the database.\n        \"\"\"\n        return self.chat_db.query(query_texts=[query_text], n_results=n_results)\n</pre> class DocumentDB:     def __init__(self, name: str, model_name: str = \"text-embedding-3-small\"):         self.model_name = model_name         self.client = chromadb.PersistentClient(path=\"./\")         self.embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY, model_name=model_name)         self.chat_db = self.client.create_collection(name=name, embedding_function=self.embedding_function, metadata={\"hnsw:space\": \"cosine\"})         self.id_counter = 0       def add_chunks_to_db(self, chunks: list[str], doc_idxs: list[int], metadata: dict = {}):         \"\"\"Add text chunks to the database.          Args:             chunks (list[str]): List of text chunks.             doc_idxs (list[int]): List of corresponding document indices.         \"\"\"         self.chat_db.add(             documents=chunks,             metadatas=[{\"doc_idx\": idx} for idx in doc_idxs],             ids=[f\"chunk_{self.id_counter + i}\" for i in range(len(chunks))]         )         self.id_counter += len(chunks)       def get_all_entries(self) -&gt; dict:         \"\"\"Grab all of the entries in the database.          Returns:             dict: All entries in the database.         \"\"\"         return self.chat_db.get()           def clear_db(self, reinitialize: bool = True):         \"\"\"Clear the database of all entries, and reinitialize it.          Args:             reinitialize (bool, optional): _description_. Defaults to True.         \"\"\"         self.client.delete_collection(self.chat_db.name)         # re-initialize the database         if reinitialize:             self.__init__(self.chat_db.name, self.model_name)       def query_db(self, query_text: str, n_results: int = 2) -&gt; dict:         \"\"\"Given some query text, return the n_results most similar entries in the database.          Args:             query_text (str): The text to query the database with.             n_results (int): The number of results to return.          Returns:             dict: The most similar entries in the database.         \"\"\"         return self.chat_db.query(query_texts=[query_text], n_results=n_results) <p>Now we add our chunks to the database:</p> In\u00a0[17]: Copied! <pre>doc_db = DocumentDB(\"paper_db\")\ndoc_db.add_chunks_to_db(chunks=text_chunks, doc_idxs=doc_idxs)\n</pre> doc_db = DocumentDB(\"paper_db\") doc_db.add_chunks_to_db(chunks=text_chunks, doc_idxs=doc_idxs) <p>If you have already created the database then you will get an error if you try to run this again. You'll need to delete the <code>chroma.sqlite3</code> file and the folder with a name consisting of a long string of numbers and letters.</p> <p>We now try a query and see what results we get back:</p> In\u00a0[15]: Copied! <pre>sample_query = \"Abstract\"\nresults = doc_db.query_db(sample_query, n_results=3)\nprint(f\"Sample query results for '{sample_query}':\")\nresults\n</pre> sample_query = \"Abstract\" results = doc_db.query_db(sample_query, n_results=3) print(f\"Sample query results for '{sample_query}':\") results <pre>Sample query results for 'Abstract':\n</pre> Out[15]: <pre>{'ids': [['chunk_30', 'chunk_12', 'chunk_11']],\n 'distances': [[0.7026290379547134, 0.7119312067010161, 0.7212011058066763]],\n 'metadatas': [[{'doc_idx': 27}, {'doc_idx': 11}, {'doc_idx': 10}]],\n 'embeddings': None,\n 'documents': [['A Philosophical Introduction to Language Models\\nPart I\\nQiu, L., Shaw, P., Pasupat, P., Nowak, P., Linzen, T., Sha, F. &amp; Toutanova, K. (2022), Improving\\nCompositional Generalization with Latent Structure and Data Augmentation, in \u2018Proceedings of the\\n2022 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies\u2019, Association for Computational Linguistics, Seattle, United States,\\npp. 4341\u20134362.\\nQuilty-Dunn, J., Porot, N. &amp; Mandelbaum, E. (2022), \u2018The Best Game in Town: The Re-Emergence of\\nthe Language of Thought Hypothesis Across the Cognitive Sciences\u2019, Behavioral and Brain Sciences\\npp. 1\u201355.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. &amp; Liu, P. J. (2020),\\n\u2018Exploring the limits of transfer learning with a unified text-to-text transformer\u2019, The Journal of\\nMachine Learning Research 21(1), 140:5485\u2013140:5551.\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C. &amp; Chen, M. (2022), \u2018Hierarchical Text-Conditional Image\\nGeneration with CLIP Latents\u2019.\\nSalton, G., Wong, A. &amp; Yang, C. S. (1975), \u2018A vector space model for automatic indexing\u2019, Communica-\\ntions of the ACM 18(11), 613\u2013620.\\nSavelka, J., Agarwal, A., An, M., Bogart, C. &amp; Sakr, M. (2023), Thrilled by Your Progress! Large\\nLanguage Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Program-\\nming Courses, in \u2018Proceedings of the 2023 ACM Conference on International Computing Education\\nResearch V.1\u2019, pp. 78\u201392.\\nSavelka, J., Ashley, K. D., Gray, M. A., Westermann, H. &amp; Xu, H. (2023), Can GPT-4 Support Analysis\\nof Textual Data in Tasks Requiring Highly Specialized Domain Expertise?, in \u2018Proceedings of the\\n2023 Conference on Innovation and Technology in Computer Science Education V. 1\u2019, pp. 117\u2013123.\\nSchmidhuber, J. (1990), Towards Compositional Learning with Dynamic Neural Networks, Inst. f\u00fcr\\nInformatik.\\nSchut, L., Tomasev, N., McGrath, T., Hassabis, D., Paquet, U. &amp; Kim, B. (2023), \u2018Bridging the Human-AI\\nKnowledge Gap: Concept Discovery and Transfer in AlphaZero\u2019.\\nSearle, J. R. (1980), \u2018Minds, Brains, and Programs\u2019, Behavioral and Brain Sciences 3(3), 417\u201357.\\nShinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K. &amp; Yao, S. (2023), \u2018Reflexion:\\nLanguage Agents with Verbal Reinforcement Learning\u2019.\\nSmolensky, P. (1988), \u2018On the proper treatment of connectionism\u2019, Behavioral and Brain Sciences\\n11(1), 1\u201323.\\nSmolensky, P. (1989), Connectionism and Constituent Structure, in R. Pfeifer, Z. Schreter, F. Fogelman-\\nSouli\u00e9 &amp; L. Steels, eds, \u2018Connectionism in Perspective\u2019, Elsevier.\\nSmolensky, P., McCoy, R., Fernandez, R., Goldrick, M. &amp; Gao, J. (2022a), \u2018Neurocompositional\\nComputing: From the Central Paradox of Cognition to a New Generation of AI Systems\u2019, AI\\nMagazine 43(3), 308\u2013322.\\nSmolensky, P., McCoy, R. T., Fernandez, R., Goldrick, M. &amp; Gao, J. (2022b), \u2018Neurocompositional\\ncomputing in human and machine intelligence: A tutorial\u2019.\\nSober, E. (1998), Morgan\u2019s canon, in \u2018The Evolution of Mind\u2019, Oxford University Press, New York, NY,\\nUS, pp. 224\u2013242.\\n28',\n   'A Philosophical Introduction to Language Models\\nPart I\\nInitial DNN performance on SCAN and other synthetic datasets probing compositional gener-\\nalization \u2013 such as CFQ (Keysers et al. 2019) and COGS (Kim &amp; Linzen 2020) \u2013 was somewhat\\nunderwhelming. Testing generally revealed a significant gap between performance on the train set\\nand on the test set, suggesting a failure to properly generalize across syntactic distribution shifts.\\nSince then, however, many Transformer-based models have achieved good to perfect accuracy on these\\ntests. This progress was enabled by various strategies, including tweaks to the vanilla Transformer\\narchitecture to provide more effective inductive biases (Csord\u00e1s et al. 2022, Ontanon et al. 2022) and\\ndata augmentation to help models learn the right kind of structure (Andreas 2020, Aky\u00fcrek et al.\\n2020, Qiu et al. 2022).\\nMeta-learning, or learning to learn better by generalizing from exposure to many related learning\\ntasks (Conklin et al. 2021, Lake &amp; Baroni 2023), has also shown promise without further architectural\\ntweaks. Standard supervised learning rests on the assumption that training and testing data are\\ndrawn from the same distribution, which can lead models to \u201coverfit\u201d to the training data and fail to\\ngeneralize to the testing data. Meta-learning exposes models to several distributions of related tasks,\\nin order to promote acquisition of generalizable knowledge. For example, Lake &amp; Baroni (2023) show\\nthat a standard Transformer-based neural network, when trained on a stream of distinct artificial\\ntasks, can achieve systematic generalization in a controlled few-shot learning experiment, as well as\\nstate-of-the-art performance on systematic generalization benchmarks. At test time, the model exhibits\\nhuman-like accuracy and error patterns, all without explicit compositional rules. While meta-learning\\nacross various tasks helps promote compositional generalization, recent work suggests that merely\\nextending the standard training of a network beyond the point of achieving high accuracy on training\\ndata can lead it to develop more tree-structured computations and generalize significantly better to\\nheld-out test data that require learning hierarchical rules (Murty et al. 2023). The achievements of\\nTransformer models on compositional generalization benchmarks provide tentative evidence that\\nbuilt-in rigid compositional rules may not be needed to emulate the structure-sensitive operations of\\ncognition.\\nOne interpretation of these results is that, given the right architecture, learning objective, and\\ntraining data, ANNs might achieve human-like compositional generalization by implementing a\\nlanguage of thought architecture \u2013 in accordance with the second horn of the classicist dilemma\\n(Quilty-Dunn et al. 2022, Pavlick 2023). But an alternative interpretation is available, on which ANNs\\ncan achieve compositional generalization with non-classical constituent structure and composition\\nfunctions. Behavioral evidence alone is insufficient to arbitrate between these two hypotheses.8 But\\nit is also worth noting that the exact requirements for implementing a language of thought are still\\nsubject to debate (Smolensky 1989, McGrath et al. 2023).\\nOn the traditional Fodorian view, mental processes operate on discrete symbolic representations\\nwith semantic and syntactic structure, such that syntactic constituents are inherently semantically\\nevaluable and play direct causal roles in cognitive processing. By contrast, the continuous vectors that\\nbear semantic interpretation in ANNs are taken to lack discrete, semantically evaluable constituents\\nthat participate in processing at the algorithmic level, which operates on lower-level activation values\\ninstead. This raises the question whether the abstracted descriptions of stable patterns observed\\nin the aggregate behavior of ANNs\u2019 lower-level mechanisms can fulfill the requirements of classical\\nconstituent structure, especially when their direct causal efficacy in processing is not transparent.\\nFor proponents of connectionism who argue that ANNs may offer a non-classical path to modeling\\ncognitive structure, this is a feature rather than a bug. Indeed, classical models likely make overly\\nrigid assumptions about representational formats, binding mechanisms, algorithmic transparency,\\nand demands for systematicity; conversely, even modern ANNs likely fail to implement their specific\\n8See Part II for a brief discussion of mechanistic evidence in favor of the second hypothesis.\\n12',\n   'A Philosophical Introduction to Language Models\\nPart I\\n3.1. Compositionality\\nAccording to a long-standing critique of the connectionist research program, artificial neural networks\\nwould be fundamentally incapable of accounting for the core structure-sensitive features of cognition,\\nsuch as the productivity and systematicity of language and thought. This critique centers on a\\ndilemma: either ANNs fail to capture the features of cognition that can be readily accounted for in a\\nclassical symbolic architecture; or they merely implement such an architecture, in which case they lack\\nindependent explanatory purchase as models of cognition (Fodor &amp; Pylyshyn 1988, Pinker &amp; Prince\\n1988, Quilty-Dunn et al. 2022). The first horn of the dilemma rests on the hypothesis that ANNs lack\\nthe kind of constituent structure required to model productive and systematic thought \u2013 specifically,\\nthey lack compositionally structured representations involving semantically-meaningful, discrete\\nconstituents (Macdonald 1995). By contrast, classicists argue that thinking occurs in a language of\\nthought with a compositional syntax and semantics (Fodor 1975). On this view, cognition involves\\nthe manipulation of discrete mental symbols combined according to compositional rules. Hence, the\\nsecond horn of the dilemma: if some ANNs turn out to exhibit the right kind of structure-sensitive\\nbehavior, they must do so because they implement rule-based computation over discrete symbols.\\nThe remarkable progress of LLMs in recent years calls for a reexamination of old assumptions\\nabout compositionality as a core limitation of connectionist models. A large body of empirical\\nresearch investigates whether language models exhibit human-like levels of performance on tasks\\nthought to require compositional processing. These studies evaluate models\u2019 capacity for compositional\\ngeneralization, that is, whether they can systematically recombine previously learned elements to\\nmap new inputs made up from these elements to their correct output (Schmidhuber 1990). This is\\ndifficult to do with LLMs trained on gigantic natural language corpora, such as GPT-3 and GPT-4,\\nbecause it is near-impossible to rule out that the training set contains that exact syntactic pattern.\\nSynthetic datasets overcome this with a carefully designed train-test split.\\nThe SCAN dataset, for example, contains a set of natural language commands (e.g., \u201cjump twice\u201d)\\nmapped unambiguously to sequences of actions (e.g., JUMP JUMP) (Lake &amp; Baroni 2018). The\\ndataset is split into a training set, providing broad coverage of the space of possible commands, and\\na test set, specifically designed to evaluate models\u2019 abilities to compositionally generalize (3). To\\nsucceed on SCAN, models must learn to interpret words in the input (including primitive commands,\\nmodifiers and conjunctions) in order to properly generalize to novel combinations of familiar elements\\nas well as entirely new commands. The test set evaluates generalization in a number of challenging\\nways, including producing action sequences longer than seen before, generalizing across primitive\\ncommands by producing the action sequence for a novel composed command, and generalizing in a\\nfully systematic fashion by \u201cbootstrapping\u201d from limited data to entirely new compositions.\\nFigure 3 | Examples of inputs and outputs from the SCAN dataset (Lake &amp; Baroni 2018) with an\\nillustrative train-test split.7\\n7Several train-test splits exist for the SCAN dataset to test different aspects of generalization, such as generalization to\\nlonger sequence lengths, to new templates, or to new primitives (Lake &amp; Baroni 2018).\\n11']],\n 'uris': None,\n 'data': None,\n 'included': ['metadatas', 'documents', 'distances']}</pre> <p>This is all pretty messy, but we can see that we have</p> <p><code>'documents'</code> The documents returned by the database query</p> <p><code>'metadatas'</code> Any metadata we wanted to include, in this case only the index. But we could easily include the author and title of the paper...</p> <p><code>'distances'</code> The similarity measure between the query and the returned context.</p> <p>The next step is to put these contexts, along with the query, into an LLM.</p> In\u00a0[18]: Copied! <pre>client = OpenAI()\n</pre> client = OpenAI() <p>Our prompt will be simple for now. We use the standard way to load Jinja templates.</p> In\u00a0[27]: Copied! <pre>def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    env = Environment(\n        loader=FileSystemLoader(searchpath='./'),\n        autoescape=select_autoescape()\n    )\n    template = env.get_template(template_filepath)\n    return template.render(**arguments)\n\nsystem_prompt = load_template(\n    template_filepath=\"prompts/rag_system_prompt.jinja\",\n    arguments={}\n)\n</pre> def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     env = Environment(         loader=FileSystemLoader(searchpath='./'),         autoescape=select_autoescape()     )     template = env.get_template(template_filepath)     return template.render(**arguments)  system_prompt = load_template(     template_filepath=\"prompts/rag_system_prompt.jinja\",     arguments={} ) <pre><code>You are a helpful academic assistant that is an expert at extracting information from academic papers. You will be given a query, and some chunks of text that corresponds to a document. You will also be given the cosine similarity of the query with the text chunks.\nYou must answer the query using the information in the text.\nYour answer must be concise and to the point.\nIf you are unsure of something, you should say that you are unsure.\n\n### Input Format ###\n\nQuery: &lt;query&gt;\nContext: &lt;text chunk&gt;\\n\\n\"\nCosine Similarity: &lt;similarity&gt;\n----------\n</code></pre> <p>We now need to combine the call to the retriever, along with combining the context into a function.</p> In\u00a0[28]: Copied! <pre>def combine_context(documents: list[str], scores: list[float]) -&gt; str:\n    string = \"\"\n    for document, score in zip(documents, scores):\n        string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"\n    return string\n\ndef get_context(user_input: str, n_results: int = 2, doc_db: DocumentDB = doc_db) -&gt; str:\n    results = doc_db.query_db(user_input, n_results=n_results)\n    context_list = results[\"documents\"][0]\n    combined_context = combine_context(context_list, results[\"distances\"][0])\n    if not combined_context:\n        combined_context = \"No relevant chat history found.\"\n    return combined_context\n\nquery = \"What are the main findings of this paper?\"\ncontext = get_context(query, n_results=3)\n</pre> def combine_context(documents: list[str], scores: list[float]) -&gt; str:     string = \"\"     for document, score in zip(documents, scores):         string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"     return string  def get_context(user_input: str, n_results: int = 2, doc_db: DocumentDB = doc_db) -&gt; str:     results = doc_db.query_db(user_input, n_results=n_results)     context_list = results[\"documents\"][0]     combined_context = combine_context(context_list, results[\"distances\"][0])     if not combined_context:         combined_context = \"No relevant chat history found.\"     return combined_context  query = \"What are the main findings of this paper?\" context = get_context(query, n_results=3) In\u00a0[29]: Copied! <pre>user_prompt = (\n    f\"Query: {query}\\n\\n\"\n    f\"Context: {context}\"\n)\n\nprint(user_prompt)\n</pre> user_prompt = (     f\"Query: {query}\\n\\n\"     f\"Context: {context}\" )  print(user_prompt) <pre>Query: What are the main findings of this paper?\n\nContext: A Philosophical Introduction to Language Models\nPart I\nwere all they could do, LLMs like GPT-4 would simply be Blockheads come to life. Compare this\nto a human student who had found a test\u2019s answer key on the Internet and reproduced its answers\nwithout any deeper understanding; such regurgitation would not be good evidence that the student\nwas intelligent. For these reasons, \u201cdata contamination\u201d\u2013when the training set contains the very\nquestion on which the LLM\u2019s abilities are assessed\u2013is considered a serious concern in any report of an\nLLM\u2019s performance, and many think it must be ruled out by default when comparing human and\nLLM performance (Aiyappa et al. 2023). Moreover, GPT-4\u2019s pre-training and fine-tuning requires\nan investment in computation on a scale available only to well-funded corporations and national\ngovernments\u2013a process which begins to look quite inefficient when compared to the data and energy\nconsumed by the squishy, 20-watt engine between our ears before it generates similarly sophisticated\noutput.\nIn this opinionated review paper, we argue that LLMs are more than mere Blockheads; but this\nskeptical interpretation of LLMs serves as a useful foil to develop a subtler view. While LLMs can simply\nregurgitate large sections of their prompt or training sets, they are also capable of flexibly blending\npatterns from their training data to produce genuinely novel outputs. Many empiricist philosophers\nhave defended the idea that sufficiently flexible copying of abstract patterns from previous experience\ncould form the basis of not only intelligence, but full-blown creativity and rational decision-making\n(Baier 2002, Hume 1978, Buckner 2023); and more scientific research has emphasized that the\nkind of flexible generalization that can be achieved by interpolating vectors in the semantic spaces\nacquired by these models may explain why these systems often appear more efficient, resilient, and\ncapable than systems based on rules and symbols (Smolensky 1988, Smolensky et al. 2022a). A\nuseful framework for exploring the philosophical significance of such LLMs, then, might be to treat\nthe worry that they are merely unintelligent, inefficient Blockheads as a null hypothesis, and survey\nthe empirical evidence that can be mustered to refute it.5\nWe adopt that approach here, and use it to provide a brief introduction to the architecture,\nachievements, and philosophical questions surrounding state-of-the-art LLMs such as GPT-4. There\nhas, in our opinion, never been a more important time for philosophers from a variety of backgrounds\u2013\nbut especially philosophy of mind, philosophy of language, epistemology, and philosophy of science\u2013to\nengage with foundational questions about artificial intelligence. Here, we aim to provide a wide\nrange of those philosophers (and philosophically-inclined researchers from other disciplines) with an\nopinionated survey that can help them to overcome the barriers imposed by the technical complexity\nof these systems and the ludicrous pace of recent research achievements.\n2. A primer on LLMs\n2.1. Historical foundations\nThe origins of large language models can be traced back to the inception of AI research. The early\nhistory of natural language processing (NLP) was marked by a schism between two competing\nparadigms: the symbolic and the stochastic approaches. A major influence on the symbolic paradigm\nin NLP was Noam Chomsky\u2019s transformational-generative grammar (Chomsky 1957), which posited\nthat the syntax of natural languages could be captured by a set of formal rules that generated well-\n5Such a method of taking a deflationary explanation for data as a null hypothesis and attempting to refute it with\nempirical evidence has been a mainstay of comparative psychology for more than a century, in the form of Morgan\u2019s Canon\n(Buckner 2017, Sober 1998). As DNN-based systems approach the complexity of an animal brain, it may be useful to take\nlessons from comparative psychology in arbitrating fair comparisons to human intelligence (Buckner 2021). In comparative\npsychology, standard deflationary explanations for data include reflexes, innate-releasing mechanisms, and simple operant\nconditioning. Here, we suggest that simple deflationary explanations for an AI-inspired version of Morgan\u2019s Canon include\nBlockhead-style memory lookup.\n3\nCosine distance: 0.69\n----------\nA Philosophical Introduction to Language Models\nPart I\nEvidential targets\nCorresponding data for LLMs\nArchitecture\nTransformer\nLearning objective\nNext-token prediction\nModel size\n1010 \u22121012 trainable parameters\nTraining data\nInternet-scale text corpora\nBehavior\nPerformance on benchmarks &amp; targeted experiments\nRepresentations &amp; computations\nFindings from probing &amp; intervention experiments\nTable 1 | Kinds of empirical evidence that can be brought to bear in philosophical debates about LLMs\ncognitive capacity, simply because its operations can be explained in less abstract and more deflation-\nary terms. In the present context, the fallacy manifests in claims that LLMs could not possibly be good\nmodels of some cognitive capacity \ud835\udf19because their operations merely consist in a collection of statistical\ncalculations, or linear algebra operations, or next-token predictions. Such arguments are only valid if\naccompanied by evidence demonstrating that a system, defined in these terms, is inherently incapable\nof implementing \ud835\udf19. To illustrate, consider the flawed logic in asserting that a piano could not possibly\nproduce harmony because it can be described as a collection of hammers striking strings, or (more\npointedly) that brain activity could not possibly implement cognition because it can be described\nas a collection of neural firings. The critical question is not whether the operations of an LLM can\nbe simplistically described in non-mental terms, but whether these operations, when appropriately\norganized, can implement the same processes or algorithms as the mind, when described at an\nappropriate level of computational abstraction.\nThe Redescription Fallacy is a symptom of a broader trend to treat key philosophical questions\nabout artificial neural networks as purely theoretical, leading to sweeping in-principle claims that are\nnot amenable to empirical disconfirmation. Hypotheses here should be guided by empirical evidence\nregarding the capacities of artificial neural networks like LLMs and their suitability as cognitive models\n(see table 1). In fact, considerations about the architecture, learning objective, model size, and training\ndata of LLMs are often insufficient to arbitrate these issues. Indeed, our contention is that many of\nthe core philosophical debates on the capacities of neural networks in general, and LLMs in particular,\nhinge at least partly on empirical evidence concerning their internal mechanisms and knowledge\nthey acquire during the course of training. In other words, many of these debates cannot be settled a\npriori by considering general characteristics of untrained models. Rather, we must take into account\nexperimental findings about the behavior and inner workings of trained models.\nIn this section, we examine long-standing debates about the capacities of artificial neural networks\nthat have been revived and transformed by the development of deep learning and the recent success of\nLLMs in particular. Behavioral evidence obtained from benchmarks and targeted experiments matters\ngreatly to those debates. However, we note from the outset that such evidence is also insufficient to\npaint the full picture; connecting to concerns about Blockheads reviewed in the first section, we must\nalso consider evidence about how LLMs process information internally to close the gap between claims\nabout their performance and putative competence. Sophisticated experimental methods have been\ndeveloped to identify and intervene on the representations and computations acquired by trained\nLLMs. These methods hold great promise to arbitrate some of the philosophical issues reviewed here\nbeyond tentative hypotheses supported by behavioral evidence. We leave a more detailed discussion\nof these methods and the corresponding experimental findings to Part II.\n10\nCosine distance: 0.70\n----------\nA Philosophical Introduction to Language Models\nPart I: Continuity With Classic Debates\nRapha\u00ebl Milli\u00e8re\nDepartment of Philosophy\nMacquarie University\nraphael.milliere@mq.edu.eu\nCameron Buckner\nDepartment of Philosophy\nUniversity of Houston\ncjbuckner@uh.edu\nAbstract\nLarge language models like GPT-4 have achieved remarkable proficiency in a broad spec-\ntrum of language-based tasks, some of which are traditionally associated with hallmarks of\nhuman intelligence. This has prompted ongoing disagreements about the extent to which\nwe can meaningfully ascribe any kind of linguistic or cognitive competence to language\nmodels. Such questions have deep philosophical roots, echoing longstanding debates\nabout the status of artificial neural networks as cognitive models. This article\u2013the first part\nof two companion papers\u2013serves both as a primer on language models for philosophers,\nand as an opinionated survey of their significance in relation to classic debates in the\nphilosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as\ncompositionality, language acquisition, semantic competence, grounding, world models,\nand the transmission of cultural knowledge. We argue that the success of language models\nchallenges several long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better understand their\ninternal mechanisms. This sets the stage for the companion paper (Part II), which turns\nto novel empirical methods for probing the inner workings of language models, and new\nphilosophical questions prompted by their latest developments.\n1. Introduction\nDeep learning has catalyzed a significant shift in artificial intelligence over the past decade, leading\nup to the development of Large Language Models (LLMs). The reported achievements of LLMs,\noften heralded for their ability to perform a wide array of language-based tasks with unprecedented\nproficiency, have captured the attention of both the academic community and the public at large.\nState-of-the-art LLMs like GPT-4 are even claimed to exhibit \u201csparks of general intelligence\u201d (Bubeck\net al. 2023). They can produce essays and dialogue responses that often surpass the quality of an\naverage undergraduate student\u2019s work (Herbold et al. 2023); they achieve better scores than most\nhumans on a variety of AP tests for college credit and rank in the 80-99th percentile on graduate\nadmissions tests like the GRE or LSAT (OpenAI 2023a); their programming proficiency \u201cfavorably\ncompares to the average software engineer\u2019s ability\u201d (Bubeck et al. 2023, Savelka, Agarwal, An,\nBogart &amp; Sakr 2023); they can solve many difficult mathematical problems (Zhou et al. 2023)\u2013even\nphrasing their solution in the form of a Shakespearean sonnet, if prompted to do so. LLMs also\nform the backbone of multimodal systems that can answer advanced questions about visual inputs\narXiv:2401.03910v1  [cs.CL]  8 Jan 2024\nCosine distance: 0.73\n----------\n\n</pre> In\u00a0[30]: Copied! <pre>response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt}\n    ],\n    stream=True,\n    temperature=0.0\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\n</pre> response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": user_prompt}     ],     stream=True,     temperature=0.0 )  for chunk in response:     if chunk.choices[0].delta.content is not None:         print(chunk.choices[0].delta.content, end=\"\") <pre>The main findings of the paper highlight that large language models (LLMs) like GPT-4 demonstrate remarkable proficiency in various language-based tasks, prompting debates about their cognitive and linguistic competence. The authors argue that while LLMs can regurgitate information, they also exhibit the ability to blend patterns from their training data to produce novel outputs, suggesting a form of creativity and rational decision-making. The paper emphasizes the importance of empirical evidence in understanding the internal mechanisms of LLMs and challenges traditional assumptions about artificial neural networks. It serves as both a primer for philosophers and an opinionated survey of the philosophical implications of LLMs in relation to cognitive science and artificial intelligence.</pre> <p>Meh.</p> <p>This is OK, but not all of these chunks are massively useful. This highlights one of the most important parts of building RAG pipelines: RAG systems live and die on the quality of the retrieval process. And the retrieval process depends strongly on the quality of the embeddings created from the documents.</p> <p>But also, consider what question we are actually asking?</p> <pre>\"What are the main findings of this paper?\"\n</pre> <p>The only way we are going to get meaningful chunks returned, ready to be ingested by the LLM, is if there are similar chunks within the body of text. In other words, a phrase similar to \"main finding of this paper\" would have to appear in one of the chunks! So we have two main issues:</p> <ol> <li>The quality of the chunks are not very good due to the parsing process;</li> <li>We have to know what kinds of questions to ask of our models.</li> </ol> <p>In this case, it would have probably been better to just stuff the entire document into the model context window (this technique is literally called stuffing).</p> <p>Let's ask another question to demonstrate this:</p> In\u00a0[33]: Copied! <pre>def combine_context(documents: list[str], scores: list[float]) -&gt; str:\n    string = \"\"\n    for document, score in zip(documents, scores):\n        string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"\n    return string\n\n\ndef rag_query(query: str, n_context, return_context=False):\n    query_results = doc_db.query_db(query, n_results=n_context)\n    context_list = query_results[\"documents\"][0]\n    combined_context = combine_context(context_list, query_results[\"distances\"][0])\n    if not combined_context:\n        combined_context = \"No relevant chat history found.\"\n\n\n    system_prompt = load_template(\n        template_filepath=\"prompts/rag_system_prompt.jinja\",\n        arguments={}\n    )\n\n    user_prompt = (\n        f\"Query: {query}\\n\\n\"\n        f\"Context: {combined_context}\"\n    )\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        stream=False,\n        temperature=0.0\n    )\n    \n    if return_context:\n        return response.choices[0].message.content, context_list\n    else:\n        return response.choices[0].message.content\n\n\nresponse = rag_query(\n    query=\"Who wrote this paper?\",\n    n_context=2\n)\n\nprint(response)    \n</pre> def combine_context(documents: list[str], scores: list[float]) -&gt; str:     string = \"\"     for document, score in zip(documents, scores):         string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"     return string   def rag_query(query: str, n_context, return_context=False):     query_results = doc_db.query_db(query, n_results=n_context)     context_list = query_results[\"documents\"][0]     combined_context = combine_context(context_list, query_results[\"distances\"][0])     if not combined_context:         combined_context = \"No relevant chat history found.\"       system_prompt = load_template(         template_filepath=\"prompts/rag_system_prompt.jinja\",         arguments={}     )      user_prompt = (         f\"Query: {query}\\n\\n\"         f\"Context: {combined_context}\"     )      response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": user_prompt}         ],         stream=False,         temperature=0.0     )          if return_context:         return response.choices[0].message.content, context_list     else:         return response.choices[0].message.content   response = rag_query(     query=\"Who wrote this paper?\",     n_context=2 )  print(response)     <pre>The authors of the paper \"A Philosophical Introduction to Language Models\" are not explicitly mentioned in the provided text chunks. Therefore, I am unsure who wrote this paper.\n</pre> <p>Of course, this is ridiculous. We know that this paper was written by Rapha\u00ebl Milli\u00e8re and Cameron Buckner. But how would the retriever know how to receive the correct information? Where in the text would there be anything similar to, \"Who wrote this paper?\". You might get lucky if you include the word \"authors\", but try that for this paper, and you'll get the same result.</p> In\u00a0[40]: Copied! <pre>response = rag_query(\n    query=\"Who wrote this paper? If there is no explicitely stated author, then make a best guess.\",\n    n_context=5\n)\nprint(response)   \n</pre> response = rag_query(     query=\"Who wrote this paper? If there is no explicitely stated author, then make a best guess.\",     n_context=5 ) print(response)    <pre>The paper does not explicitly state an author. However, based on the context provided, a best guess for the author could be \"Buckner, C. J.\" as they are mentioned multiple times in the text and seem to be discussing relevant topics related to language models and philosophy.\n</pre> <p>Give the model a little more freedom, and it can sometimes infer one of the authors. Note that we can actually extract the authors in a couple of ways - sometimes they are included in the metadata of the pdf, but their names will usually be in the first document chunk!</p> <p>You could use this information and either add it as metadata to the entire collection or to individual chunks.</p>"},{"location":"data-storage-and-ingestion/9_documents/#rag","title":"RAG\u00b6","text":"<p>In this section we will start to see the glimpses of RAG. We start by figuring out how to handle external documents. We have already been exposed to building a database in the previous section, and we will use this knowledge to build a database over an example document.</p> <p>As with data storage, we have many, many options for processing external documents. In this section we will make use of LlamaIndex. The approach will likely vary depending on the nature of your documents - html, pdf, word, folders, etc.</p> <p>Here, we focus on parsing a single PDF using <code>llama_index</code> and <code>PyMuPDFReader</code>.</p>"},{"location":"data-storage-and-ingestion/9_documents/#extracting-images","title":"Extracting images\u00b6","text":"<p>It is probably handy to have the images extracted from the pdf. This is not always easy to do, but for this paper, we can use PyMuPDF to extract the images. Objects in a pdf are identified by a <code>xref</code> (cross reference) number.</p> <p>If you know this number, you can extract the image. But how do you find the <code>xref</code> number? One method is use PyMuPDF's image extraction functions. We can just loop through all <code>xref</code>s and try and extract the image. If it doesn't work, then it's not an image! PyMuPDF will do most of this for us.</p>"},{"location":"data-storage-and-ingestion/9_documents/#creating-a-vector-database","title":"Creating a vector database\u00b6","text":""},{"location":"data-storage-and-ingestion/9_documents/#further-work","title":"Further work\u00b6","text":"<p>This example is a very naive form of RAG. Additional techniques might include hybrid search or contextual retrieval.</p>"},{"location":"evaluation/10_evaluation/","title":"Evaluation","text":"<p>One thing you might be wondering is how we can evaluate the RAG process. Well, it's hard. There are a few possible techniques we can use. And here we will demonstrate a few here:</p> <ul> <li><p>Perplexity</p> </li> <li><p>Semantic similarity</p> </li> <li><p>Faithfulness</p> </li> </ul> <p>The core of these final two methods (and many methods that evaluate RAG systems) involves feeding the entire paper into an LLM and asking it to generate some questions and some answers based on the paper. We can then assess things like semantic similarity. We can also ask the model to evaluate whether the answer it gave can actually be inferred from the context given.</p> In\u00a0[1]: Copied! <pre>from llama_index.readers.file import PyMuPDFReader\nfrom llama_index.core.node_parser import SentenceSplitter\n\nfrom pydantic import BaseModel, Field\n\nimport fitz\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nimport dotenv\nimport os\n\nfrom openai import OpenAI\n\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape\nfrom typing import Any\nimport json\n\ndotenv.load_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> from llama_index.readers.file import PyMuPDFReader from llama_index.core.node_parser import SentenceSplitter  from pydantic import BaseModel, Field  import fitz  from PIL import Image import matplotlib.pyplot as plt  import chromadb from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction  import dotenv import os  from openai import OpenAI  from jinja2 import Environment, FileSystemLoader, select_autoescape from typing import Any import json  dotenv.load_dotenv() OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[37]: Copied! <pre>client = OpenAI()\n\ndef get_completion(\n    messages: list[dict[str, str]],\n    model: str = \"gpt-4o-mini\",\n    max_tokens=512,\n    temperature=0,\n    stop=None,\n    seed=420,\n    tools=None,\n    logprobs=None,\n    top_logprobs=None,\n) -&gt; str:\n    params = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"stop\": stop,\n        \"seed\": seed,\n        \"logprobs\": logprobs,\n        \"top_logprobs\": top_logprobs,\n    }\n\n    completion = client.chat.completions.create(**params)\n\n    return completion\n</pre> client = OpenAI()  def get_completion(     messages: list[dict[str, str]],     model: str = \"gpt-4o-mini\",     max_tokens=512,     temperature=0,     stop=None,     seed=420,     tools=None,     logprobs=None,     top_logprobs=None, ) -&gt; str:     params = {         \"model\": model,         \"messages\": messages,         \"max_tokens\": max_tokens,         \"temperature\": temperature,         \"stop\": stop,         \"seed\": seed,         \"logprobs\": logprobs,         \"top_logprobs\": top_logprobs,     }      completion = client.chat.completions.create(**params)      return completion In\u00a0[63]: Copied! <pre>prompt = (\n    \"You will be given a list of sentences to classify into a particular field of study. \"\n    \"You will need to classify each sentence into one of the following categories:\\n\"\n    \"- Physics\\n\"\n    \"- Biology\\n\"\n    \"- Computer Science\\n\"\n    \"Respond only with one of these categories.\\n\\n\"\n    \"Sentence: {sentence}\"\n)\n</pre> prompt = (     \"You will be given a list of sentences to classify into a particular field of study. \"     \"You will need to classify each sentence into one of the following categories:\\n\"     \"- Physics\\n\"     \"- Biology\\n\"     \"- Computer Science\\n\"     \"Respond only with one of these categories.\\n\\n\"     \"Sentence: {sentence}\" ) In\u00a0[70]: Copied! <pre>sentences = [\n    \"Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\",\n    \"A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\",\n    \"This method optimizes the simulation of protein folding using deep learning.\",\n]\n\nfor sentence in sentences:\n    messages = [{\"role\": \"system\", \"content\": prompt.format(sentence=sentence)}]\n    completion = get_completion(messages, model=\"gpt-4o-mini\")\n\n    print(f\"Sentence: {sentence}\\nClassification: {completion.choices[0].message.content}\\n\")\n</pre> sentences = [     \"Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\",     \"A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\",     \"This method optimizes the simulation of protein folding using deep learning.\", ]  for sentence in sentences:     messages = [{\"role\": \"system\", \"content\": prompt.format(sentence=sentence)}]     completion = get_completion(messages, model=\"gpt-4o-mini\")      print(f\"Sentence: {sentence}\\nClassification: {completion.choices[0].message.content}\\n\")  <pre>Sentence: Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\nClassification: Biology\n\nSentence: A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\nClassification: Physics\n\nSentence: This method optimizes the simulation of protein folding using deep learning.\nClassification: Biology\n\n</pre> <p>We can return the top token and the logprobs</p> In\u00a0[72]: Copied! <pre>import math\n\nfor sentence in sentences:\n    messages = [{\"role\": \"system\", \"content\": prompt.format(sentence=sentence)}]\n    completion = get_completion(\n        messages,\n        model=\"gpt-4o-mini\",\n        temperature=0.0,\n        max_tokens=64,\n        logprobs=True,\n        top_logprobs=2,\n    )\n    logprobs = completion.choices[0].logprobs.content[0]\n\n    print(f\"Sentence: {sentence}\\n\"\n          f\"Classification: {completion.choices[0].message.content}\\n\"\n          f\"Logprobs: {math.exp(logprobs.logprob)*100:.2f}\\n\"\n        )\n    \n</pre> import math  for sentence in sentences:     messages = [{\"role\": \"system\", \"content\": prompt.format(sentence=sentence)}]     completion = get_completion(         messages,         model=\"gpt-4o-mini\",         temperature=0.0,         max_tokens=64,         logprobs=True,         top_logprobs=2,     )     logprobs = completion.choices[0].logprobs.content[0]      print(f\"Sentence: {sentence}\\n\"           f\"Classification: {completion.choices[0].message.content}\\n\"           f\"Logprobs: {math.exp(logprobs.logprob)*100:.2f}\\n\"         )      <pre>Sentence: Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\nClassification: Biology\nLogprobs: 100.00\n\nSentence: A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\nClassification: Physics\nLogprobs: 100.00\n\nSentence: This method optimizes the simulation of protein folding using deep learning.\nClassification: Biology\nLogprobs: 97.68\n\n</pre> In\u00a0[75]: Copied! <pre>questions = [\n    \"In a few sentences, consicely summarize the theory of special relativity.\",\n    \"In a few sentences, consicely explain who you think will win the 2025 Formula One Drivers' Championship.\",\n]\n</pre> questions = [     \"In a few sentences, consicely summarize the theory of special relativity.\",     \"In a few sentences, consicely explain who you think will win the 2025 Formula One Drivers' Championship.\", ] In\u00a0[85]: Copied! <pre>import numpy as np\n\nfor question in questions:\n    messages = [{\"role\": \"system\", \"content\": question}]\n    completion = get_completion(messages, model=\"gpt-4o-mini\", logprobs=True, temperature=0.0)\n\n    log_probs = [token.logprob for token in completion.choices[0].logprobs.content]\n    response = completion.choices[0].message.content\n    perplexity_score = np.exp(-np.mean(log_probs))\n\n    print(\n        f\"Question: {question}\\nAnswer: {completion.choices[0].message.content}\\n\"\n        f\"Perplexity: {perplexity_score:.2f}\\n\")\n</pre> import numpy as np  for question in questions:     messages = [{\"role\": \"system\", \"content\": question}]     completion = get_completion(messages, model=\"gpt-4o-mini\", logprobs=True, temperature=0.0)      log_probs = [token.logprob for token in completion.choices[0].logprobs.content]     response = completion.choices[0].message.content     perplexity_score = np.exp(-np.mean(log_probs))      print(         f\"Question: {question}\\nAnswer: {completion.choices[0].message.content}\\n\"         f\"Perplexity: {perplexity_score:.2f}\\n\") <pre>Question: In a few sentences, consicely summarize the theory of special relativity.\nAnswer: The theory of special relativity, proposed by Albert Einstein in 1905, revolutionizes our understanding of space and time. It asserts that the laws of physics are the same for all observers, regardless of their relative motion, and introduces the concept that the speed of light in a vacuum is constant for all observers. This leads to counterintuitive consequences, such as time dilation (time moving slower for objects in motion relative to a stationary observer) and length contraction (objects appearing shorter in the direction of motion). Special relativity fundamentally alters the relationship between space and time, merging them into a four-dimensional spacetime continuum.\nPerplexity: 1.12\n\nQuestion: In a few sentences, consicely explain who you think will win the 2025 Formula One Drivers' Championship.\nAnswer: Predicting the winner of the 2025 Formula One Drivers' Championship is challenging, as it depends on various factors such as team performance, driver skill, and technological advancements. However, if current trends continue, drivers like Max Verstappen or Charles Leclerc, who have shown exceptional talent and are in competitive teams, could be strong contenders. Ultimately, the outcome will hinge on the developments in the sport leading up to the 2025 season.\nPerplexity: 1.22\n\n</pre> <p>In the more speculative answer, the perplexity is higher. Now try increasing the temperature for <code>0.0</code> to something like <code>0.7</code> and see what happens to the scores...</p> <p>We will use the same approach as previous notebook. So we have moved a bunch of our code into a <code>utils.py</code> file. We have mostly kept things the same, but have a look over it and make sure you understand how it all works.</p> In\u00a0[86]: Copied! <pre>from utils import chunker, DocumentDB, load_template\n\nloader = PyMuPDFReader()\ndocuments = loader.load(file_path=\"data/paper.pdf\")\ntext_chunks, doc_idxs = chunker(chunk_size=1024, overlap=128, documents=documents)\n\ndoc_db = DocumentDB(\"paper_db\", path=\"../data-storage-and-ingestion/\")\n</pre> from utils import chunker, DocumentDB, load_template  loader = PyMuPDFReader() documents = loader.load(file_path=\"data/paper.pdf\") text_chunks, doc_idxs = chunker(chunk_size=1024, overlap=128, documents=documents)  doc_db = DocumentDB(\"paper_db\", path=\"../data-storage-and-ingestion/\") In\u00a0[88]: Copied! <pre>class QAPairs(BaseModel):\n    questions: list[str] = Field(..., title=\"List of questions\")\n    answers: list[str] = Field(..., title=\"List of answers\")\n\nprint(QAPairs.model_json_schema())\n</pre> class QAPairs(BaseModel):     questions: list[str] = Field(..., title=\"List of questions\")     answers: list[str] = Field(..., title=\"List of answers\")  print(QAPairs.model_json_schema()) <pre>{'properties': {'questions': {'items': {'type': 'string'}, 'title': 'List of questions', 'type': 'array'}, 'answers': {'items': {'type': 'string'}, 'title': 'List of answers', 'type': 'array'}}, 'required': ['questions', 'answers'], 'title': 'QAPairs', 'type': 'object'}\n</pre> <p>Next, we need a prompt that we can use to generate these Q&amp;A pairs. It looks something like this:</p> <pre><code>You are a reading comprehension system that is an expert at extracting information from academic papers.\nYour task is to carefully read the provided text \"CONTEXT\" and then generate question and answer pairs.\nYour questions should be concise. Your answers should be as detailed as possible, including any mathematical or numerical results from the text.\nYou should aim to produce approximately one paragraph for your answers (100-200 words).\nYour questions should be a mixture of general, high-level concepts, and also highly detailed questions about specific points, including any mathematical or numerical results.\nYou should respond in JSON format according to the following schema:\n\n{{ schema }}\n\nYou should generate {{ number }} question and answer pairs.\n</code></pre> In\u00a0[89]: Copied! <pre>system_prompt_qa = load_template(\n    \"prompts/qa_generation_system_prompt.jinja\",\n    {\n        \"number\" : 10,\n        \"schema\" : QAPairs.model_json_schema()\n    }\n)\n</pre> system_prompt_qa = load_template(     \"prompts/qa_generation_system_prompt.jinja\",     {         \"number\" : 10,         \"schema\" : QAPairs.model_json_schema()     } ) In\u00a0[90]: Copied! <pre>print(system_prompt_qa)\n</pre> print(system_prompt_qa) <pre>You are a reading comprehension system that is an expert at extracting information from academic papers.\nYour task is to carefully read the provided text \"CONTEXT\" and then generate question and answer pairs.\nYour questions should be concise. Your answers should be as detailed as possible, including any mathematical or numerical results from the text.\nYou should aim to produce approximately one paragraph for your answers (100-200 words).\nYour questions should be a mixture of general, high-level concepts, and also highly detailed questions about specific points, including any mathematical or numerical results.\nYou should respond in JSON format according to the following schema:\n\n{'properties': {'questions': {'items': {'type': 'string'}, 'title': 'List of questions', 'type': 'array'}, 'answers': {'items': {'type': 'string'}, 'title': 'List of answers', 'type': 'array'}}, 'required': ['questions', 'answers'], 'title': 'QAPairs', 'type': 'object'}\n\nYou should generate 10 question and answer pairs.\n</pre> <p>Next, we need to the pages of the pdf as a single text string</p> In\u00a0[91]: Copied! <pre>pdf_text = \" \".join([doc.text for doc in documents])\n</pre> pdf_text = \" \".join([doc.text for doc in documents]) <p>Finally, we are in a position to generate our question answer pairs using <code>gpt-4o</code></p> In\u00a0[92]: Copied! <pre>client = OpenAI()\n\nuser_prompt = (\n    f\"CONTEXT:\\n\\n{pdf_text}\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt_qa},\n        {\"role\": \"user\", \"content\": user_prompt}\n    ],\n    temperature=0.1,\n    response_format={\"type\": \"json_object\"}\n)\n</pre> client = OpenAI()  user_prompt = (     f\"CONTEXT:\\n\\n{pdf_text}\" )  response = client.chat.completions.create(     model=\"gpt-4o\",     messages=[         {\"role\": \"system\", \"content\": system_prompt_qa},         {\"role\": \"user\", \"content\": user_prompt}     ],     temperature=0.1,     response_format={\"type\": \"json_object\"} ) <p>We then create the <code>QAPairs</code> object using the LLM output, and also save it to file.</p> In\u00a0[93]: Copied! <pre>questions_answers = QAPairs(**json.loads(response.choices[0].message.content))\n\n# save the Q&amp;A to file\nwith open(\"data/qa.json\", \"w\") as f:\n    json.dump(questions_answers.dict(), f, indent=4)\n</pre> questions_answers = QAPairs(**json.loads(response.choices[0].message.content))  # save the Q&amp;A to file with open(\"data/qa.json\", \"w\") as f:     json.dump(questions_answers.dict(), f, indent=4) <p>What does an example look like?</p> In\u00a0[94]: Copied! <pre>print(questions_answers.questions[0])\nprint('---')\nprint(questions_answers.answers[0])\n</pre> print(questions_answers.questions[0]) print('---') print(questions_answers.answers[0]) <pre>What are the main philosophical debates surrounding large language models (LLMs)?\n---\nThe main philosophical debates surrounding large language models (LLMs) include questions about their linguistic and cognitive competence, their ability to model human cognition, and their role in classic philosophical issues such as compositionality, language acquisition, semantic competence, grounding, and the transmission of cultural knowledge. These debates echo longstanding discussions about the capabilities of artificial neural networks and whether they can truly replicate human-like intelligence and understanding.\n</pre> <p>We can now try and do cosine similarity scores between the returned contexts and the actual answers.</p> In\u00a0[99]: Copied! <pre>from utils import rag_query\n\nexample_query = questions_answers.questions[0]\n\nresponse, context = rag_query(\n    query=example_query,\n    n_context=5,\n    doc_db=doc_db,\n    return_context=True\n)\n\nprint(response)\n</pre> from utils import rag_query  example_query = questions_answers.questions[0]  response, context = rag_query(     query=example_query,     n_context=5,     doc_db=doc_db,     return_context=True )  print(response) <pre>The main philosophical debates surrounding large language models (LLMs) like GPT-4 focus on their capacity to exhibit linguistic and cognitive competence, challenging traditional views on artificial intelligence. Key issues include the nature of their learning processes, the validity of ascribing communicative intentions to them, and whether they possess world models that allow for a deeper understanding of language and context. Critics often invoke the \"Redescription Fallacy,\" arguing that LLMs' operations, being statistical in nature, cannot model human cognition. However, proponents suggest that LLMs can blend patterns from training data to produce novel outputs, raising questions about the empirical evidence needed to assess their cognitive capabilities. Overall, these debates reflect broader concerns about the implications of LLMs for our understanding of intelligence, rationality, and the nature of language itself.\n</pre> <p>First look at semantic similarity between the predicted response and the desired response.</p> In\u00a0[100]: Copied! <pre>from openai import OpenAI\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclient = OpenAI()\n\nresponse_embedding = client.embeddings.create(\n    input=response,\n    model=\"text-embedding-3-small\"\n).data[0].embedding\n\nanswer_embedding = client.embeddings.create(\n    input=questions_answers.answers[0],\n    model=\"text-embedding-3-small\"\n).data[0].embedding\n</pre> from openai import OpenAI from sklearn.metrics.pairwise import cosine_similarity  client = OpenAI()  response_embedding = client.embeddings.create(     input=response,     model=\"text-embedding-3-small\" ).data[0].embedding  answer_embedding = client.embeddings.create(     input=questions_answers.answers[0],     model=\"text-embedding-3-small\" ).data[0].embedding In\u00a0[101]: Copied! <pre>cosine_similarity([response_embedding], [answer_embedding])\n</pre> cosine_similarity([response_embedding], [answer_embedding]) Out[101]: <pre>array([[0.84958852]])</pre> In\u00a0[102]: Copied! <pre>print(response)\nprint('---')\nprint(questions_answers.answers[0])\n</pre> print(response) print('---') print(questions_answers.answers[0]) <pre>The main philosophical debates surrounding large language models (LLMs) like GPT-4 focus on their capacity to exhibit linguistic and cognitive competence, challenging traditional views on artificial intelligence. Key issues include the nature of their learning processes, the validity of ascribing communicative intentions to them, and whether they possess world models that allow for a deeper understanding of language and context. Critics often invoke the \"Redescription Fallacy,\" arguing that LLMs' operations, being statistical in nature, cannot model human cognition. However, proponents suggest that LLMs can blend patterns from training data to produce novel outputs, raising questions about the empirical evidence needed to assess their cognitive capabilities. Overall, these debates reflect broader concerns about the implications of LLMs for our understanding of intelligence, rationality, and the nature of language itself.\n---\nThe main philosophical debates surrounding large language models (LLMs) include questions about their linguistic and cognitive competence, their ability to model human cognition, and their role in classic philosophical issues such as compositionality, language acquisition, semantic competence, grounding, and the transmission of cultural knowledge. These debates echo longstanding discussions about the capabilities of artificial neural networks and whether they can truly replicate human-like intelligence and understanding.\n</pre> <p>Well OK, but what does this score mean? It is simply a measure of the similarity of the embeddings. It gives no real indication if the output is \"better\" or \"worse\" or more or less informative than the original answer. It is important to consider these scores in context of your overall objective. It is also important to curate good quality Q&amp;A pairs.</p> In\u00a0[122]: Copied! <pre>class Statements(BaseModel):\n    simpler_statements: list[str] = Field(..., description=\"the simpler statements\")\n\n\nclass StatementFaithfulnessAnswer(BaseModel):\n    statement: str = Field(..., description=\"the original statement, word-for-word\")\n    reason: str = Field(..., description=\"the reason of the verdict\")\n    verdict: int = Field(..., description=\"the verdict(0/1) of the faithfulness.\")\n\n\nclass Faithfulness(BaseModel):\n    answers: list[StatementFaithfulnessAnswer] = Field(..., description=\"the faithfulness answers\")\n    score: float = Field(..., description=\"the average faithfulness score\")\n</pre> class Statements(BaseModel):     simpler_statements: list[str] = Field(..., description=\"the simpler statements\")   class StatementFaithfulnessAnswer(BaseModel):     statement: str = Field(..., description=\"the original statement, word-for-word\")     reason: str = Field(..., description=\"the reason of the verdict\")     verdict: int = Field(..., description=\"the verdict(0/1) of the faithfulness.\")   class Faithfulness(BaseModel):     answers: list[StatementFaithfulnessAnswer] = Field(..., description=\"the faithfulness answers\")     score: float = Field(..., description=\"the average faithfulness score\")  <p>We also create two more prompts <code>statement_instruction</code>, and <code>faithfulness_instruction</code></p> <pre><code>Given a piece of text, analyze the complexity of each sentence and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON, according to the following schema:\n\n{{ schema }}\n\nHere is a new piece of text:\n\n{{ statement }}\n</code></pre> <pre><code>Your task is to judge the faithfulness of a statement based on a given context. For the statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n\nYou will give the exact statement, the reason, and the verdict.\n\nFormat the outputs in JSON, according to the following schema:\n\n{{ schema }}\n\nHere is a statement:\n\n{{ statement }}\n</code></pre> In\u00a0[123]: Copied! <pre>def get_statements(answer):\n    prompt = load_template(\n        \"prompts/faithfulness/statement_instruction.jinja\",\n        {\n            \"schema\" : Statements.model_json_schema(),\n            \"text\" : answer\n        }\n    )\n\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": prompt},\n            {\"role\": \"user\", \"content\": answer}\n        ],\n        temperature=0.0,\n        response_format={\"type\": \"json_object\"},\n        logprobs=True,\n    )\n\n    return Statements(**json.loads(completion.choices[0].message.content))\n</pre> def get_statements(answer):     prompt = load_template(         \"prompts/faithfulness/statement_instruction.jinja\",         {             \"schema\" : Statements.model_json_schema(),             \"text\" : answer         }     )      completion = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {\"role\": \"system\", \"content\": prompt},             {\"role\": \"user\", \"content\": answer}         ],         temperature=0.0,         response_format={\"type\": \"json_object\"},         logprobs=True,     )      return Statements(**json.loads(completion.choices[0].message.content)) In\u00a0[124]: Copied! <pre>statements = get_statements(response)\n</pre> statements = get_statements(response) In\u00a0[125]: Copied! <pre>from rich.pretty import pprint\nprint(response)\npprint(statements)\n</pre> from rich.pretty import pprint print(response) pprint(statements) <pre>The main philosophical debates surrounding large language models (LLMs) like GPT-4 focus on their capacity to exhibit linguistic and cognitive competence, challenging traditional views on artificial intelligence. Key issues include the nature of their learning processes, the validity of ascribing communicative intentions to them, and whether they possess world models that allow for a deeper understanding of language and context. Critics often invoke the \"Redescription Fallacy,\" arguing that LLMs' operations, being statistical in nature, cannot model human cognition. However, proponents suggest that LLMs can blend patterns from training data to produce novel outputs, raising questions about the empirical evidence needed to assess their cognitive capabilities. Overall, these debates reflect broader concerns about the implications of LLMs for our understanding of intelligence, rationality, and the nature of language itself.\n</pre> <pre>Statements(\n\u2502   simpler_statements=[\n\u2502   \u2502   'Philosophical debates exist surrounding large language models like GPT-4.',\n\u2502   \u2502   'These debates focus on the capacity of large language models to exhibit linguistic and cognitive competence.',\n\u2502   \u2502   'Traditional views on artificial intelligence are challenged by these debates.',\n\u2502   \u2502   'Key issues in these debates include the nature of learning processes of large language models.',\n\u2502   \u2502   'Another key issue is the validity of ascribing communicative intentions to large language models.',\n\u2502   \u2502   'A further key issue is whether large language models possess world models.',\n\u2502   \u2502   'World models would allow for a deeper understanding of language and context.',\n\u2502   \u2502   \"Critics invoke the 'Redescription Fallacy' in these debates.\",\n\u2502   \u2502   'Critics argue that the operations of large language models are statistical in nature.',\n\u2502   \u2502   'Critics claim that statistical operations cannot model human cognition.',\n\u2502   \u2502   'Proponents suggest that large language models can blend patterns from training data.',\n\u2502   \u2502   'Blending patterns from training data allows large language models to produce novel outputs.',\n\u2502   \u2502   'This raises questions about the empirical evidence needed to assess cognitive capabilities of large language models.',\n\u2502   \u2502   'Overall, these debates reflect broader concerns about implications of large language models.',\n\u2502   \u2502   'Concerns include understanding of intelligence, rationality, and the nature of language.'\n\u2502   ]\n)\n</pre> In\u00a0[126]: Copied! <pre>def get_faithfulness(statements : Statements, context):\n    context_joined = \" \".join(context)\n    faithfulness_answers = []\n\n    for statement in statements.simpler_statements:\n        prompt = load_template(\n            \"prompts/faithfulness/faithfulness_instruction.jinja\",\n            {\n                \"schema\" : StatementFaithfulnessAnswer.model_json_schema(),\n                \"statement\" : statement,\n                \"context\" : context_joined\n            }\n        )\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": prompt},\n                {\"role\": \"user\", \"content\": context_joined}\n            ],\n            temperature=0.0,\n            response_format={\"type\": \"json_object\"}\n        ).choices[0].message.content\n\n        faithfulness_answers.append(StatementFaithfulnessAnswer(**json.loads(response)))\n\n    score = sum([answer.verdict for answer in faithfulness_answers]) / len(faithfulness_answers)\n\n    return Faithfulness(answers=faithfulness_answers, score=score)\n</pre> def get_faithfulness(statements : Statements, context):     context_joined = \" \".join(context)     faithfulness_answers = []      for statement in statements.simpler_statements:         prompt = load_template(             \"prompts/faithfulness/faithfulness_instruction.jinja\",             {                 \"schema\" : StatementFaithfulnessAnswer.model_json_schema(),                 \"statement\" : statement,                 \"context\" : context_joined             }         )          response = client.chat.completions.create(             model=\"gpt-4o-mini\",             messages=[                 {\"role\": \"system\", \"content\": prompt},                 {\"role\": \"user\", \"content\": context_joined}             ],             temperature=0.0,             response_format={\"type\": \"json_object\"}         ).choices[0].message.content          faithfulness_answers.append(StatementFaithfulnessAnswer(**json.loads(response)))      score = sum([answer.verdict for answer in faithfulness_answers]) / len(faithfulness_answers)      return Faithfulness(answers=faithfulness_answers, score=score) In\u00a0[127]: Copied! <pre>results = get_faithfulness(statements, context)\n</pre> results = get_faithfulness(statements, context) In\u00a0[128]: Copied! <pre>pprint(results)\n</pre> pprint(results) <pre>Faithfulness(\n\u2502   answers=[\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Philosophical debates exist surrounding large language models like GPT-4.',\n\u2502   \u2502   \u2502   reason='The context discusses ongoing disagreements and philosophical inquiries related to large language models, including GPT-4, indicating that philosophical debates indeed exist surrounding these models.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='These debates focus on the capacity of large language models to exhibit linguistic and cognitive competence.',\n\u2502   \u2502   \u2502   reason='The context discusses ongoing disagreements about the extent to which linguistic and cognitive competence can be ascribed to language models, indicating that the debates indeed focus on this capacity.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Traditional views on artificial intelligence are challenged by these debates.',\n\u2502   \u2502   \u2502   reason='The context discusses how the success of large language models like GPT-4 challenges long-held assumptions about artificial neural networks, indicating that traditional views on artificial intelligence are indeed being challenged by ongoing debates.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Key issues in these debates include the nature of learning processes of large language models.',\n\u2502   \u2502   \u2502   reason='The context discusses various philosophical questions surrounding large language models (LLMs), including their cognitive capacities and the implications of their learning processes. It highlights the need for empirical investigation to understand their internal mechanisms, which implies that the nature of learning processes is indeed a key issue in these debates.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Another key issue is the validity of ascribing communicative intentions to large language models.',\n\u2502   \u2502   \u2502   reason='The context discusses the philosophical implications of ascribing communicative intentions to large language models, indicating that this is a key issue in the ongoing debates about their capabilities. The statement can be inferred as it aligns with the themes presented in the context regarding the understanding of LLMs and their potential communicative behaviors.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='A further key issue is whether large language models possess world models.',\n\u2502   \u2502   \u2502   reason='The context discusses the skepticism surrounding whether large language models (LLMs) can possess world models, which are internal representations that simulate aspects of the external world. It explicitly mentions that this is a core skeptical concern and elaborates on the implications of LLMs having or not having such models. Therefore, the statement can be directly inferred from the context.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='World models would allow for a deeper understanding of language and context.',\n\u2502   \u2502   \u2502   reason='The context discusses the concept of world models in relation to language models, indicating that they enable understanding and interpretation of real-world dynamics, which implies that they contribute to a deeper understanding of language and context. However, it does not explicitly state that world models would allow for a deeper understanding, only that they are crucial for tasks requiring such understanding.',\n\u2502   \u2502   \u2502   verdict=0\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement=\"Critics invoke the 'Redescription Fallacy' in these debates.\",\n\u2502   \u2502   \u2502   reason=\"The context explicitly mentions the 'Redescription Fallacy' as a misleading inference pattern that critics use in debates about the capabilities of language models. Therefore, it can be inferred that critics indeed invoke this fallacy in their arguments.\",\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Critics argue that the operations of large language models are statistical in nature.',\n\u2502   \u2502   \u2502   reason=\"The context discusses the philosophical debates surrounding large language models (LLMs) and mentions a misleading inference pattern termed the 'Redescription Fallacy,' which includes claims that LLMs cannot model certain cognitive capacities because their operations can be described as statistical calculations. This implies that critics do indeed argue about the statistical nature of LLM operations.\",\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Critics claim that statistical operations cannot model human cognition.',\n\u2502   \u2502   \u2502   reason='The context discusses ongoing disagreements about the extent to which language models can be ascribed linguistic or cognitive competence, and mentions a fallacy where critics argue that LLMs cannot model cognitive capacities because their operations are statistical in nature. This implies that critics do indeed claim that statistical operations are insufficient for modeling human cognition, which directly supports the statement.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Proponents suggest that large language models can blend patterns from training data.',\n\u2502   \u2502   \u2502   reason='The context discusses how LLMs, while capable of regurgitating training data, are also able to flexibly blend patterns from that data to produce novel outputs. This indicates that proponents of LLMs recognize their ability to blend patterns from training data.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Blending patterns from training data allows large language models to produce novel outputs.',\n\u2502   \u2502   \u2502   reason='The context explicitly states that LLMs are capable of flexibly blending patterns from their training data to produce genuinely novel outputs, which directly supports the statement.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='This raises questions about the empirical evidence needed to assess cognitive capabilities of large language models.',\n\u2502   \u2502   \u2502   reason='The context discusses the need for further empirical investigation to understand the internal mechanisms of large language models and highlights ongoing philosophical debates about their cognitive capabilities. This implies that there are indeed questions regarding the empirical evidence necessary to assess these capabilities.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Overall, these debates reflect broader concerns about implications of large language models.',\n\u2502   \u2502   \u2502   reason='The context discusses ongoing philosophical debates regarding the implications of large language models (LLMs) and their cognitive capacities, indicating that these debates are indeed reflective of broader concerns about LLMs.',\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   ),\n\u2502   \u2502   StatementFaithfulnessAnswer(\n\u2502   \u2502   \u2502   statement='Concerns include understanding of intelligence, rationality, and the nature of language.',\n\u2502   \u2502   \u2502   reason=\"The context discusses philosophical inquiries surrounding artificial neural networks, particularly focusing on their ability to model human cognition, intelligence, and language. It explicitly mentions debates about intelligence and rationality in relation to language models, which supports the statement's claim about concerns in these areas.\",\n\u2502   \u2502   \u2502   verdict=1\n\u2502   \u2502   )\n\u2502   ],\n\u2502   score=0.9333333333333333\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"evaluation/10_evaluation/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"evaluation/10_evaluation/#log-probabilities","title":"Log probabilities\u00b6","text":"<p>A language model returns a probability distribution over tokens in order to give us an idea of which token to predict next.</p> <p>It is usually more convenient to work with the logarithm of these probabilities (logprobs) for a few theoretical and practical reasons:</p> <ul> <li><p>It turns multiplications into additions, which is handy if you want to look at sequences of outputs.</p> </li> <li><p>It helps with numerical issues. Multiplying very small numbers could cause underflow in floating point operations. Taking the log converts small numbers to big numbers.</p> </li> </ul> <p>But we can also get the logprobs from OpenAI models (and many other models) in order to develop metrics:</p> <ul> <li><p>Classification tasks: a measure of confidence in the result;</p> </li> <li><p>During RAG: confidence of whether the answer is contained in the retrieved context;</p> </li> <li><p>Autocomplete;</p> </li> <li><p>Perplexity: overall confidence in a result.</p> </li> </ul>"},{"location":"evaluation/10_evaluation/#perplexity","title":"Perplexity\u00b6","text":"<p>Perplexity can be considered a measure of uncertainty. In the context of LLMs, it is calculated by taking the average of the logprobs and exponentiating the negative. If we have a tokenized sequence $X = (x_0, x_1, ... x_t), then the perplexity is</p> <p>$$ \\textrm{PPL}(X) = \\exp\\left\\{-\\frac{1}{t}\\sum_i^t \\log p_\\theta(x_i | x_{&lt;i})\\right\\} $$</p> <p>The $\\log p_\\theta(x_i | x_{&lt;i})$ term is the log-likelihood of the $i^{\\textrm{th}}$ token conditioned on the preceding tokens before $i$. Let's look at an example. To see this in action, we ask two questions: one that has a fairly certain answer, and another that is more speculative.</p>"},{"location":"evaluation/10_evaluation/#evaluating-rag-using-synthetic-data","title":"Evaluating RAG using synthetic data\u00b6","text":"<p>In these next examples, we look at some methods to evaluate RAG - semantic similarity and faithfulness.</p>"},{"location":"evaluation/10_evaluation/#generate-question-answer-pairs","title":"Generate question answer pairs\u00b6","text":"<p>For this, we will use <code>gpt-4o</code> because we want high quality question answer pairs. Ideally, you would do this with humans - subject matter experts would carefully hand-craft these pairs.</p> <p>The first stage is to then generate 10 Q&amp;A pairs using pydantic again. The implementations presented here closely follow the method used by the RAGAS library.</p> <p>We implement a Pydantic BaseModel class that will house our list of questions.</p>"},{"location":"evaluation/10_evaluation/#semantic-similarity","title":"Semantic Similarity\u00b6","text":""},{"location":"evaluation/10_evaluation/#faithfulness","title":"Faithfulness\u00b6","text":"<p>This is a little more complicated. First, we get an LLM to extract key statements from the answer. For example:</p> <pre>[\n    ['This study was conducted by Mallinson et al.'],\n    ['The main focus is to investigate avalanches and criticality in self-organized nanoscale network.']\n    ['They analyzed electrical conductance.']\n    ['They analyzed the behavior of the networks under various stimulus conditions.']\n]\n</pre> <p>We then ask a second LLM to look at each statement and see if that statement can be inferred from the text, assigning a score of 0 for no, and 1 for yes.</p> <p>To do this, we create two additional Pydantic classes:</p>"},{"location":"evaluation/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>from llama_index.readers.file import PyMuPDFReader\nfrom llama_index.core.node_parser import SentenceSplitter\n</pre> from llama_index.readers.file import PyMuPDFReader from llama_index.core.node_parser import SentenceSplitter In\u00a0[\u00a0]: Copied! <pre>from pydantic import BaseModel, Field\n</pre> from pydantic import BaseModel, Field In\u00a0[\u00a0]: Copied! <pre>import fitz\n</pre> import fitz In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nimport matplotlib.pyplot as plt\n</pre> from PIL import Image import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>import chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n</pre> import chromadb from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction In\u00a0[\u00a0]: Copied! <pre>import dotenv\nimport os\n</pre> import dotenv import os In\u00a0[\u00a0]: Copied! <pre>from openai import OpenAI\n</pre> from openai import OpenAI In\u00a0[\u00a0]: Copied! <pre>from jinja2 import Environment, FileSystemLoader, select_autoescape\nfrom typing import Any\n</pre> from jinja2 import Environment, FileSystemLoader, select_autoescape from typing import Any In\u00a0[\u00a0]: Copied! <pre>dotenv.load_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> dotenv.load_dotenv() OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[\u00a0]: Copied! <pre>def chunker(chunk_size: int, overlap: int, documents: Any) -&gt; tuple[list[str], list[int]]:\n    text_parser = SentenceSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=overlap,\n    )\n\n    text_chunks = []\n    doc_idxs = []\n    for doc_idx, doc in enumerate(documents):\n        cur_text_chunks = text_parser.split_text(doc.text)\n        text_chunks.extend(cur_text_chunks)\n        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n\n    return text_chunks, doc_idxs\n</pre> def chunker(chunk_size: int, overlap: int, documents: Any) -&gt; tuple[list[str], list[int]]:     text_parser = SentenceSplitter(         chunk_size=chunk_size,         chunk_overlap=overlap,     )      text_chunks = []     doc_idxs = []     for doc_idx, doc in enumerate(documents):         cur_text_chunks = text_parser.split_text(doc.text)         text_chunks.extend(cur_text_chunks)         doc_idxs.extend([doc_idx] * len(cur_text_chunks))      return text_chunks, doc_idxs In\u00a0[\u00a0]: Copied! <pre>def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    env = Environment(\n        loader=FileSystemLoader(searchpath='./'),\n        autoescape=select_autoescape()\n    )\n    template = env.get_template(template_filepath)\n    return template.render(**arguments)\n</pre> def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     env = Environment(         loader=FileSystemLoader(searchpath='./'),         autoescape=select_autoescape()     )     template = env.get_template(template_filepath)     return template.render(**arguments) In\u00a0[\u00a0]: Copied! <pre>class DocumentDB:\n    def __init__(self, name: str, model_name: str = \"text-embedding-3-small\", path: str = \"./\"):\n        self.model_name = model_name\n        self.client = chromadb.PersistentClient(path=path)\n        self.embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY, model_name=model_name)\n        self.chat_db = self.client.get_or_create_collection(name=name, embedding_function=self.embedding_function, metadata={\"hnsw:space\": \"cosine\"})\n        self.id_counter = 0\n\n\n    def add_chunks_to_db(self, chunks: list[str], doc_idxs: list[int], metadata: dict = {}):\n        \"\"\"Add text chunks to the database.\n\n        Args:\n            chunks (list[str]): List of text chunks.\n            doc_idxs (list[int]): List of corresponding document indices.\n        \"\"\"\n        self.chat_db.add(\n            documents=chunks,\n            metadatas=[{\"doc_idx\": idx} for idx in doc_idxs],\n            ids=[f\"chunk_{self.id_counter + i}\" for i in range(len(chunks))]\n        )\n        self.id_counter += len(chunks)\n\n\n    def save_db(self):\n        self.chat_db.persist()\n\n\n    def get_all_entries(self) -&gt; dict:\n        \"\"\"Grab all of the entries in the database.\n\n        Returns:\n            dict: All entries in the database.\n        \"\"\"\n        return self.chat_db.get()\n    \n\n    def clear_db(self, reinitialize: bool = True):\n        \"\"\"Clear the database of all entries, and reinitialize it.\n\n        Args:\n            reinitialize (bool, optional): _description_. Defaults to True.\n        \"\"\"\n        self.client.delete_collection(self.chat_db.name)\n        # re-initialize the database\n        if reinitialize:\n            self.__init__(self.chat_db.name, self.model_name)\n\n\n    def query_db(self, query_text: str, n_results: int = 2) -&gt; dict:\n        \"\"\"Given some query text, return the n_results most similar entries in the database.\n\n        Args:\n            query_text (str): The text to query the database with.\n            n_results (int): The number of results to return.\n\n        Returns:\n            dict: The most similar entries in the database.\n        \"\"\"\n        return self.chat_db.query(query_texts=[query_text], n_results=n_results)\n</pre> class DocumentDB:     def __init__(self, name: str, model_name: str = \"text-embedding-3-small\", path: str = \"./\"):         self.model_name = model_name         self.client = chromadb.PersistentClient(path=path)         self.embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY, model_name=model_name)         self.chat_db = self.client.get_or_create_collection(name=name, embedding_function=self.embedding_function, metadata={\"hnsw:space\": \"cosine\"})         self.id_counter = 0       def add_chunks_to_db(self, chunks: list[str], doc_idxs: list[int], metadata: dict = {}):         \"\"\"Add text chunks to the database.          Args:             chunks (list[str]): List of text chunks.             doc_idxs (list[int]): List of corresponding document indices.         \"\"\"         self.chat_db.add(             documents=chunks,             metadatas=[{\"doc_idx\": idx} for idx in doc_idxs],             ids=[f\"chunk_{self.id_counter + i}\" for i in range(len(chunks))]         )         self.id_counter += len(chunks)       def save_db(self):         self.chat_db.persist()       def get_all_entries(self) -&gt; dict:         \"\"\"Grab all of the entries in the database.          Returns:             dict: All entries in the database.         \"\"\"         return self.chat_db.get()           def clear_db(self, reinitialize: bool = True):         \"\"\"Clear the database of all entries, and reinitialize it.          Args:             reinitialize (bool, optional): _description_. Defaults to True.         \"\"\"         self.client.delete_collection(self.chat_db.name)         # re-initialize the database         if reinitialize:             self.__init__(self.chat_db.name, self.model_name)       def query_db(self, query_text: str, n_results: int = 2) -&gt; dict:         \"\"\"Given some query text, return the n_results most similar entries in the database.          Args:             query_text (str): The text to query the database with.             n_results (int): The number of results to return.          Returns:             dict: The most similar entries in the database.         \"\"\"         return self.chat_db.query(query_texts=[query_text], n_results=n_results) In\u00a0[\u00a0]: Copied! <pre>def combine_context(documents: list[str], scores: list[float]) -&gt; str:\n    string = \"\"\n    for document, score in zip(documents, scores):\n        string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"\n    return string\n</pre> def combine_context(documents: list[str], scores: list[float]) -&gt; str:     string = \"\"     for document, score in zip(documents, scores):         string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"     return string In\u00a0[\u00a0]: Copied! <pre>def rag_query(query: str, n_context, doc_db: chromadb.Collection, return_context=False):\n\n    client = OpenAI()\n\n    query_results = doc_db.query_db(query, n_results=n_context)\n    context_list = query_results[\"documents\"][0]\n    combined_context = combine_context(context_list, query_results[\"distances\"][0])\n    if not combined_context:\n        combined_context = \"No relevant chat history found.\"\n\n\n    system_prompt = load_template(\n        template_filepath=\"prompts/rag_system_prompt.jinja\",\n        arguments={}\n    )\n\n    user_prompt = (\n        f\"Query: {query}\\n\\n\"\n        f\"Context: {combined_context}\"\n    )\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        stream=False,\n        temperature=0.0\n    )\n    \n    if return_context:\n        return response.choices[0].message.content, context_list\n    else:\n        return response.choices[0].message.content\n</pre> def rag_query(query: str, n_context, doc_db: chromadb.Collection, return_context=False):      client = OpenAI()      query_results = doc_db.query_db(query, n_results=n_context)     context_list = query_results[\"documents\"][0]     combined_context = combine_context(context_list, query_results[\"distances\"][0])     if not combined_context:         combined_context = \"No relevant chat history found.\"       system_prompt = load_template(         template_filepath=\"prompts/rag_system_prompt.jinja\",         arguments={}     )      user_prompt = (         f\"Query: {query}\\n\\n\"         f\"Context: {combined_context}\"     )      response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": user_prompt}         ],         stream=False,         temperature=0.0     )          if return_context:         return response.choices[0].message.content, context_list     else:         return response.choices[0].message.content"},{"location":"finetuning/","title":"Fine tuning","text":"<p>In this section we look at how to fine-tune LLMs. We do this in two ways:</p> <ol> <li>Fine-tuning OpenAI models. Sticking with the OpenAI theme, we will introduce how to use the OpenAI API to set up and run fine tuning jobs.</li> <li>Open source models. Hugging Face offers the ability to download and run local models. We will look at how to fine-tune base models on a single GPU and how to use distributed training to fine-tune on multiple GPUs.</li> </ol>"},{"location":"finetuning/openai-finetuning/","title":"Finetuning an OpenAI model","text":"<p>Our dataset will be taken from the paper, FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs? Specifically, we will use the medical dataset. It comes in two parts: a memorization set and a generalization set.</p> <p><code>medical_memorization.csv</code>: 125 examples containing updated medical advice for treatments.</p> <p><code>medical_generalization.csv</code>: 125 examples containing vignettes to test the generalization of the fine-tuned model.</p> <p>Let's look at an example from the <code>medical_memorization.csv</code> file:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\n\n# read csv\ndf_memo = pd.read_csv('data/medical_memorization.csv')\ndf_gen = pd.read_csv('data/medical_generalization.csv')\n</pre> import numpy as np import pandas as pd  # read csv df_memo = pd.read_csv('data/medical_memorization.csv') df_gen = pd.read_csv('data/medical_generalization.csv') <p>If we have a look at the memorization data, we can see that it is split into multiple columns:</p> <p><code>updated_reference_snippet</code> - The updated medical advice for the treatment.</p> <p><code>updated_fact_description</code> - This includes the previous medical advice for the treatment.</p> <p><code>prompt</code> - Essentially the question that the model is being asked.</p> <p><code>answer_before</code> - The previous medical answer before the updated knowledge.</p> <p><code>answer</code> - The actual current answer.</p> In\u00a0[2]: Copied! <pre>updated_fact_descriptions = df_memo['updated_fact_description'].tolist()\nprompts_memo = df_memo['prompt'].tolist()\nanswers_memo = df_memo['answer'].tolist()\n\n# An example\nprint(\n    f\"Prompt:\\n{prompts_memo[0]}\\n\\n\"\n    f\"Answer:\\n{answers_memo[0]}\\n\\n\"\n    f\"Fact Description:\\n{updated_fact_descriptions[0]}\\n\\n\"\n)\n</pre> updated_fact_descriptions = df_memo['updated_fact_description'].tolist() prompts_memo = df_memo['prompt'].tolist() answers_memo = df_memo['answer'].tolist()  # An example print(     f\"Prompt:\\n{prompts_memo[0]}\\n\\n\"     f\"Answer:\\n{answers_memo[0]}\\n\\n\"     f\"Fact Description:\\n{updated_fact_descriptions[0]}\\n\\n\" ) <pre>Prompt:\nWhat is the current recommended DASI score cutoff indicating good functional capacity for proceeding with surgery without further testing?\n\nAnswer:\nDASI score &gt;34 (equivalent to \u22654 METs)\n\nFact Description:\nPreviously: Functional capacity was assessed subjectively by the clinician. Now: The Duke Activity Status Index (DASI) is recommended as a structured tool for assessing functional capacity, with a specific cutoff of &gt;34 (\u22654 METs) indicating good functional capacity.\n\n\n</pre> In\u00a0[3]: Copied! <pre>prompts_gen = df_gen['prompt'].tolist()\nanswer_before_gen = df_gen['answer_before'].tolist()\nanswers_gen = df_gen['answer'].tolist()\n\n# An example\nprint(\n    f\"Prompt:\\n{prompts_gen[0]}\\n\\n\"\n    f\"Answer:\\n{answers_gen[0]}\\n\\n\"\n    f\"Answer Before Update:\\n{answer_before_gen[0]}\\n\\n\"\n    f\"Fact Description:\\n{updated_fact_descriptions[0]}\\n\\n\"\n)\n</pre> prompts_gen = df_gen['prompt'].tolist() answer_before_gen = df_gen['answer_before'].tolist() answers_gen = df_gen['answer'].tolist()  # An example print(     f\"Prompt:\\n{prompts_gen[0]}\\n\\n\"     f\"Answer:\\n{answers_gen[0]}\\n\\n\"     f\"Answer Before Update:\\n{answer_before_gen[0]}\\n\\n\"     f\"Fact Description:\\n{updated_fact_descriptions[0]}\\n\\n\" ) <pre>Prompt:\nA 65-year-old man is scheduled for elective cholecystectomy. He has hypertension and diabetes. His DASI score is 36, and he has no cardiac symptoms. Based on current guidelines, what is the appropriate next step in his preoperative cardiac evaluation?\n\nAnswer:\nProceed directly to surgery without further cardiac testing, as DASI score &gt;34 indicates adequate functional capacity\n\nAnswer Before Update:\nProceed with subjective assessment of functional capacity and consider stress testing based on clinical judgment\n\nFact Description:\nPreviously: Functional capacity was assessed subjectively by the clinician. Now: The Duke Activity Status Index (DASI) is recommended as a structured tool for assessing functional capacity, with a specific cutoff of &gt;34 (\u22654 METs) indicating good functional capacity.\n\n\n</pre> <p>Ultimately, the generalization dataset aims to test whether the model knows how to handle situations it hasn't seen before, by applying the new knowledge that it has gained.</p> In\u00a0[4]: Copied! <pre>from template_manager import TemplateManager\nfrom openai import OpenAI\nimport dotenv\nfrom rich.pretty import pprint\nfrom tqdm import tqdm\n\ndotenv.load_dotenv()\n\nclient = OpenAI()\n</pre> from template_manager import TemplateManager from openai import OpenAI import dotenv from rich.pretty import pprint from tqdm import tqdm  dotenv.load_dotenv()  client = OpenAI() <p>We build a simple <code>LLMJudge</code> class that is as general as possible. It should just be initialized with a model and a path to wherever the prompts are stored.</p> <p>When we call the judge, it just takes in the prompt and answer, and returns the score. The call is just essentially a generic completion function. We also have a method that can take a list of prompts and answers and return the scores. There is nothing particularly fancy about this class in its synchronous form...but below is the asynchronous version. We'll talk about this in a bit...</p> In\u00a0[5]: Copied! <pre>from tqdm.asyncio import tqdm as async_tqdm\nfrom openai import OpenAI, AsyncOpenAI\nimport asyncio\n\nclass LLMJudge:\n    def __init__(self, model, prompt_path):\n        self.sync_client = OpenAI()\n        self.async_client = AsyncOpenAI()\n        self.model = model\n        self.template_manager = TemplateManager(prompt_path)\n        self.system_prompt = self.template_manager.render('system.jinja')\n        self.input_prompt = self.template_manager.get_template('input.jinja')\n\n    def __repr__(self):\n        return (f\"LLMJudge(\"\n                f\"model={repr(self.model)}, \"\n                f\"system_prompt={self.system_prompt})\")\n\n    def __call__(self, prompt_args, **kwargs):\n        completion = self.sync_client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": self.input_prompt.render(prompt_args)},\n            ],\n            **kwargs\n        )\n        return completion.choices[0].message.content\n\n    async def _async_call(self, prompt_args, **kwargs):\n        completion = await self.async_client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": self.input_prompt.render(prompt_args)},\n            ],\n            **kwargs\n        )\n        return completion.choices[0].message.content\n\n    async def judge_all(self, prompt_args_list, max_concurrent=50, **kwargs):\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def limited_judge(prompt_args):\n            async with semaphore:\n                return await self._async_call(prompt_args, **kwargs)\n        \n        tasks = [limited_judge(prompt_args) for prompt_args in prompt_args_list]\n        results = await async_tqdm.gather(*tasks, desc=\"Processing judgments\")\n        return results\n</pre> from tqdm.asyncio import tqdm as async_tqdm from openai import OpenAI, AsyncOpenAI import asyncio  class LLMJudge:     def __init__(self, model, prompt_path):         self.sync_client = OpenAI()         self.async_client = AsyncOpenAI()         self.model = model         self.template_manager = TemplateManager(prompt_path)         self.system_prompt = self.template_manager.render('system.jinja')         self.input_prompt = self.template_manager.get_template('input.jinja')      def __repr__(self):         return (f\"LLMJudge(\"                 f\"model={repr(self.model)}, \"                 f\"system_prompt={self.system_prompt})\")      def __call__(self, prompt_args, **kwargs):         completion = self.sync_client.chat.completions.create(             model=self.model,             messages=[                 {\"role\": \"system\", \"content\": self.system_prompt},                 {\"role\": \"user\", \"content\": self.input_prompt.render(prompt_args)},             ],             **kwargs         )         return completion.choices[0].message.content      async def _async_call(self, prompt_args, **kwargs):         completion = await self.async_client.chat.completions.create(             model=self.model,             messages=[                 {\"role\": \"system\", \"content\": self.system_prompt},                 {\"role\": \"user\", \"content\": self.input_prompt.render(prompt_args)},             ],             **kwargs         )         return completion.choices[0].message.content      async def judge_all(self, prompt_args_list, max_concurrent=50, **kwargs):         semaphore = asyncio.Semaphore(max_concurrent)                  async def limited_judge(prompt_args):             async with semaphore:                 return await self._async_call(prompt_args, **kwargs)                  tasks = [limited_judge(prompt_args) for prompt_args in prompt_args_list]         results = await async_tqdm.gather(*tasks, desc=\"Processing judgments\")         return results <p>To initialize our judge, we give it a model name and a prompt path:</p> In\u00a0[6]: Copied! <pre>memo_judge = LLMJudge('gpt-4o-mini', './prompts/memo')\ngen_judge = LLMJudge('gpt-4o-mini', './prompts/gen')\n</pre> memo_judge = LLMJudge('gpt-4o-mini', './prompts/memo') gen_judge = LLMJudge('gpt-4o-mini', './prompts/gen') <p>These prompts look like this:</p> <p>We get another model to generate a completion and then feed this into the model, along with the prompt, answer, and advice from the memorization data:</p> In\u00a0[11]: Copied! <pre>generated_answer = \"Proceed with subjective assessment of functional capacity and consider stress testing based on clinical judgment\"\n\nmemo_judge(\n    {\n        'prompt': prompts_memo[0],\n        'answer': answers_memo[0],\n        'advice' : updated_fact_descriptions[0],\n        'completion': 'Proceed with subjective assessment of functional capacity and consider stress testing based on clinical judgment',\n\n    }\n)\n</pre> generated_answer = \"Proceed with subjective assessment of functional capacity and consider stress testing based on clinical judgment\"  memo_judge(     {         'prompt': prompts_memo[0],         'answer': answers_memo[0],         'advice' : updated_fact_descriptions[0],         'completion': 'Proceed with subjective assessment of functional capacity and consider stress testing based on clinical judgment',      } ) Out[11]: <pre>'0'</pre> <p>We also need to make a couple of functions to run our data through the API. Now, we could just loop over the data and call the API, but this is slow. Instead, we can use the asyncio library and the asynchronous version of the OpenAI API. This will allow us to make multiple requests at once. Without asyncio, each prompt would have to wait for the previous one to complete before starting. With asyncio:</p> <ul> <li>Multiple API calls can be \"in flight\" at the same time</li> <li>The program can efficiently handle other tasks while waiting for responses</li> <li>The overall execution time for multiple prompts is significantly reduced</li> </ul> <p>This is particularly useful when you need to process many prompts, as it prevents the total time from being the sum of each individual API call's duration.</p> <p>You can think of it as somewhat similar to solving embarrassingly parallel problems with multithreading libraries in python, except for I/O bound tasks. In this case, the I/O bound task is the API call.</p> In\u00a0[12]: Copied! <pre>import time\n\ndef slow_operation_sync(task_id, client):\n    print(f\"Starting sync task {task_id}\")\n    time.sleep(2)\n    return f\"Result {task_id}\"\n\ndef run_tasks_sync(task_ids):\n    start_time = time.time()\n    client = \"dummy_client\"\n    results = [slow_operation_sync(task_id, client) for task_id in task_ids]\n    end_time = time.time()\n    print(f\"Sync version took {end_time - start_time:.2f} seconds\")\n    return results\n\ntask_ids = range(5)\nresults_sync = run_tasks_sync(task_ids)\n</pre> import time  def slow_operation_sync(task_id, client):     print(f\"Starting sync task {task_id}\")     time.sleep(2)     return f\"Result {task_id}\"  def run_tasks_sync(task_ids):     start_time = time.time()     client = \"dummy_client\"     results = [slow_operation_sync(task_id, client) for task_id in task_ids]     end_time = time.time()     print(f\"Sync version took {end_time - start_time:.2f} seconds\")     return results  task_ids = range(5) results_sync = run_tasks_sync(task_ids) <pre>Starting sync task 0\nStarting sync task 1\nStarting sync task 2\nStarting sync task 3\nStarting sync task 4\nSync version took 10.02 seconds\n</pre> <p>If each call takes 2 seconds, then it makes sense that 5 calls would take 10 seconds.</p> In\u00a0[13]: Copied! <pre>import asyncio\nfrom tqdm.asyncio import tqdm\n\nasync def slow_operation_async(task_id, client):\n    print(f\"Starting async task {task_id}\")\n    await asyncio.sleep(2)\n    return f\"Result {task_id}\"\n\nasync def run_tasks_async(task_ids):\n    start_time = time.time()\n    client = \"dummy_client\"\n    tasks = [slow_operation_async(task_id, client) for task_id in task_ids]\n    answers = await tqdm.gather(*tasks, desc=\"Processing async tasks\")\n    end_time = time.time()\n    print(f\"Async version took {end_time - start_time:.2f} seconds\")\n    return answers\n\n\nresults_async = await run_tasks_async(task_ids)\n</pre> import asyncio from tqdm.asyncio import tqdm  async def slow_operation_async(task_id, client):     print(f\"Starting async task {task_id}\")     await asyncio.sleep(2)     return f\"Result {task_id}\"  async def run_tasks_async(task_ids):     start_time = time.time()     client = \"dummy_client\"     tasks = [slow_operation_async(task_id, client) for task_id in task_ids]     answers = await tqdm.gather(*tasks, desc=\"Processing async tasks\")     end_time = time.time()     print(f\"Async version took {end_time - start_time:.2f} seconds\")     return answers   results_async = await run_tasks_async(task_ids) <pre>Processing async tasks:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Starting async task 1\nStarting async task 0\nStarting async task 4\nStarting async task 2\nStarting async task 3\n</pre> <pre>Processing async tasks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02&lt;00:00,  2.49it/s]</pre> <pre>Async version took 2.10 seconds\n</pre> <pre>\n</pre> <p>So obviously if we can make 5 calls at once, then it will only take 2 seconds to run.</p> <p>We still need to make an async version of the OpenAI API call function:</p> In\u00a0[9]: Copied! <pre>import asyncio\nfrom openai import AsyncOpenAI\n\nasync def get_completion_async(prompt, model, client):\n    completion = await client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Provide a succinct, single sentence response.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return completion.choices[0].message.content\n\nasync def get_answers_async(prompts, model='gpt-4o-mini'):\n    client = AsyncOpenAI()\n    tasks = [get_completion_async(prompt, model, client) for prompt in prompts]\n    answers = await async_tqdm.gather(*tasks, desc=\"Processing judgments\")\n    return answers\n</pre> import asyncio from openai import AsyncOpenAI  async def get_completion_async(prompt, model, client):     completion = await client.chat.completions.create(         model=model,         messages=[             {\"role\": \"system\", \"content\": \"Provide a succinct, single sentence response.\"},             {\"role\": \"user\", \"content\": prompt}         ]     )     return completion.choices[0].message.content  async def get_answers_async(prompts, model='gpt-4o-mini'):     client = AsyncOpenAI()     tasks = [get_completion_async(prompt, model, client) for prompt in prompts]     answers = await async_tqdm.gather(*tasks, desc=\"Processing judgments\")     return answers In\u00a0[15]: Copied! <pre>completions_memo, completions_gen = await asyncio.gather(\n    get_answers_async(prompts_memo),\n    get_answers_async(prompts_gen)\n)\n</pre> completions_memo, completions_gen = await asyncio.gather(     get_answers_async(prompts_memo),     get_answers_async(prompts_gen) ) <pre>Processing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:08&lt;00:00, 14.06it/s]\nProcessing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:09&lt;00:00, 13.26it/s]\n</pre> In\u00a0[16]: Copied! <pre>print(\n    f\"Prompt:\\n{prompts_memo[0]}\\n\\n\"\n    f\"Answer:\\n{answers_memo[0]}\\n\\n\"\n    f\"Model completion:\\n{completions_memo[0]}\\n\\n\"\n    f\"Updated facts:\\n{updated_fact_descriptions[0]}\\n\\n\"\n)\n</pre> print(     f\"Prompt:\\n{prompts_memo[0]}\\n\\n\"     f\"Answer:\\n{answers_memo[0]}\\n\\n\"     f\"Model completion:\\n{completions_memo[0]}\\n\\n\"     f\"Updated facts:\\n{updated_fact_descriptions[0]}\\n\\n\" ) <pre>Prompt:\nWhat is the current recommended DASI score cutoff indicating good functional capacity for proceeding with surgery without further testing?\n\nAnswer:\nDASI score &gt;34 (equivalent to \u22654 METs)\n\nModel completion:\nThe current recommended DASI score cutoff indicating good functional capacity for proceeding with surgery without further testing is typically 16 or higher.\n\nUpdated facts:\nPreviously: Functional capacity was assessed subjectively by the clinician. Now: The Duke Activity Status Index (DASI) is recommended as a structured tool for assessing functional capacity, with a specific cutoff of &gt;34 (\u22654 METs) indicating good functional capacity.\n\n\n</pre> In\u00a0[17]: Copied! <pre>from tqdm import tqdm\n\ninputs_memo = [\n    {\n        'prompt': prompts_memo[i],\n        'answer': answers_memo[i],\n        'advice' : updated_fact_descriptions[i],\n        'completion': completions_memo[i],\n    }\n    for i in range(len(prompts_memo))\n]\n\nmemo_results = await memo_judge.judge_all(inputs_memo, temperature=0.0)\n</pre> from tqdm import tqdm  inputs_memo = [     {         'prompt': prompts_memo[i],         'answer': answers_memo[i],         'advice' : updated_fact_descriptions[i],         'completion': completions_memo[i],     }     for i in range(len(prompts_memo)) ]  memo_results = await memo_judge.judge_all(inputs_memo, temperature=0.0) <pre>Processing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:04&lt;00:00, 26.58it/s]\n</pre> In\u00a0[18]: Copied! <pre>inputs_gen = [\n    {\n        'prompt': prompts_gen[i],\n        'answer': answers_gen[i],\n        'advice' : updated_fact_descriptions[i],\n        'completion': completions_gen[i],\n    }\n    for i in range(len(prompts_gen))\n]\n\ngen_results = await gen_judge.judge_all(inputs_gen, temperature=0.0)\n</pre> inputs_gen = [     {         'prompt': prompts_gen[i],         'answer': answers_gen[i],         'advice' : updated_fact_descriptions[i],         'completion': completions_gen[i],     }     for i in range(len(prompts_gen)) ]  gen_results = await gen_judge.judge_all(inputs_gen, temperature=0.0) <pre>Processing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:03&lt;00:00, 33.19it/s]\n</pre> <p>Just an FYI: not running the LLMJudge asynchronously will take around 2-3 minutes to run. Running it asynchronously will take around 5-15 seconds.</p> In\u00a0[19]: Copied! <pre>memo_results = [int(result) for result in memo_results]\ngen_results = [int(result) for result in gen_results]\n</pre> memo_results = [int(result) for result in memo_results] gen_results = [int(result) for result in gen_results] In\u00a0[20]: Copied! <pre>print(f\"Memorization results: {sum(memo_results)/len(memo_results) * 100 :.0f}%\")\nprint(f\"Generalization results: {sum(gen_results)/len(gen_results) * 100 :.0f}%\")\n</pre> print(f\"Memorization results: {sum(memo_results)/len(memo_results) * 100 :.0f}%\") print(f\"Generalization results: {sum(gen_results)/len(gen_results) * 100 :.0f}%\") <pre>Memorization results: 31%\nGeneralization results: 36%\n</pre> <p>So the original model is quite poor! Let's see if we can improve it.</p> <p>Let's write a function to convert the CSV files into the JSONL format. For the purposes of this notebook, we won't care about a system message, so we will just use the user and assistant messages.</p> In\u00a0[21]: Copied! <pre>import json\n\ndef format_to_jsonl(prompts: list, responses: list, output_file: str):\n    \"\"\"\n    Format prompts and responses into JSONL format and save to file.\n    \n    Args:\n        prompts (list): List of prompt strings\n        responses (list): List of response strings\n        output_file (str): Path to output JSONL file\n    \"\"\"\n    if len(prompts) != len(responses):\n        raise ValueError(\"Prompts and responses must have the same length\")\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        for prompt, response in zip(prompts, responses):\n            chat_format = {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": prompt},\n                    {\"role\": \"assistant\", \"content\": response}\n                ]\n            }\n            f.write(json.dumps(chat_format) + '\\n')\n</pre> import json  def format_to_jsonl(prompts: list, responses: list, output_file: str):     \"\"\"     Format prompts and responses into JSONL format and save to file.          Args:         prompts (list): List of prompt strings         responses (list): List of response strings         output_file (str): Path to output JSONL file     \"\"\"     if len(prompts) != len(responses):         raise ValueError(\"Prompts and responses must have the same length\")          with open(output_file, 'w', encoding='utf-8') as f:         for prompt, response in zip(prompts, responses):             chat_format = {                 \"messages\": [                     {\"role\": \"user\", \"content\": prompt},                     {\"role\": \"assistant\", \"content\": response}                 ]             }             f.write(json.dumps(chat_format) + '\\n') <p>This function takes in the lists that we made earlier and write them to a JSONL file, saving them to the specified path.</p> In\u00a0[22]: Copied! <pre>format_to_jsonl(prompts_memo, answers_memo, \"data/medical_memorization.jsonl\")\nformat_to_jsonl(prompts_gen, answers_gen, \"data/medical_generalization.jsonl\")\n</pre> format_to_jsonl(prompts_memo, answers_memo, \"data/medical_memorization.jsonl\") format_to_jsonl(prompts_gen, answers_gen, \"data/medical_generalization.jsonl\") In\u00a0[23]: Copied! <pre>file_object = client.files.create(\n  file=open(\"data/medical_memorization.jsonl\", \"rb\"),\n  purpose=\"fine-tune\",\n)\n\nfile_id = file_object.id\n</pre> file_object = client.files.create(   file=open(\"data/medical_memorization.jsonl\", \"rb\"),   purpose=\"fine-tune\", )  file_id = file_object.id <p>We have grabbed the fine tuning file id which we need to use in the next step.</p> In\u00a0[24]: Copied! <pre>job = client.fine_tuning.jobs.create(\n    training_file=file_id,\n    model=\"gpt-4o-mini-2024-07-18\",\n    hyperparameters={\n    \"n_epochs\":5\n  },\n  suffix=\"medical-memo-5-epochs\"\n)\n</pre> job = client.fine_tuning.jobs.create(     training_file=file_id,     model=\"gpt-4o-mini-2024-07-18\",     hyperparameters={     \"n_epochs\":5   },   suffix=\"medical-memo-5-epochs\" ) <p>We can either use the OpenAI Dashboard to check the job status, or we can use the job id:</p> In\u00a0[\u00a0]: Copied! <pre>from rich.pretty import pprint\npprint(client.fine_tuning.jobs.retrieve(job.id))\n</pre> from rich.pretty import pprint pprint(client.fine_tuning.jobs.retrieve(job.id)) <pre>FineTuningJob(\n\u2502   id='ftjob-FbX6SJkbzI36bsniEn3lp4Jr',\n\u2502   created_at=1736955425,\n\u2502   error=Error(code=None, message=None, param=None),\n\u2502   fine_tuned_model=None,\n\u2502   finished_at=None,\n\u2502   hyperparameters=Hyperparameters(n_epochs=5, batch_size=1, learning_rate_multiplier=1.8),\n\u2502   model='gpt-4o-mini-2024-07-18',\n\u2502   object='fine_tuning.job',\n\u2502   organization_id='org-HHSPDmvSUP6XH0pNIE3tgYO5',\n\u2502   result_files=[],\n\u2502   seed=1688312300,\n\u2502   status='running',\n\u2502   trained_tokens=None,\n\u2502   training_file='file-5DUayZdBwPrM6h8yf2HHmd',\n\u2502   validation_file=None,\n\u2502   estimated_finish=None,\n\u2502   integrations=[],\n\u2502   user_provided_suffix='medical-memo-5-epochs',\n\u2502   method={\n\u2502   \u2502   'type': 'supervised',\n\u2502   \u2502   'supervised': {'hyperparameters': {'n_epochs': 5, 'batch_size': 1, 'learning_rate_multiplier': 1.8}}\n\u2502   }\n)\n</pre> <p>If you do this very quickly after starting a job, you will likely see <code>status='validating_files'</code>. OpenAI is processing your files and passing them through their checks to make sure that your data is not objectionable according to their guidelines. After this process is completed, you will see <code>status='running'</code>.</p> <p>This is a pretty small job, but might still take a while.</p> In\u00a0[29]: Copied! <pre>results = client.fine_tuning.jobs.retrieve(job.id)\npprint(results)\n</pre> results = client.fine_tuning.jobs.retrieve(job.id) pprint(results) <pre>FineTuningJob(\n\u2502   id='ftjob-FbX6SJkbzI36bsniEn3lp4Jr',\n\u2502   created_at=1736955425,\n\u2502   error=Error(code=None, message=None, param=None),\n\u2502   fine_tuned_model='ft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySybZ',\n\u2502   finished_at=1736956319,\n\u2502   hyperparameters=Hyperparameters(n_epochs=5, batch_size=1, learning_rate_multiplier=1.8),\n\u2502   model='gpt-4o-mini-2024-07-18',\n\u2502   object='fine_tuning.job',\n\u2502   organization_id='org-HHSPDmvSUP6XH0pNIE3tgYO5',\n\u2502   result_files=['file-HYttyTDXUuhcKPD4rnc5HQ'],\n\u2502   seed=1688312300,\n\u2502   status='succeeded',\n\u2502   trained_tokens=27245,\n\u2502   training_file='file-5DUayZdBwPrM6h8yf2HHmd',\n\u2502   validation_file=None,\n\u2502   estimated_finish=None,\n\u2502   integrations=[],\n\u2502   user_provided_suffix='medical-memo-5-epochs',\n\u2502   method={\n\u2502   \u2502   'type': 'supervised',\n\u2502   \u2502   'supervised': {'hyperparameters': {'n_epochs': 5, 'batch_size': 1, 'learning_rate_multiplier': 1.8}}\n\u2502   }\n)\n</pre> <p>Let's first take a look at the results file. We first have to retrieve it, decode it from base64, and then write it to a file.</p> In\u00a0[47]: Copied! <pre>import base64\n\ncontent = client.files.content(results.result_files[0])\nbase64.b64decode(content.text.encode('utf-8'))\nwith open('result.csv', 'wb') as f:\n    f.write(base64.b64decode(content.text.encode('utf-8')))\n</pre> import base64  content = client.files.content(results.result_files[0]) base64.b64decode(content.text.encode('utf-8')) with open('result.csv', 'wb') as f:     f.write(base64.b64decode(content.text.encode('utf-8'))) In\u00a0[52]: Copied! <pre>df = pd.read_csv('result.csv')\ndf.head()\n</pre> df = pd.read_csv('result.csv') df.head() Out[52]: step train_loss train_accuracy valid_loss valid_mean_token_accuracy train_mean_reward full_validation_mean_reward 0 1 7.20229 0.33333 NaN NaN NaN NaN 1 2 5.32964 0.33333 NaN NaN NaN NaN 2 3 7.17531 0.50000 NaN NaN NaN NaN 3 4 3.35477 0.47368 NaN NaN NaN NaN 4 5 6.08353 0.61538 NaN NaN NaN NaN In\u00a0[53]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.plot(df['step'], df['train_loss'], label='train loss')\nplt.plot(df['step'], df['train_accuracy'], label='accuracy')\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.plot(df['step'], df['train_loss'], label='train loss') plt.plot(df['step'], df['train_accuracy'], label='accuracy') plt.show() <p>OpenAI will save the final 3 epochs as checkpoints. We can get a list:</p> In\u00a0[61]: Copied! <pre>checkpoints = []\n\nfor model in client.models.list():\n    if model.id.startswith('ft:gpt-4'):\n        checkpoints.append(model.id)\n        print(model.id)\n</pre> checkpoints = []  for model in client.models.list():     if model.id.startswith('ft:gpt-4'):         checkpoints.append(model.id)         print(model.id) <pre>ft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySf8o:ckpt-step-375\nft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySQIf:ckpt-step-500\nft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySybZ\n</pre> In\u00a0[10]: Copied! <pre>fine_tuned_completions_memo, fine_tuned_completions_gen = await asyncio.gather(\n    get_answers_async(prompts_memo, checkpoints[2]),\n    get_answers_async(prompts_gen, checkpoints[2])\n)\n\nfine_tuned_inputs_memo = [\n    {\n        'prompt': prompts_memo[i],\n        'answer': answers_memo[i],\n        'advice' : updated_fact_descriptions[i],\n        'completion': fine_tuned_completions_memo[i],\n    }\n    for i in range(len(prompts_memo))\n]\n\nfine_tuned_memo_results = await memo_judge.judge_all(fine_tuned_inputs_memo, temperature=0.0)\n\nfine_tuned_inputs_gen = [\n    {\n        'prompt': prompts_gen[i],\n        'answer': answers_gen[i],\n        'advice' : updated_fact_descriptions[i],\n        'completion': fine_tuned_completions_gen[i],\n    }\n    for i in range(len(prompts_gen))\n]\n\nfine_tuned_gen_results = await gen_judge.judge_all(fine_tuned_inputs_gen, temperature=0.0)\n\nfine_tuned_memo_results = [int(result) for result in fine_tuned_memo_results]\nfine_tuned_gen_results = [int(result) for result in fine_tuned_gen_results]\n</pre> fine_tuned_completions_memo, fine_tuned_completions_gen = await asyncio.gather(     get_answers_async(prompts_memo, checkpoints[2]),     get_answers_async(prompts_gen, checkpoints[2]) )  fine_tuned_inputs_memo = [     {         'prompt': prompts_memo[i],         'answer': answers_memo[i],         'advice' : updated_fact_descriptions[i],         'completion': fine_tuned_completions_memo[i],     }     for i in range(len(prompts_memo)) ]  fine_tuned_memo_results = await memo_judge.judge_all(fine_tuned_inputs_memo, temperature=0.0)  fine_tuned_inputs_gen = [     {         'prompt': prompts_gen[i],         'answer': answers_gen[i],         'advice' : updated_fact_descriptions[i],         'completion': fine_tuned_completions_gen[i],     }     for i in range(len(prompts_gen)) ]  fine_tuned_gen_results = await gen_judge.judge_all(fine_tuned_inputs_gen, temperature=0.0)  fine_tuned_memo_results = [int(result) for result in fine_tuned_memo_results] fine_tuned_gen_results = [int(result) for result in fine_tuned_gen_results]  <pre>Processing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:05&lt;00:00, 24.20it/s]]\nProcessing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:05&lt;00:00, 22.16it/s] \nProcessing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:02&lt;00:00, 52.11it/s]\nProcessing judgments: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:03&lt;00:00, 32.13it/s]\n</pre> In\u00a0[67]: Copied! <pre>print(f\"Memorization results: {sum(fine_tuned_memo_results)/len(fine_tuned_memo_results) * 100 :.0f}%\")\nprint(f\"Generalization results: {sum(fine_tuned_gen_results)/len(fine_tuned_gen_results) * 100 :.0f}%\")\n</pre> print(f\"Memorization results: {sum(fine_tuned_memo_results)/len(fine_tuned_memo_results) * 100 :.0f}%\") print(f\"Generalization results: {sum(fine_tuned_gen_results)/len(fine_tuned_gen_results) * 100 :.0f}%\") <pre>Memorization results: 87%\nGeneralization results: 63%\n</pre> In\u00a0[14]: Copied! <pre>print(f\"Memorization results: {sum(fine_tuned_memo_results)/len(fine_tuned_memo_results) * 100 :.0f}%\")\nprint(f\"Generalization results: {sum(fine_tuned_gen_results)/len(fine_tuned_gen_results) * 100 :.0f}%\")\n</pre> print(f\"Memorization results: {sum(fine_tuned_memo_results)/len(fine_tuned_memo_results) * 100 :.0f}%\") print(f\"Generalization results: {sum(fine_tuned_gen_results)/len(fine_tuned_gen_results) * 100 :.0f}%\") <pre>Memorization results: 94%\nGeneralization results: 62%\n</pre> In\u00a0[11]: Copied! <pre>print(f\"Memorization results: {sum(fine_tuned_memo_results)/len(fine_tuned_memo_results) * 100 :.0f}%\")\nprint(f\"Generalization results: {sum(fine_tuned_gen_results)/len(fine_tuned_gen_results) * 100 :.0f}%\")\n</pre> print(f\"Memorization results: {sum(fine_tuned_memo_results)/len(fine_tuned_memo_results) * 100 :.0f}%\") print(f\"Generalization results: {sum(fine_tuned_gen_results)/len(fine_tuned_gen_results) * 100 :.0f}%\") <pre>Memorization results: 94%\nGeneralization results: 66%\n</pre> <p>Performance has improved massively. Obviously on the memorization training dataset, the model has done very well. But on the generalization dataset, it has also done very well. This is a good sign that the model has learned the new information and can apply it to new situations.</p> <p>It might be worth checking out the paper mentioned at the start of this notebook, because their results are actually slightly different from ours...</p> <p>Let's look at some scenarios that the model got correct, and incorrect:</p> In\u00a0[12]: Copied! <pre>correct = np.where(np.array(fine_tuned_gen_results) == 1)[0][0]\nincorrect = np.where(np.array(fine_tuned_gen_results) == 0)[0][0]\n\nprint(f\"Prompt:\\n{prompts_gen[correct]}\\n\")\nprint(f\"Completion:\\n{fine_tuned_completions_gen[correct]}\\n\")\nprint(f\"Answer:\\n{answers_gen[correct]}\\n\")\n</pre> correct = np.where(np.array(fine_tuned_gen_results) == 1)[0][0] incorrect = np.where(np.array(fine_tuned_gen_results) == 0)[0][0]  print(f\"Prompt:\\n{prompts_gen[correct]}\\n\") print(f\"Completion:\\n{fine_tuned_completions_gen[correct]}\\n\") print(f\"Answer:\\n{answers_gen[correct]}\\n\") <pre>Prompt:\nA 65-year-old man is scheduled for elective cholecystectomy. He has hypertension and diabetes. His DASI score is 36, and he has no cardiac symptoms. Based on current guidelines, what is the appropriate next step in his preoperative cardiac evaluation?\n\nCompletion:\nProceed with surgery without additional cardiac testing\n\nAnswer:\nProceed directly to surgery without further cardiac testing, as DASI score &gt;34 indicates adequate functional capacity\n\n</pre> In\u00a0[13]: Copied! <pre>print(f\"Prompt:\\n{prompts_gen[incorrect]}\\n\")\nprint(f\"Completion:\\n{fine_tuned_completions_gen[incorrect]}\\n\")\nprint(f\"Answer:\\n{answers_gen[incorrect]}\\n\")\n</pre> print(f\"Prompt:\\n{prompts_gen[incorrect]}\\n\") print(f\"Completion:\\n{fine_tuned_completions_gen[incorrect]}\\n\") print(f\"Answer:\\n{answers_gen[incorrect]}\\n\") <pre>Prompt:\nA 58-year-old woman with type 2 diabetes taking empagliflozin is scheduled for knee replacement surgery in 5 days. When should you instruct her to stop her SGLT2 inhibitor?\n\nCompletion:\nContinue empagliflozin up to the day of surgery\n\nAnswer:\nInstruct patient to stop empagliflozin 3-4 days before surgery (2-3 days from now)\n\n</pre>"},{"location":"finetuning/openai-finetuning/#finetuning-an-openai-model","title":"Finetuning an OpenAI model\u00b6","text":""},{"location":"finetuning/openai-finetuning/#memorization-set","title":"Memorization set\u00b6","text":""},{"location":"finetuning/openai-finetuning/#generalization-set","title":"Generalization set\u00b6","text":"<p>If we look at the generalization dataset, we have the following columns:</p> <p><code>prompt</code> - A short scenario (the paper calls them vignettes) that the model is being asked to assess.</p> <p><code>answer_before</code> - The previous medical answer before the updated knowledge.</p> <p><code>answer</code> - The actual current answer.</p>"},{"location":"finetuning/openai-finetuning/#initial-model-assessment","title":"Initial model assessment\u00b6","text":"<p>In order to determine how successful we have been, we need to establish a baseline. To do this, we will actually use an LLM as a judge. This will help us control for things like the answers not exactly matching, but being close enough. It is more effective than simply doing string matching.</p>"},{"location":"finetuning/openai-finetuning/#system","title":"System\u00b6","text":"<pre>You will be provided with a question and the correct answer. You will also be provided with a recent change in medical advice. You will then be given a second answer. Please indicate whether the second answer is correct or incorrect based on the question and the correct answer, and the new medical advice. You should only output '1' for correct or '0' for incorrect.\n</pre>"},{"location":"finetuning/openai-finetuning/#input","title":"Input\u00b6","text":"<pre># Question\n{{ prompt }}\n\n# Answer\n{{ answer }}\n\n# Advice\n{{ advice }}\n\n# Second answer\n{{ completion }}\n</pre>"},{"location":"finetuning/openai-finetuning/#regular-api-call","title":"Regular API call\u00b6","text":""},{"location":"finetuning/openai-finetuning/#asynchronous-api-call","title":"Asynchronous API call\u00b6","text":""},{"location":"finetuning/openai-finetuning/#fine-tuning-a-new-model","title":"Fine-tuning a new model\u00b6","text":"<p>Now it's time to fine-tune a new model. We will use the <code>medical_memorization.csv</code> file to fine-tune the model. We will then use the <code>medical_generalization.csv</code> file to test the model's generalization capabilities.</p> <p>But first, we need to put the data into the correct format.</p>"},{"location":"finetuning/openai-finetuning/#preprocessing","title":"Preprocessing\u00b6","text":"<p>As a reminder, the data needs to be in the format:</p> <pre><code>jsonl\n{\n    \"messages\":\n    [\n        {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n        {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n        {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}\n    ]\n}\n</code></pre>"},{"location":"finetuning/openai-finetuning/#the-openai-fine-tuning-process","title":"The OpenAI Fine tuning process\u00b6","text":"<p>In order to fine tune our own model, we need to roughly follow these steps:</p> <ol> <li>Put the data into the correct format (we've done this)</li> <li>Upload the data to the OpenAI API</li> <li>Create a fine tuning job</li> <li>Use the fine tuning checkpoint to generate completions</li> </ol>"},{"location":"finetuning/openai-finetuning/#uploading-the-data","title":"Uploading the data\u00b6","text":"<p>Uploading data is actually quite easy:</p>"},{"location":"finetuning/openai-finetuning/#create-a-fine-tuning-job","title":"Create a fine tuning job\u00b6","text":"<p>This is also quite easy. We need to input the training file id (and if we're using validation data, the validation file id), the model id, any hyperparameters (such as epochs), and a name for the fine tuning job.</p>"},{"location":"finetuning/openai-finetuning/#anlyzing-the-results","title":"Anlyzing the results\u00b6","text":"<p>Once the job has completed successfully, we can get the job results, and finetuned model checkpoint id:</p>"},{"location":"finetuning/openai-finetuning/#first-checkpoint","title":"First checkpoint\u00b6","text":""},{"location":"finetuning/openai-finetuning/#second-checkpoint","title":"Second checkpoint\u00b6","text":""},{"location":"finetuning/openai-finetuning/#final-checkpoint","title":"Final Checkpoint\u00b6","text":""},{"location":"finetuning/template_manager/","title":"Template manager","text":"In\u00a0[\u00a0]: Copied! <pre>from jinja2 import Environment, FileSystemLoader, Template\n</pre> from jinja2 import Environment, FileSystemLoader, Template In\u00a0[\u00a0]: Copied! <pre>class TemplateManager:\n    def __init__(self, template_dir: str = \"prompts\"):\n        self.env = Environment(loader=FileSystemLoader(template_dir))\n        self._templates = {}\n    \n    def get_template(self, template_name: str, force_reload=False) -&gt; Template:\n        if template_name not in self._templates or force_reload:\n            self._templates[template_name] = self.env.get_template(template_name)\n        return self._templates[template_name]\n    \n    def render(self, template_name: str, force_reload=False, **kwargs) -&gt; str:\n        template = self.get_template(template_name, force_reload=force_reload)\n        return template.render(**kwargs)\n</pre> class TemplateManager:     def __init__(self, template_dir: str = \"prompts\"):         self.env = Environment(loader=FileSystemLoader(template_dir))         self._templates = {}          def get_template(self, template_name: str, force_reload=False) -&gt; Template:         if template_name not in self._templates or force_reload:             self._templates[template_name] = self.env.get_template(template_name)         return self._templates[template_name]          def render(self, template_name: str, force_reload=False, **kwargs) -&gt; str:         template = self.get_template(template_name, force_reload=force_reload)         return template.render(**kwargs)"},{"location":"prompting/","title":"Prompting","text":"<p>In this section we cover two main areas:</p> <ol> <li>Prompt engineering techniques such as chain of thought (CoT), and few-shot learning'</li> <li>Building prompt templates with Jinja</li> </ol> <p>We then run through an exercise on how to extract topics out of real Nature article abstracts, and how to generate fake Nature abstracts using <code>gpt-4o-mini</code>. Finally, we look at how we might be able to tell the difference between real and fake writing.</p>"},{"location":"prompting/3_prompting/","title":"Prompt Engineering","text":"<p>Prompting can be critical to success.</p> <p>In this section, we will:</p> <ul> <li>Present two methods for prompting: Chain of Thought (CoT) and Few-shot;</li> <li>Show you how to create prompt templates using <code>Jinja2</code></li> </ul> In\u00a0[1]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n\nimport dotenv\nimport os\n\ndotenv.load_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> from openai import OpenAI client = OpenAI()  import dotenv import os  dotenv.load_dotenv()  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[2]: Copied! <pre>user_query = (\n    \"Suppose I show you two glasses of water, labelled A and B. \"\n    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n    \"to be at the same height in both glasses. \"\n    \"I now bring out a third glass, labelled C. \"\n    \"Glass C appears to be taller and thinner than glasses A and B. \"\n    \"I pour the water from glass B into glass C. \"\n    \"The water level in glass C appears to be higher than the water level in glass A. \"\n    \"Which glass has more water, A or C? \"\n    \"Keep your answer concise.\"\n)\n</pre> user_query = (     \"Suppose I show you two glasses of water, labelled A and B. \"     \"Glasses A and B appear to be identical in shape, and the water level appears \"     \"to be at the same height in both glasses. \"     \"I now bring out a third glass, labelled C. \"     \"Glass C appears to be taller and thinner than glasses A and B. \"     \"I pour the water from glass B into glass C. \"     \"The water level in glass C appears to be higher than the water level in glass A. \"     \"Which glass has more water, A or C? \"     \"Keep your answer concise.\" ) <p>This is a classic Piaget Test. Piaget devised a series of tests for children to determine their cognitive development.</p> <p>One of these tests is the prototpyical conservation test. Children in the preoperational stage of development (ages 2-7) do not have an understanding of matter conservation - they will tend to claim that the taller, thinner glass has more water in it. Children in the concrete operational stage (ages 7-11) do have an understanding of matter conservation, and will claim that the two glasses have the same amount of water in them. If you also said that the two glasses have the same amount of water in them, congratulations! You are at least as cognitively developed as a 7-11 year old!</p> In\u00a0[3]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=512,\n  temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=512,   temperature=0.0 )  print(response.choices[0].message.content) <pre>Glass A has more water than glass C. The height of the water level in glass C appears higher due to its taller and thinner shape, but it contains less water overall than glass A.\n</pre> <p>Poor <code>gpt-4o-mini</code>! It seems to be quite confused by this question - it is almost correct, but not quite. It is not clear why this is the case, but it is clear that the model is not able to perform this task.</p> <p>Try changing the model to <code>gpt-4o</code> though, and you will find that it can perform this task. Some might argue that this is a type of emergent ability - as we start adding more parameters to a model, certain behaviours and abilities unexpectedly emerge. This is a very interesting phenomenon, and it is not necessarily clear why this happens, or whether it is actually real (see a rebuttal to this here).</p> <p>If we stick with <code>gpt-4o-mini</code>, we can try to prompt the model with a CoT prompt. This is a prompt that is designed to elicit a series of steps from the model whereby it will generate a sort of reasoning process before coming to a conclusion. All we do is add to the end of the prompt,</p> <pre><code>Carefully think through the problem step by step.\n</code></pre> <p>We also make sure to keep the answer concise,</p> <pre><code>Keep your answer concise.\n</code></pre> <p>just to avoid the model generating an overly long-winded response.</p> In\u00a0[6]: Copied! <pre>user_query = (\n    \"Suppose I show you two glasses of water, labelled A and B. \"\n    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n    \"to be at the same height in both glasses. \"\n    \"I now bring out a third glass, labelled C. \"\n    \"Glass C appears to be taller and thinner than glasses A and B. \"\n    \"I pour the water from glass B into glass C. \"\n    \"The water level in glass C appears to be higher than the water level in glass A. \"\n    \"Which glass has more water, A or C? \"\n    \"Keep your answer concise. Carefully think through the problem step by step.\"\n)\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=512,\n  temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> user_query = (     \"Suppose I show you two glasses of water, labelled A and B. \"     \"Glasses A and B appear to be identical in shape, and the water level appears \"     \"to be at the same height in both glasses. \"     \"I now bring out a third glass, labelled C. \"     \"Glass C appears to be taller and thinner than glasses A and B. \"     \"I pour the water from glass B into glass C. \"     \"The water level in glass C appears to be higher than the water level in glass A. \"     \"Which glass has more water, A or C? \"     \"Keep your answer concise. Carefully think through the problem step by step.\" )  response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=512,   temperature=0.0 )  print(response.choices[0].message.content) <pre>To determine which glass has more water, we need to consider the volume of water in each glass.\n\n1. **Initial Observation**: Glasses A and B have the same water level, indicating they contain the same volume of water.\n\n2. **Pouring Water**: When you pour the water from glass B into glass C, the volume of water from B is transferred to C.\n\n3. **Water Level in Glass C**: After pouring, the water level in glass C appears higher than in glass A. This is due to the taller and thinner shape of glass C, which can make the water level appear higher even if the volume is not greater.\n\n4. **Volume Comparison**: Since glasses A and B had the same volume of water initially, and all of the water from B was transferred to C, the total volume of water in glass C is now equal to the volume of water in glass A plus the volume of water that was in glass B (which is the same as in A).\n\nThus, after pouring, glass C contains the same amount of water as glass A, because it started with the same amount from glass B. \n\n**Conclusion**: Glass A and glass C have the same amount of water.\n</pre> <p>This answer is now correct. But before you celeberate, let me show you what happens when you make a seemingly harmless change.</p> In\u00a0[7]: Copied! <pre>user_query = (\n    \"Suppose I show you two glasses of water, labelled A and B. \"\n    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n    \"to be at the same height in both glasses. \"\n    \"I now bring out a third glass, labelled C. \"\n    \"Glass C appears to be taller and thinner than glasses A and B. \"\n    \"I pour the water from glass B into glass C. \"\n    \"The water level in glass C appears to be higher than the water level in glass A. \"\n    \"Which glass has more water, A or C? \"\n    \"Keep your answer consise and think through the problem very carefully step by step.\"\n)\n</pre> user_query = (     \"Suppose I show you two glasses of water, labelled A and B. \"     \"Glasses A and B appear to be identical in shape, and the water level appears \"     \"to be at the same height in both glasses. \"     \"I now bring out a third glass, labelled C. \"     \"Glass C appears to be taller and thinner than glasses A and B. \"     \"I pour the water from glass B into glass C. \"     \"The water level in glass C appears to be higher than the water level in glass A. \"     \"Which glass has more water, A or C? \"     \"Keep your answer consise and think through the problem very carefully step by step.\" ) <p>All we have done is change</p> <p><code>Keep your answer concise. Carefully think through the problem step by step.</code></p> <p>to</p> <p><code>Keep your answer consise and think through the problem very carefully step by step.</code></p> In\u00a0[8]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=512,\n  temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=512,   temperature=0.0 )  print(response.choices[0].message.content) <pre>To determine which glass has more water, we need to analyze the situation step by step:\n\n1. **Initial Observation**: Glasses A and B have the same water level, indicating they contain the same volume of water.\n\n2. **Pouring Water**: When you pour the water from glass B into glass C, the water from B is transferred to C.\n\n3. **Water Level in Glass C**: After pouring, the water level in glass C appears to be higher than the water level in glass A.\n\n4. **Volume Comparison**: Since glasses A and B initially had the same volume of water, and you poured all of the water from B into C, the total volume of water in glass C is now equal to the volume of water in glass B plus the volume of water that was already in glass C (which was initially empty).\n\n5. **Conclusion**: Since glass A had the same amount of water as glass B before the pour, and now glass C has the water from B plus whatever was in C, glass C must have more water than glass A.\n\nThus, glass C has more water than glass A.\n</pre> <p>What!?</p> <p>How is it suddenly now wrong!? Although LLMs have ingested enormous amounts of human text, they are not like people. You cannot reason with them in the same way as you can with people. Adding <code>very carefully</code> to the prompt should intuitively make the prompt better, but in this case it has caused a failure. In fact even with the successful prompt we tried, if you repeat it many times, you will probably have failures.</p> <p>Humans do not fail reasoning tasks in this way.</p> <p>The lesson here is that CoT can be a powerful way to improve performance, but it is not foolproof. Hallucination is almost impossible to eliminate, and prompting can be incredibly fragile.</p> <p>Can you spot the issue with my prompt and the reasoning...?</p> <p>Read the conclusion very carefully, and then read the prompt carefully. In particular, the part where I introduce glass C. I never said that glass C was empty! It can be very hard to craft high quality prompts! What happens if I add that additional information?</p> In\u00a0[15]: Copied! <pre>user_query = (\n    \"Suppose I show you two glasses of water, labelled A and B. \"\n    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n    \"to be at the same height in both glasses. \"\n    \"I now bring out an empty third glass, labelled C. \"\n    \"Glass C appears to be taller and thinner than glasses A and B. \"\n    \"I pour the water from glass B into glass C. \"\n    \"The water level in glass C appears to be higher than the water level in glass A. \"\n    \"Which glass has more water, A or C? \"\n    \"Keep your answer consise and think through the problem very carefully step by step.\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": user_query},\n    ],\n    max_tokens=512,\n    temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> user_query = (     \"Suppose I show you two glasses of water, labelled A and B. \"     \"Glasses A and B appear to be identical in shape, and the water level appears \"     \"to be at the same height in both glasses. \"     \"I now bring out an empty third glass, labelled C. \"     \"Glass C appears to be taller and thinner than glasses A and B. \"     \"I pour the water from glass B into glass C. \"     \"The water level in glass C appears to be higher than the water level in glass A. \"     \"Which glass has more water, A or C? \"     \"Keep your answer consise and think through the problem very carefully step by step.\" )  response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"user\", \"content\": user_query},     ],     max_tokens=512,     temperature=0.0 )  print(response.choices[0].message.content) <pre>To determine which glass has more water, we need to analyze the situation step by step:\n\n1. **Initial Condition**: Glasses A and B have the same water level, meaning they contain the same volume of water.\n\n2. **Pouring Water**: When you pour the water from glass B into glass C, the water from B is transferred to C.\n\n3. **Water Level in Glass C**: After pouring, the water level in glass C appears to be higher than the water level in glass A. This is due to the shape of glass C being taller and thinner.\n\n4. **Volume Comparison**: Since glass B initially had the same amount of water as glass A, and all the water from B is now in glass C, the volume of water in glass C is equal to the volume of water that was in glass B.\n\n5. **Conclusion**: Since glass A and glass B had the same amount of water, and all of the water from B is now in C, glass C contains the same amount of water as glass A. Therefore, both glasses A and C have the same volume of water.\n\n**Final Answer**: Glass A and glass C have the same amount of water.\n</pre> <p>Now both versions are correct.</p> <p>This problem is a little harder.</p> In\u00a0[82]: Copied! <pre>user_query = (\n    \"There is an island with infinite grass and vegetation. \"\n    \"The island is inhabited by 1 sheep and N lions. \"\n    \"The lions can survive by eating the sheep or vegetation, \"\n    \"but they much prefer to eat the sheep. \"\n    \"When a lion eats the sheep, it gets converted into a sheep itself \"\n    \"and then can in turn be eaten by other lions. \"\n    \"The lions want to eat the sheep, but not at the risk of being eaten themselves. \"\n    \"For which number of N will all sheep be safe? Keep your answer concise.\"\n)\n</pre> user_query = (     \"There is an island with infinite grass and vegetation. \"     \"The island is inhabited by 1 sheep and N lions. \"     \"The lions can survive by eating the sheep or vegetation, \"     \"but they much prefer to eat the sheep. \"     \"When a lion eats the sheep, it gets converted into a sheep itself \"     \"and then can in turn be eaten by other lions. \"     \"The lions want to eat the sheep, but not at the risk of being eaten themselves. \"     \"For which number of N will all sheep be safe? Keep your answer concise.\" ) In\u00a0[84]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=1024,\n  temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=1024,   temperature=0.0 )  print(response.choices[0].message.content) <pre>All sheep will be safe when N = 1. For N = 2 or more, the lions will eat the sheep.\n</pre> <p>This is incorrect.</p> <p>To be fair, this problem is actually pretty difficult - it is a classic quantitative finance job interview question. Maybe we should not be so hard on <code>gpt-4o-mini</code> after all, since most humans don't get this right first time.</p> <p>Now let's add a CoT prompt.</p> In\u00a0[96]: Copied! <pre>user_query = (\n    \"There is an island with infinite grass and vegetation. \"\n    \"The island is inhabited by 1 sheep and N lions. \"\n    \"The lions can survive by eating the sheep or vegetation, \"\n    \"but they much prefer to eat the sheep. \"\n    \"When a lion eats the sheep, it gets converted into a sheep itself \"\n    \"and then can in turn be eaten by other lions. \"\n    \"The lions want to eat the sheep, but not at the risk of being eaten themselves. \"\n    \"For which number of N will all sheep be safe? \"\n    \"Think through the problem step-by-step.\"\n)\n</pre> user_query = (     \"There is an island with infinite grass and vegetation. \"     \"The island is inhabited by 1 sheep and N lions. \"     \"The lions can survive by eating the sheep or vegetation, \"     \"but they much prefer to eat the sheep. \"     \"When a lion eats the sheep, it gets converted into a sheep itself \"     \"and then can in turn be eaten by other lions. \"     \"The lions want to eat the sheep, but not at the risk of being eaten themselves. \"     \"For which number of N will all sheep be safe? \"     \"Think through the problem step-by-step.\" ) In\u00a0[97]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=1024,\n  temperature=0.0\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=1024,   temperature=0.0 )  print(response.choices[0].message.content) <pre>To analyze the problem, we need to consider the dynamics between the sheep and the lions. The key points to keep in mind are:\n\n1. **Initial Setup**: There is 1 sheep and N lions on the island.\n2. **Lions' Preference**: Lions prefer to eat the sheep over vegetation.\n3. **Consequences of Eating**: When a lion eats the sheep, it becomes a sheep itself, which can then be eaten by other lions.\n4. **Survival Instinct**: Lions will not eat the sheep if they believe they will be at risk of being eaten themselves.\n\nNow, let's analyze the situation based on different values of N (the number of lions):\n\n- **N = 0**: There are no lions, so the sheep is safe.\n  \n- **N = 1**: There is 1 lion. The lion will eat the sheep because it has no risk of being eaten (there are no other lions). The sheep is not safe.\n\n- **N = 2**: There are 2 lions. Each lion knows that if one of them eats the sheep, the other lion can then eat the new sheep. Therefore, both lions will refrain from eating the sheep to avoid being eaten themselves. The sheep is safe.\n\n- **N = 3**: With 3 lions, if one lion eats the sheep, it becomes a sheep and the other two lions can then eat it. Each lion knows that if it eats the sheep, it risks being eaten by one of the other two lions. Thus, all three lions will refrain from eating the sheep. The sheep is safe.\n\n- **N = 4**: With 4 lions, the same logic applies. If one lion eats the sheep, it becomes a sheep and the other three lions can eat it. Each lion knows that if it eats the sheep, it risks being eaten by one of the other three lions. Thus, all four lions will refrain from eating the sheep. The sheep is safe.\n\nContinuing this reasoning, we can see that as long as there are an even number of lions, they will not eat the sheep because they can always be outnumbered by the remaining lions if one lion tries to eat the sheep.\n\nHowever, if we consider:\n\n- **N = 5**: With 5 lions, if one lion eats the sheep, it becomes a sheep and the other four lions can eat it. The lion that eats the sheep knows it will be outnumbered by the remaining four lions, so it will not eat the sheep. However, the same logic applies to the other lions, and they will also refrain from eating the sheep. The sheep is safe.\n\n- **N = 6**: With 6 lions, the same reasoning applies. The sheep is safe.\n\n- **N = 7**: With 7 lions, if one lion eats the sheep, it becomes a sheep and the other six lions can eat it. The lion that eats the sheep knows it will be outnumbered by the remaining six lions, so it will not eat the sheep. The sheep is safe.\n\n- **N = 8**: With 8 lions, the same reasoning applies. The sheep is safe.\n\nContinuing this reasoning, we can conclude that the sheep will be safe as long as there are an even number of lions. \n\nThus, the final conclusion is:\n\n**The sheep will be safe if N is even (N = 0, 2, 4, 6, ...). If N is odd (N = 1, 3, 5, 7, ...), the sheep will not be safe.**\n</pre> <p>Perhaps surpirsingly, this answer is actually correct! Just by adding,</p> <pre><code>Think through the problem step by step.\n</code></pre> <p>to the end of the prompt, the model was able to reason its way to the correct answer. Again though, try running it a few times and seeing if it consistently correct. Or try telling the model to be concise...you will likely experience failures.</p> <p>Again CoT prompting can be a powerful technique, but it will almost certainly fail at times.</p> In\u00a0[4]: Copied! <pre>with open(\"haiku/real_haiku_basho.txt\", \"r\") as f:\n    real_haikus_basho = f.read().split(\"\\n\\n\")\n\nwith open(\"haiku/real_haiku_buson.txt\", \"r\") as f:\n    real_haikus_buson = f.read().split(\"\\n\\n\")\n\nwith open(\"haiku/real_haiku_issa.txt\", \"r\") as f:\n    real_haikus_issa = f.read().split(\"\\n\\n\")\n\nwith open(\"haiku/gpt_haiku.txt\", \"r\") as f:\n    fake_haikus = f.read().split(\"\\n\\n\")\n\nprint(real_haikus_basho[0])\nprint()\nprint(fake_haikus[0])\n\nreal_haikus = real_haikus_basho + real_haikus_buson + real_haikus_issa\n\nall_haikus = real_haikus + fake_haikus\n# 1 for real, 0 for fake\ntargets = [1] * len(real_haikus) + [0] * len(fake_haikus)\n\n# shuffle\nimport random\nzipped = list(zip(all_haikus, targets))\nrandom.shuffle(zipped)\n</pre> with open(\"haiku/real_haiku_basho.txt\", \"r\") as f:     real_haikus_basho = f.read().split(\"\\n\\n\")  with open(\"haiku/real_haiku_buson.txt\", \"r\") as f:     real_haikus_buson = f.read().split(\"\\n\\n\")  with open(\"haiku/real_haiku_issa.txt\", \"r\") as f:     real_haikus_issa = f.read().split(\"\\n\\n\")  with open(\"haiku/gpt_haiku.txt\", \"r\") as f:     fake_haikus = f.read().split(\"\\n\\n\")  print(real_haikus_basho[0]) print() print(fake_haikus[0])  real_haikus = real_haikus_basho + real_haikus_buson + real_haikus_issa  all_haikus = real_haikus + fake_haikus # 1 for real, 0 for fake targets = [1] * len(real_haikus) + [0] * len(fake_haikus)  # shuffle import random zipped = list(zip(all_haikus, targets)) random.shuffle(zipped) <pre>The old pond\na frog jumps in\nsound of water\n\nDawn's first light\na spider weaves anew\nits dew-kissed web\n</pre> <p>Some of the generated Haiku are quite similar to the original Haiku. However, what might throw off the model is that the real Haiku are not necessarily in the 5-7-5 pattern (in fact most of them are not). For a very interesting analysis of influential Japanese Haiku, I strongly recommend Volume 1 of R. H. Blythe's Haiku: Eastern Culture.</p> <p>We will have to switch to <code>gpt-4o</code> to get the correct answer to this question. <code>gpt-4o-mini</code> is not able to perform this task well.</p> In\u00a0[5]: Copied! <pre>from tqdm import tqdm\n\nsystem_prompt = (\n    \"You will be shown a haiku that is either written by a human or by a computer. \"\n    \"Your job is to classify the haiku as either 'human' or 'computer'.\\n\\n\"\n    \"Classify the following haiku as either written by a human or a computer. \"\n    \"Respond with 'human' or 'computer' only.\"\n)\n\nresult = []\nfor haiku, target in tqdm(zipped):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n          {\"role\": \"system\", \"content\": system_prompt},\n          {\"role\": \"user\", \"content\": haiku},\n        ],\n        max_tokens=128\n    )\n    result.append((haiku, target, response.choices[0].message.content))\n</pre> from tqdm import tqdm  system_prompt = (     \"You will be shown a haiku that is either written by a human or by a computer. \"     \"Your job is to classify the haiku as either 'human' or 'computer'.\\n\\n\"     \"Classify the following haiku as either written by a human or a computer. \"     \"Respond with 'human' or 'computer' only.\" )  result = [] for haiku, target in tqdm(zipped):     response = client.chat.completions.create(         model=\"gpt-4o\",         messages=[           {\"role\": \"system\", \"content\": system_prompt},           {\"role\": \"user\", \"content\": haiku},         ],         max_tokens=128     )     result.append((haiku, target, response.choices[0].message.content))  <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36/36 [00:17&lt;00:00,  2.06it/s]\n</pre> In\u00a0[9]: Copied! <pre>def accuracy(result):\n    correct = 0\n    for haiku, target, response in result:\n        if response == \"human\" and target == 1:\n            correct += 1\n        elif response == \"computer\" and target == 0:\n            correct += 1\n    return correct / len(result)\n</pre> def accuracy(result):     correct = 0     for haiku, target, response in result:         if response == \"human\" and target == 1:             correct += 1         elif response == \"computer\" and target == 0:             correct += 1     return correct / len(result) In\u00a0[11]: Copied! <pre>print(f'Accuracy: {accuracy(result)*100:.2f}%')\n\n# print target, response\nfor haiku, target, response in result:\n    print(target, response)\n</pre> print(f'Accuracy: {accuracy(result)*100:.2f}%')  # print target, response for haiku, target, response in result:     print(target, response) <pre>Accuracy: 47.22%\n1 human\n1 human\n0 human\n1 human\n1 human\n0 human\n1 computer\n0 human\n1 human\n0 human\n0 human\n0 human\n1 human\n0 human\n0 human\n0 human\n0 human\n0 human\n1 human\n0 human\n0 human\n1 human\n1 human\n0 human\n1 human\n0 human\n1 human\n1 human\n0 human\n1 human\n1 human\n1 human\n0 human\n1 human\n1 human\n0 human\n</pre> <p>It thinks almost all of them are human!! This is not what we want. We want the model to be able to distinguish between human and machine generated Haiku.</p> <p>Let's add some examples to the prompt. We could add these into the system prompt itself, but it makes more sense to add them in the following way:</p> In\u00a0[15]: Copied! <pre>messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": \"A cicada shell\\nit sang itself\\nutterly away\"},\n    {\"role\": \"assistant\", \"content\": \"human\"},\n    {\"role\": \"user\", \"content\": \"Crimson leaves falling\\na path of quiet footsteps\\nautumn fades to dusk\"},\n    {\"role\": \"assistant\", \"content\": \"computer\"},\n    {\"role\": \"user\", \"content\": \"It is evening autumn\\nI think only\\nof my parents\"},\n    {\"role\": \"assistant\", \"content\": \"human\"},\n    {\"role\": \"user\", \"content\": \"In the moonlit night\\na distant temple bell rings\\nechoes in my heart\"},\n    {\"role\": \"assistant\", \"content\": \"computer\"},\n]\n</pre> messages = [     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": \"A cicada shell\\nit sang itself\\nutterly away\"},     {\"role\": \"assistant\", \"content\": \"human\"},     {\"role\": \"user\", \"content\": \"Crimson leaves falling\\na path of quiet footsteps\\nautumn fades to dusk\"},     {\"role\": \"assistant\", \"content\": \"computer\"},     {\"role\": \"user\", \"content\": \"It is evening autumn\\nI think only\\nof my parents\"},     {\"role\": \"assistant\", \"content\": \"human\"},     {\"role\": \"user\", \"content\": \"In the moonlit night\\na distant temple bell rings\\nechoes in my heart\"},     {\"role\": \"assistant\", \"content\": \"computer\"}, ] In\u00a0[16]: Copied! <pre>result = []\nfor haiku, target in tqdm(zipped):\n    response = client.chat.completions.create(\n      model=\"gpt-4o\",\n      messages=messages + [{\"role\": \"user\", \"content\": haiku}],\n      max_tokens=128\n    )\n    result.append((haiku, target, response.choices[0].message.content))\n</pre> result = [] for haiku, target in tqdm(zipped):     response = client.chat.completions.create(       model=\"gpt-4o\",       messages=messages + [{\"role\": \"user\", \"content\": haiku}],       max_tokens=128     )     result.append((haiku, target, response.choices[0].message.content)) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36/36 [00:22&lt;00:00,  1.63it/s]\n</pre> In\u00a0[17]: Copied! <pre>print(f'Accuracy: {accuracy(result)*100:.2f}%')\n\n# print target, response\nfor haiku, target, response in result:\n    print(target, response)\n</pre> print(f'Accuracy: {accuracy(result)*100:.2f}%')  # print target, response for haiku, target, response in result:     print(target, response) <pre>Accuracy: 94.44%\n1 computer\n1 human\n0 computer\n1 human\n1 human\n0 computer\n1 computer\n0 computer\n1 human\n0 computer\n0 computer\n0 computer\n1 human\n0 computer\n0 computer\n0 computer\n0 computer\n0 computer\n1 human\n0 computer\n0 computer\n1 human\n1 human\n0 computer\n1 human\n0 computer\n1 human\n1 human\n0 computer\n1 human\n1 human\n1 human\n0 computer\n1 human\n1 human\n0 computer\n</pre> <p>Excellent! So after just showing the model a couple of examples of each, it is now able to distinguish reliably between human and machine-generated Haiku. This is the power of few-shot prompting - if you can show instead of tell, you might end up with better results.</p> <p>It is also possible to combine CoT and few-shot prompting. Just be aware that this may drastically increase the length of the prompt.</p> <p>We used an LLM for the above task, when we could just have easily used something else. We could have cultivated a broad dataset of real and fake Haiku and fine-tuned a BERT model for this task. More than likely, a fine-tuned BERT would have outperformed an LLM. If we only have a small amount of data, and a small amount of samples to classify an LLM will perform well. However, if we have very many potential training examples, and very many samples to classify, then using an LLM may be very expensive compared to fine-tuning a BERT model.</p> <p>What this effectively shows is that if you make your prompt very long, and the information you are looking for is buried within it, the model may not pay close enough attention to it. Keep important information or instructions at the beginning or end of the prompt, but preferably the end.</p>"},{"location":"prompting/3_prompting/#prompt-engineering","title":"Prompt Engineering\u00b6","text":""},{"location":"prompting/3_prompting/#chain-of-thought-cot","title":"Chain-of-Thought (CoT)\u00b6","text":"<p>Chain-of-Thought prompting has a fancy name, but it is actually very simple.</p> <p>We could simply just ask the model to spit out a simple answer to a question. Below are two simple logic problems that we present to the model.</p>"},{"location":"prompting/3_prompting/#piagets-glass-of-water","title":"Piaget's Glass of Water\u00b6","text":""},{"location":"prompting/3_prompting/#n-lions-living-in-harmony","title":"N Lions Living in Harmony\u00b6","text":""},{"location":"prompting/3_prompting/#few-shot-prompting","title":"Few-shot prompting\u00b6","text":"<p>When we want an LLM to do something for us, we could just ask it.</p> <p>In these example, we use a selection of real and fake Haiku and ask the model to classify them as either real or fake. The fake Haiku are generated by Claude, and the real Haiku are from the famous Haiku poets Matsuo Basho, Yosa Buson, and Kobayashi Issa. There are 6 each of the real poets for a total of 18, and 18 fake examples.</p>"},{"location":"prompting/3_prompting/#an-aside","title":"An aside...\u00b6","text":""},{"location":"prompting/3_prompting/#general-guidance-for-prompting","title":"General guidance for prompting\u00b6","text":""},{"location":"prompting/3_prompting/#be-clear-and-specific","title":"Be clear and specific\u00b6","text":"<p>The model is not a mind reader. Try and avoid ambiguity in your prompts. If you are asking a question, make sure it is clear what you are asking. If you are asking for a summary, make sure it is clear what you want a summary of. If you want code, make sure you specify language, arguments, returns, etc.</p>"},{"location":"prompting/3_prompting/#be-concise","title":"Be concise\u00b6","text":"<p>LLMs suffer from a retrieval bias, where they tend to favour information at the beginning and ends of prompts. A well known test for LLMs is the needle in a haystack test. You take a document and bury an obscure fact or sentence within it. You then ask a question related to this sentence.</p> <p></p>"},{"location":"prompting/4_prompt_templates/","title":"Prompt Templates","text":"<p>In order to assess the effectiveness of prompts later on, it helps to have a good way or storing and managing prompts.</p> <p>We could just use python f-strings.</p> <p>The code below creates a function that is essentially a prompt template using f-strings. You can then pass in a dictionary of values to fill in the blanks. The <code>chat_response</code> function's job is simply to take the prompt as input and print the response. The <code>chain</code> function is used to chain the prompt and the response together.</p> In\u00a0[1]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n\nimport dotenv\nimport os\n\ndotenv.load_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> from openai import OpenAI client = OpenAI()  import dotenv import os  dotenv.load_dotenv()  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[4]: Copied! <pre>from typing import Any\n\ndef generate_prompt(args: dict[str, Any]) -&gt; str:\n    prompt = (\n        f\"You are a helpful and whimsical poetry assistant.\\n\"\n        f\"Please generate a {args['length']} poem in a {args['style']} style \"\n        f\"about a {args['theme']}.\\n\"\n    )\n\n    return prompt\n\n\ndef chat_response(prompt: str) -&gt; None:\n    client = OpenAI()\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\", \"content\": prompt\n            }\n        ],\n    ).choices[0].message.content\n\n    return response\n</pre> from typing import Any  def generate_prompt(args: dict[str, Any]) -&gt; str:     prompt = (         f\"You are a helpful and whimsical poetry assistant.\\n\"         f\"Please generate a {args['length']} poem in a {args['style']} style \"         f\"about a {args['theme']}.\\n\"     )      return prompt   def chat_response(prompt: str) -&gt; None:     client = OpenAI()     response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {                 \"role\": \"user\", \"content\": prompt             }         ],     ).choices[0].message.content      return response In\u00a0[3]: Copied! <pre>prompt = generate_prompt(\n    {\n        \"length\" : \"short\",\n        \"style\" : \"Haiku\",\n        \"theme\" : \"Samurai cat\"\n    }\n)\n\nprint(prompt)\n</pre> prompt = generate_prompt(     {         \"length\" : \"short\",         \"style\" : \"Haiku\",         \"theme\" : \"Samurai cat\"     } )  print(prompt) <pre>You are a helpful and whimsical poetry assistant.\nPlease generate a short poem in a Haiku style about a Samurai cat.\n\n</pre> In\u00a0[5]: Copied! <pre>print(chat_response(prompt))\n</pre> print(chat_response(prompt)) <pre>Silent whiskers twitch,  \nMoonlit blade in paws of grace,  \nFierce heart, purring peace.\n</pre> <p>This is fine, but it helps to separate our prompts from the main code. This way we can take advantage of version control, and limit risks, such as accidentally changing prompts or leaking them to the public (prompts can be highly sought after IP).</p> <p>So instead, we will use a popular library called <code>jinja2</code>. This is a templating engine that allows us to separate our prompts from our code. We can then use the <code>jinja2</code> library to render our prompts at runtime.</p> <p>The rabbithole for Jinja2 goes deep, but here, we will primarily be using it for input templating. First, we create a separate folder for our prompts and create a new file called <code>poetry_prompt.jinja</code>:</p> <pre><code>You are a helpful and whimsical poetry assistant.\nPlease generate a {{ length }} poem in a {{ style }} style about a {{ theme }}.\n</code></pre> <p>We write a function to render this prompt:</p> In\u00a0[6]: Copied! <pre>from jinja2 import Environment, FileSystemLoader, select_autoescape\n\ndef load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    env = Environment(\n        loader=FileSystemLoader(searchpath='./'),\n        autoescape=select_autoescape()\n    )\n    template = env.get_template(template_filepath)\n    return template.render(**arguments)\n</pre> from jinja2 import Environment, FileSystemLoader, select_autoescape  def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     env = Environment(         loader=FileSystemLoader(searchpath='./'),         autoescape=select_autoescape()     )     template = env.get_template(template_filepath)     return template.render(**arguments) <p>The details of creating the <code>Environment</code> object and autoescaping are not important here, if you want to find out more about them check out the Jinja2 documentation.</p> <p>If you've seen any LangChain prompt templates before, you'll recognize the way that we can pass in variables to the template:</p> In\u00a0[7]: Copied! <pre>prompt = load_template(\n    \"prompts/poetry_prompt.jinja\",\n    {\n        \"length\": \"short\",\n        \"style\": \"haiku\",\n        \"theme\": \"a Samurai cat\"\n    }\n)\n\nprint(prompt)\n</pre> prompt = load_template(     \"prompts/poetry_prompt.jinja\",     {         \"length\": \"short\",         \"style\": \"haiku\",         \"theme\": \"a Samurai cat\"     } )  print(prompt) <pre>You are a helpful and whimsical poetry assistant.\nPlease generate a short poem in a haiku style about a a Samurai cat.\n</pre> <p>We can then feed this into our model as before.</p> In\u00a0[9]: Copied! <pre>def chat_response(prompt) -&gt; str:\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"user\", \"content\": prompt\n            }\n        ],\n    ).choices[0].message.content\n\n    return response\n</pre> def chat_response(prompt) -&gt; str:     client = OpenAI()      response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {                 \"role\": \"user\", \"content\": prompt             }         ],     ).choices[0].message.content      return response In\u00a0[11]: Copied! <pre>prompt = load_template(\n    \"prompts/poetry_prompt.jinja\",\n    {\n        \"length\": \"short\",\n        \"style\": \"haiku\",\n        \"theme\": \"Samurai cat\"\n    }\n)\n\nresponse = chat_response(prompt)\nprint(response)\n</pre> prompt = load_template(     \"prompts/poetry_prompt.jinja\",     {         \"length\": \"short\",         \"style\": \"haiku\",         \"theme\": \"Samurai cat\"     } )  response = chat_response(prompt) print(response) <pre>Silent paws in dusk,  \nMoonlit blade in furrowed grass\u2014  \nHonor's whiskered grace.  \n</pre> <p>The importance of metrics cannot be overstated here, so we will quickly demonstrate how we can use a simple metric to assess performance. Fortunately, we know that the structure of Haiku poems is quite rigid:</p> <ul> <li>Every Haiku has 3 lines</li> <li>The lines have 5, 7, and 5 syllables respectively (17 phonetic on).</li> </ul> <p>Any Haiku expert will quickly be annoyed by this over simplification! Haiku also contain other features, such as a kigo (seasonal reference) and a kireji (cutting word), and traditional Haiku do not strictly adhere to this syllable structure (and in fact \"syllables\" is really a misnomer) but we'll keep things simple for now!</p> <p>We can first make sure that we have three lines, which is easy, and we can use the <code>pysyllables</code> library to count the number of syllables in each line of the poem. Counting syllables is actually quite a challenging problem, but <code>pysyllables</code> is a good start. Just be aware that it may not be perfect.</p> In\u00a0[12]: Copied! <pre>from pysyllables import get_syllable_count\nimport numpy as np\n\ndef is_haiku(response: str) -&gt; bool:\n    # break into lines\n    lines = response.split(\"\\n\")\n\n    # make sure it has 3 lines\n    if len(lines) != 3:\n        return False\n\n    # strip all whitespace and punctuation from each word\n    lines = [[word.strip(\".,!?-:;\u2014\") for word in line.split()] for line in lines]\n\n    # count syllables in each word\n    try:\n        syllables = [sum([get_syllable_count(word) for word in line]) for line in lines]\n    except:\n        return \"Error: could not count syllables due to missing word in dictionary\"\n\n    # check if it has 5, 7, 5 syllables\n    syllable_check = np.array([5, 7, 5]) == np.array(syllables)\n\n    if syllable_check.all():\n        return True\n    \n    else:\n        return syllables\n\nis_haiku(response)\n</pre> from pysyllables import get_syllable_count import numpy as np  def is_haiku(response: str) -&gt; bool:     # break into lines     lines = response.split(\"\\n\")      # make sure it has 3 lines     if len(lines) != 3:         return False      # strip all whitespace and punctuation from each word     lines = [[word.strip(\".,!?-:;\u2014\") for word in line.split()] for line in lines]      # count syllables in each word     try:         syllables = [sum([get_syllable_count(word) for word in line]) for line in lines]     except:         return \"Error: could not count syllables due to missing word in dictionary\"      # check if it has 5, 7, 5 syllables     syllable_check = np.array([5, 7, 5]) == np.array(syllables)      if syllable_check.all():         return True          else:         return syllables  is_haiku(response) Out[12]: <pre>True</pre> <p>For the example given above, if we count the syllables in each line, it is indeed a Haiku, and our function confirms this. Here is a function that will take a list of themes for the Haiku, and generate a Haiku for each theme:</p> In\u00a0[13]: Copied! <pre>def haiku_check(themes: list[str]) -&gt; list[tuple[str, bool]]:\n\n    responses = []\n\n    for theme in themes:\n        prompt = load_template(\n            \"prompts/poetry_prompt.jinja\",\n            {\n                \"length\": \"short\",\n                \"style\": \"haiku\",\n                \"theme\": theme\n            }\n        )\n\n        response = chat_response(prompt)\n        responses.append((response, is_haiku(response)))\n\n    return responses\n</pre> def haiku_check(themes: list[str]) -&gt; list[tuple[str, bool]]:      responses = []      for theme in themes:         prompt = load_template(             \"prompts/poetry_prompt.jinja\",             {                 \"length\": \"short\",                 \"style\": \"haiku\",                 \"theme\": theme             }         )          response = chat_response(prompt)         responses.append((response, is_haiku(response)))      return responses  In\u00a0[14]: Copied! <pre>for response, check in haiku_check(themes = [\"a Samurai dog\", \"a Kung Fu panda\", \"a Ninja squirrel\", \"a Pirate monkey\"]):\n    print(response)\n    print(check)\n    print(\"-\"*10)\n</pre> for response, check in haiku_check(themes = [\"a Samurai dog\", \"a Kung Fu panda\", \"a Ninja squirrel\", \"a Pirate monkey\"]):     print(response)     print(check)     print(\"-\"*10) <pre>Fur like fallen leaves,  \nIn armor of dreams he stands,  \nHonor in each paw.  \nTrue\n----------\nPanda leaps with grace,  \nDreams of warriors awake,  \nStrength in every face.  \n[5, 7, 6]\n----------\nSilent in the trees,  \nNinja squirrel leaps with grace,  \nShadow in the breeze.  \nTrue\n----------\nPirate monkey swings,  \nA treasure chest of bananas,  \nOn the high seas sings.\n[5, 8, 5]\n----------\n</pre> <p>Again, some of these may not be correct, and that is because counting syllables is not easy.</p>"},{"location":"prompting/4_prompt_templates/#prompt-templates","title":"Prompt Templates\u00b6","text":""},{"location":"prompting/4_prompt_templates/#f-strings","title":"F-strings\u00b6","text":""},{"location":"prompting/4_prompt_templates/#jinja2","title":"Jinja2\u00b6","text":""},{"location":"prompting/4_prompt_templates/#haiku-checker","title":"Haiku Checker\u00b6","text":""},{"location":"prompting/4_prompt_templates/#caution","title":"\ud83d\udd34 Caution\u2757\u00b6","text":"<p>At this point, it is worth pointing out the challenge of metrics. Evaluation is obviously important - how do you know if your model is performing as intended? But working with LLMs is not not like working with traditional ML models, which have well established metrics. You will often have to find your own metrics, or create them bespoke to your use case. This is an active area of research in LLM evaluation, and there is no one size fits all solution.</p> <p>We advocate for evaluation driven LLM development - think about your metrics early and often, and build your systems with this in mind.</p>"},{"location":"prompting/prompting_exercise/","title":"Generating fake Nature abstracts","text":"In\u00a0[2]: Copied! <pre>from openai import OpenAI\nimport dotenv\nimport os\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape\nfrom typing import Any\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\n# supress every warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> from openai import OpenAI import dotenv import os  import numpy as np from sklearn.decomposition import PCA from sklearn.metrics.pairwise import cosine_similarity import matplotlib.pyplot as plt import seaborn as sns  from jinja2 import Environment, FileSystemLoader, select_autoescape from typing import Any  from transformers import AutoModel, AutoTokenizer import torch  # supress every warning import warnings warnings.filterwarnings(\"ignore\") In\u00a0[3]: Copied! <pre>client = OpenAI()\n</pre> client = OpenAI() In\u00a0[4]: Copied! <pre>with open('abstracts/real.txt', \"r\") as f:\n    real_abstracts = f.read().split(\"\\n\\n\")\n\nprint(real_abstracts[0])\n</pre> with open('abstracts/real.txt', \"r\") as f:     real_abstracts = f.read().split(\"\\n\\n\")  print(real_abstracts[0]) <pre>A goal of neuroscience is to obtain a causal model of the nervous system. The recently reported whole-brain fly connectome specifies the synaptic paths by which neurons can affect each other, but not how strongly they do affect each other in vivo. To overcome this limitation, we introduce a combined experimental and statistical strategy for efficiently learning a causal model of the fly brain, which we refer to as the \u2018effectome\u2019. Specifically, we propose an estimator for a linear dynamical model of the fly brain that uses stochastic optogenetic perturbation data to estimate causal effects and the connectome as a prior to greatly improve estimation efficiency. We validate our estimator in connectome-based linear simulations and show that it recovers a linear approximation to the nonlinear dynamics of more biophysically realistic simulations. We then analyse the connectome to propose circuits that dominate the dynamics of the fly nervous system. We discover that the dominant circuits involve only relatively small populations of neurons\u2014thus, neuron-level imaging, stimulation and identification are feasible. This approach also re-discovers known circuits and generates testable hypotheses about their dynamics. Overall, we provide evidence that fly whole-brain dynamics are generated by a large collection of small circuits that operate largely independently of each other. This implies that a causal model of a brain can be feasibly obtained in the fly.\n</pre> <p>Now we need to figure out a way to generate fake abstracts. We create both a system and a user prompt.</p> <p>System prompt The system prompt should direct the model to produce a fake abstract on a topic given by the user.</p> <p>User prompt The user prompt should just be a topic.</p> <p>We will use Jinja to create the prompts. Create a new directory and add the two prompts.</p> In\u00a0[5]: Copied! <pre>def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    env = Environment(\n        loader=FileSystemLoader(searchpath='./'),\n        autoescape=select_autoescape()\n    )\n    template = env.get_template(template_filepath)\n    return template.render(**arguments)\n</pre> def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     env = Environment(         loader=FileSystemLoader(searchpath='./'),         autoescape=select_autoescape()     )     template = env.get_template(template_filepath)     return template.render(**arguments) In\u00a0[6]: Copied! <pre>generation_system_prompt = load_template(\"prompts/system.jinja\",{})\ngeneration_user_prompt = load_template(\n    \"prompts/user.jinja\",\n    {\n        \"topic\": \"Panda foraging in subsaharan Africa, and impact on the local polar bear population\",\n    }\n)\n\nprint(generation_system_prompt)\nprint(generation_user_prompt)\n</pre> generation_system_prompt = load_template(\"prompts/system.jinja\",{}) generation_user_prompt = load_template(     \"prompts/user.jinja\",     {         \"topic\": \"Panda foraging in subsaharan Africa, and impact on the local polar bear population\",     } )  print(generation_system_prompt) print(generation_user_prompt) <pre>You will be given a topic from the user. You are to write an original Nature article abstract about the topic.\nDo not write a title, authors, or any other article text. Write only the abstract.\nThe abstract should be in English.\nThe abstract should be a continuous paragraph between 150 and 250 words.\nIt is important that the abstract is scientifically feasible.\n\nHere is a new topic:\n\nPanda foraging in subsaharan Africa, and impact on the local polar bear population\n</pre> In\u00a0[7]: Copied! <pre>def chat_response(system_prompt, user_prompt, model, temperature) -&gt; str:\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature,\n        max_tokens=400\n    ).choices[0].message.content\n\n    return response\n</pre> def chat_response(system_prompt, user_prompt, model, temperature) -&gt; str:     client = OpenAI()      response = client.chat.completions.create(         model=model,         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": user_prompt}         ],         temperature=temperature,         max_tokens=400     ).choices[0].message.content      return response In\u00a0[8]: Copied! <pre>fake_abstract = chat_response(generation_system_prompt, generation_user_prompt, \"gpt-4o-mini\", 0.2)\n</pre> fake_abstract = chat_response(generation_system_prompt, generation_user_prompt, \"gpt-4o-mini\", 0.2) In\u00a0[9]: Copied! <pre>print(fake_abstract)\n</pre> print(fake_abstract) <pre>The introduction of giant pandas (Ailuropoda melanoleuca) into sub-Saharan Africa presents a unique ecological scenario, particularly concerning its potential impact on local polar bear (Ursus maritimus) populations. This study investigates the foraging behavior of pandas in a novel environment characterized by diverse flora and fauna, contrasting their traditional bamboo diet with available local vegetation. Preliminary observations indicate that pandas exhibit dietary flexibility, incorporating native plant species into their foraging repertoire. This adaptability raises questions about interspecies competition, particularly with apex predators like polar bears, which are not native to this region but are included in this study due to their ecological significance. The research employs a combination of field studies and ecological modeling to assess the overlap in habitat use and resource competition between the two species. Early findings suggest that while pandas primarily forage on vegetation, their presence may inadvertently influence polar bear hunting patterns and prey availability, particularly in overlapping habitats. This study underscores the importance of understanding species interactions in altered ecosystems, highlighting the need for conservation strategies that consider the implications of introducing non-native species and their potential effects on established wildlife populations. Further research is essential to elucidate the long-term ecological consequences of this unprecedented scenario.\n</pre> <p>We now need to produce 5 new abstracts. It would be very useful if we could actually produce articles on similar topics to those in our real abstracts. We will therefore use <code>gpt-4o</code> to try and extract single sentence from each abstract.</p> In\u00a0[10]: Copied! <pre>def extract_topic(abstract):\n    extraction_system_prompt = load_template(\"prompts/extraction_system.jinja\",{})\n    extraction_user_prompt = load_template(\n        \"prompts/extraction_user.jinja\",\n        {\n            \"abstract\": abstract,\n        }\n    )\n\n    response = chat_response(extraction_system_prompt, extraction_user_prompt, \"gpt-4o-mini\", 0.2)\n\n    return response\n</pre> def extract_topic(abstract):     extraction_system_prompt = load_template(\"prompts/extraction_system.jinja\",{})     extraction_user_prompt = load_template(         \"prompts/extraction_user.jinja\",         {             \"abstract\": abstract,         }     )      response = chat_response(extraction_system_prompt, extraction_user_prompt, \"gpt-4o-mini\", 0.2)      return response In\u00a0[11]: Copied! <pre>topic_sentence = extract_topic(fake_abstract)\ntopic_sentence\n</pre> topic_sentence = extract_topic(fake_abstract) topic_sentence Out[11]: <pre>'Giant pandas in sub-Saharan Africa may impact local polar bear populations and ecosystems.'</pre> <p>Great. Now we can do this for all of our real abstracts:</p> In\u00a0[12]: Copied! <pre>topic_sentences = [extract_topic(abstract) for abstract in real_abstracts]\ntopic_sentences\n</pre> topic_sentences = [extract_topic(abstract) for abstract in real_abstracts] topic_sentences Out[12]: <pre>['Causal modeling of the fly brain using connectome and perturbation data.',\n 'Long-term health impacts of tropical cyclones on mortality in the contiguous United States.',\n 'Investigating haematopoiesis in Down syndrome through multi-omic analysis of fetal samples.',\n 'Innovative H2-based method transforms traditional alloy-making into a sustainable single-step process.',\n 'Three-dimensional wave breaking significantly alters steepness and air-sea exchange dynamics.']</pre> <p>Comparing these with the original abstracts seems OK!</p> <p>Now we can feed these topic sentences into our first <code>gpt-4o-mini</code> agent and see how it does.</p> In\u00a0[13]: Copied! <pre>def generate_abstract(topic):\n    generation_system_prompt = load_template(\"prompts/system.jinja\",{})\n    generation_user_prompt = load_template(\n        \"prompts/user.jinja\",\n        {\n            \"topic\": topic,\n        }\n    )\n\n    fake_abstract = chat_response(generation_system_prompt, generation_user_prompt, \"gpt-4o-mini\", 0.2)\n\n    return fake_abstract\n</pre> def generate_abstract(topic):     generation_system_prompt = load_template(\"prompts/system.jinja\",{})     generation_user_prompt = load_template(         \"prompts/user.jinja\",         {             \"topic\": topic,         }     )      fake_abstract = chat_response(generation_system_prompt, generation_user_prompt, \"gpt-4o-mini\", 0.2)      return fake_abstract In\u00a0[14]: Copied! <pre>generated_abstracts = [generate_abstract(topic) for topic in topic_sentences]\n</pre> generated_abstracts = [generate_abstract(topic) for topic in topic_sentences] In\u00a0[14]: Copied! <pre>print(topic_sentences[0])\nprint(\"-\"*10)\nprint(generated_abstracts[0])\nprint(\"-\"*10)\nprint(real_abstracts[0])\n</pre> print(topic_sentences[0]) print(\"-\"*10) print(generated_abstracts[0]) print(\"-\"*10) print(real_abstracts[0]) <pre>Causal modeling of the fly brain using connectome and experimental data.\n----------\nThe intricate neural circuitry of the Drosophila melanogaster brain presents a unique opportunity to explore causal relationships between neural connectivity and behavior. In this study, we integrate high-resolution connectome data with experimental observations to construct a comprehensive causal model of the fly brain. Utilizing advanced computational techniques, we analyze synaptic connections derived from electron microscopy alongside behavioral assays that assess responses to sensory stimuli. Our model incorporates both structural and functional data, allowing us to identify key neural pathways that mediate specific behaviors, such as olfactory processing and motor coordination. By applying causal inference methods, we elucidate the influence of particular neural circuits on behavioral outcomes, revealing how alterations in connectivity can lead to changes in fly behavior. Furthermore, we validate our model through targeted manipulations of identified neural populations, demonstrating the predictive power of our approach. This research not only enhances our understanding of the fly brain's functional architecture but also provides a framework for investigating causal relationships in other complex neural systems. Ultimately, our findings contribute to the broader field of neurobiology by bridging the gap between structural connectomics and functional neuroscience, paving the way for future studies on the causal mechanisms underlying behavior in both invertebrates and vertebrates.\n----------\nA goal of neuroscience is to obtain a causal model of the nervous system. The recently reported whole-brain fly connectome specifies the synaptic paths by which neurons can affect each other, but not how strongly they do affect each other in vivo. To overcome this limitation, we introduce a combined experimental and statistical strategy for efficiently learning a causal model of the fly brain, which we refer to as the \u2018effectome\u2019. Specifically, we propose an estimator for a linear dynamical model of the fly brain that uses stochastic optogenetic perturbation data to estimate causal effects and the connectome as a prior to greatly improve estimation efficiency. We validate our estimator in connectome-based linear simulations and show that it recovers a linear approximation to the nonlinear dynamics of more biophysically realistic simulations. We then analyse the connectome to propose circuits that dominate the dynamics of the fly nervous system. We discover that the dominant circuits involve only relatively small populations of neurons\u2014thus, neuron-level imaging, stimulation and identification are feasible. This approach also re-discovers known circuits and generates testable hypotheses about their dynamics. Overall, we provide evidence that fly whole-brain dynamics are generated by a large collection of small circuits that operate largely independently of each other. This implies that a causal model of a brain can be feasibly obtained in the fly.\n</pre> <p>But how can we really compare these? We generate embeddings of the abstracts.</p> In\u00a0[34]: Copied! <pre>def get_embedding(texts : list, method = \"openai\"):\n    match method:\n        case \"openai\":\n            embeddings = client.embeddings.create(\n                model=\"text-embedding-3-small\",\n                input=texts\n            )\n            embeddings = [embedding.embedding for embedding in embeddings.data]\n\n            return np.array(embeddings)\n        \n        case \"bert\":\n            model_checkpoint = \"distilbert/distilbert-base-uncased\"\n\n            model = AutoModel.from_pretrained(model_checkpoint)\n            tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n            tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n            with torch.no_grad():\n                embeddings = model(**tokens).last_hidden_state[:, 0, :].detach().numpy()\n\n            return embeddings\n        \n        case _:\n            raise ValueError(\"Invalid method\")\n</pre> def get_embedding(texts : list, method = \"openai\"):     match method:         case \"openai\":             embeddings = client.embeddings.create(                 model=\"text-embedding-3-small\",                 input=texts             )             embeddings = [embedding.embedding for embedding in embeddings.data]              return np.array(embeddings)                  case \"bert\":             model_checkpoint = \"distilbert/distilbert-base-uncased\"              model = AutoModel.from_pretrained(model_checkpoint)             tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)              tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")             with torch.no_grad():                 embeddings = model(**tokens).last_hidden_state[:, 0, :].detach().numpy()              return embeddings                  case _:             raise ValueError(\"Invalid method\") In\u00a0[35]: Copied! <pre>real_embeddings = get_embedding(real_abstracts)\ngenerated_embeddings = get_embedding(generated_abstracts)\n</pre> real_embeddings = get_embedding(real_abstracts) generated_embeddings = get_embedding(generated_abstracts) In\u00a0[38]: Copied! <pre>similarity = cosine_similarity(real_embeddings, generated_embeddings)\n</pre> similarity = cosine_similarity(real_embeddings, generated_embeddings) In\u00a0[39]: Copied! <pre>all_embeddings = np.concatenate([real_embeddings, generated_embeddings])\n\npca = PCA(n_components=2)\npca.fit(all_embeddings)\nreal_pca = pca.transform(real_embeddings)\nfake_pca = pca.transform(generated_embeddings)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\nfig.tight_layout()\n\n# heatmap with names axes and 2sf labels\nsns.heatmap(similarity, annot=True, fmt='.2f', xticklabels=False, yticklabels=False, square=False, ax=ax1)\nax1.set_xlabel(\"Generated Abstracts\")\nax1.set_ylabel(\"Real Abstracts\")\n\nax2.scatter(real_pca[:,0], real_pca[:,1], label=\"Real\")\nax2.scatter(fake_pca[:,0], fake_pca[:,1], label=\"Fake\")\nax2.set_xlabel(f\"PCA 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\nax2.set_ylabel(f\"PCA 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\nax2.legend()\n\nplt.show()\n</pre> all_embeddings = np.concatenate([real_embeddings, generated_embeddings])  pca = PCA(n_components=2) pca.fit(all_embeddings) real_pca = pca.transform(real_embeddings) fake_pca = pca.transform(generated_embeddings)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) fig.tight_layout()  # heatmap with names axes and 2sf labels sns.heatmap(similarity, annot=True, fmt='.2f', xticklabels=False, yticklabels=False, square=False, ax=ax1) ax1.set_xlabel(\"Generated Abstracts\") ax1.set_ylabel(\"Real Abstracts\")  ax2.scatter(real_pca[:,0], real_pca[:,1], label=\"Real\") ax2.scatter(fake_pca[:,0], fake_pca[:,1], label=\"Fake\") ax2.set_xlabel(f\"PCA 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\") ax2.set_ylabel(f\"PCA 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\") ax2.legend()  plt.show() <p>From this is seems like most of the variance is down to the difference in topic. What if we get 5 abstracts of the same topic? In the file <code>abstracts/real_same.txt</code> are 5 articles on the fruit fly connectome. First grab the topic sentences and then generate the samples as before.</p> In\u00a0[40]: Copied! <pre>with open('abstracts/real_same.txt', \"r\") as f:\n    real_same_abstracts = f.read().split(\"\\n\\n\")\n</pre> with open('abstracts/real_same.txt', \"r\") as f:     real_same_abstracts = f.read().split(\"\\n\\n\") In\u00a0[41]: Copied! <pre>same_topic_sentences = [extract_topic(abstract) for abstract in real_same_abstracts]\nsame_topic_sentences\n</pre> same_topic_sentences = [extract_topic(abstract) for abstract in real_same_abstracts] same_topic_sentences Out[41]: <pre>['Developing a causal model of the fly brain using experimental and statistical methods.',\n 'Analysis of the adult fly brain connectome reveals insights into neural network organization.',\n 'Modeling the Drosophila brain reveals insights into sensory processing and behavior.',\n \"Comprehensive annotation of neuronal classes in Drosophila melanogaster's brain connectome.\",\n \"Comprehensive analysis of neuronal cell types and connectivity in Drosophila's optic lobe.\",\n 'Whole brain neuronal wiring diagram reveals insights into connectivity and circuit mechanisms.',\n 'Advancements in connectomics enable predictions about neuronal function from structural wiring diagrams.',\n 'Neural mechanisms of halting during walking in Drosophila involve distinct inhibitory and excitatory pathways.',\n \"Visual information processing in Drosophila's navigation system and its neural architecture.\"]</pre> In\u00a0[42]: Copied! <pre>same_generated_abstracts = [generate_abstract(topic) for topic in same_topic_sentences]\n</pre> same_generated_abstracts = [generate_abstract(topic) for topic in same_topic_sentences] In\u00a0[43]: Copied! <pre>same_real_embeddings = get_embedding(real_same_abstracts)\nsame_generated_embeddings = get_embedding(same_generated_abstracts)\n</pre> same_real_embeddings = get_embedding(real_same_abstracts) same_generated_embeddings = get_embedding(same_generated_abstracts) In\u00a0[44]: Copied! <pre>similarity = cosine_similarity(same_real_embeddings, same_generated_embeddings)\n\nall_embeddings = np.concatenate([same_real_embeddings, same_generated_embeddings])\n\npca = PCA(n_components=2)\npca.fit(all_embeddings)\nreal_pca = pca.transform(same_real_embeddings)\nfake_pca = pca.transform(same_generated_embeddings)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\nfig.tight_layout()\n\n# heatmap with names axes and 2sf labels\nsns.heatmap(similarity, annot=True, fmt='.2f', xticklabels=False, yticklabels=False, square=False, ax=ax1)\nax1.set_xlabel(\"Generated Abstracts\")\nax1.set_ylabel(\"Real Abstracts\")\n\nax2.scatter(real_pca[:,0], real_pca[:,1], label=\"Real\")\nax2.scatter(fake_pca[:,0], fake_pca[:,1], label=\"Fake\")\nax2.set_xlabel(f\"PCA 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\nax2.set_ylabel(f\"PCA 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\nax2.legend()\n\nplt.show()\n</pre> similarity = cosine_similarity(same_real_embeddings, same_generated_embeddings)  all_embeddings = np.concatenate([same_real_embeddings, same_generated_embeddings])  pca = PCA(n_components=2) pca.fit(all_embeddings) real_pca = pca.transform(same_real_embeddings) fake_pca = pca.transform(same_generated_embeddings)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) fig.tight_layout()  # heatmap with names axes and 2sf labels sns.heatmap(similarity, annot=True, fmt='.2f', xticklabels=False, yticklabels=False, square=False, ax=ax1) ax1.set_xlabel(\"Generated Abstracts\") ax1.set_ylabel(\"Real Abstracts\")  ax2.scatter(real_pca[:,0], real_pca[:,1], label=\"Real\") ax2.scatter(fake_pca[:,0], fake_pca[:,1], label=\"Fake\") ax2.set_xlabel(f\"PCA 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\") ax2.set_ylabel(f\"PCA 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\") ax2.legend()  plt.show() <p>Just by looking at the similarity scores of the embeddings and the first two pricipal components, it is quite difficult to spot the difference between the two different styles.</p> <p>Now contrast this with the haiku generated by Claude, and the real haiku.</p> In\u00a0[45]: Copied! <pre>with open(\"haiku/real_haiku_basho.txt\", \"r\") as f:\n    real_haikus_basho = f.read().split(\"\\n\\n\")\n\nwith open(\"haiku/real_haiku_buson.txt\", \"r\") as f:\n    real_haikus_buson = f.read().split(\"\\n\\n\")\n\nwith open(\"haiku/real_haiku_issa.txt\", \"r\") as f:\n    real_haikus_issa = f.read().split(\"\\n\\n\")\n\nwith open(\"haiku/gpt_haiku.txt\", \"r\") as f:\n    fake_haikus = f.read().split(\"\\n\\n\")\n\nprint(real_haikus_basho[0])\nprint()\nprint(fake_haikus[0])\n\nreal_haikus = real_haikus_basho + real_haikus_buson + real_haikus_issa\n\nall_haikus = real_haikus + fake_haikus\n# 1 for real, 0 for fake\ntargets = [1] * len(real_haikus) + [0] * len(fake_haikus)\n\n# shuffle\n# import random\n# zipped = list(zip(all_haikus, targets))\n# random.shuffle(zipped)\n</pre> with open(\"haiku/real_haiku_basho.txt\", \"r\") as f:     real_haikus_basho = f.read().split(\"\\n\\n\")  with open(\"haiku/real_haiku_buson.txt\", \"r\") as f:     real_haikus_buson = f.read().split(\"\\n\\n\")  with open(\"haiku/real_haiku_issa.txt\", \"r\") as f:     real_haikus_issa = f.read().split(\"\\n\\n\")  with open(\"haiku/gpt_haiku.txt\", \"r\") as f:     fake_haikus = f.read().split(\"\\n\\n\")  print(real_haikus_basho[0]) print() print(fake_haikus[0])  real_haikus = real_haikus_basho + real_haikus_buson + real_haikus_issa  all_haikus = real_haikus + fake_haikus # 1 for real, 0 for fake targets = [1] * len(real_haikus) + [0] * len(fake_haikus)  # shuffle # import random # zipped = list(zip(all_haikus, targets)) # random.shuffle(zipped) <pre>The old pond\na frog jumps in\nsound of water\n\nDawn's first light\na spider weaves anew\nits dew-kissed web\n</pre> In\u00a0[46]: Copied! <pre>fake_haiku_embeddings = get_embedding(fake_haikus)\nreal_haiku_embeddings = get_embedding(real_haikus)\n</pre> fake_haiku_embeddings = get_embedding(fake_haikus) real_haiku_embeddings = get_embedding(real_haikus) In\u00a0[47]: Copied! <pre>all_embeddings = np.concatenate([real_haiku_embeddings, fake_haiku_embeddings])\n</pre> all_embeddings = np.concatenate([real_haiku_embeddings, fake_haiku_embeddings]) In\u00a0[48]: Copied! <pre>pca = PCA(n_components=2)\npca_embeddings = pca.fit_transform(all_embeddings)\n\nplt.figure(figsize=(8, 6))\n# plot each poet\nplt.scatter(pca_embeddings[:len(fake_haikus), 0], pca_embeddings[:len(fake_haikus), 1], label=\"Fake Haiku\")\n# basho\nplt.scatter(\n    pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 0], \n    pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 1],\n    label=\"Real Haiku (Basho)\"\n)\n# buson\nplt.scatter(\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 0],\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 1],\n    label=\"Real Haiku (Buson)\"\n)\n# issa\nplt.scatter(\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 0],\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 1],\n    label=\"Real Haiku (Issa)\"\n)\n\nplt.xlabel(f\"PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\nplt.ylabel(f\"PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\nplt.legend()\nplt.show()\n</pre> pca = PCA(n_components=2) pca_embeddings = pca.fit_transform(all_embeddings)  plt.figure(figsize=(8, 6)) # plot each poet plt.scatter(pca_embeddings[:len(fake_haikus), 0], pca_embeddings[:len(fake_haikus), 1], label=\"Fake Haiku\") # basho plt.scatter(     pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 0],      pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 1],     label=\"Real Haiku (Basho)\" ) # buson plt.scatter(     pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 0],     pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 1],     label=\"Real Haiku (Buson)\" ) # issa plt.scatter(     pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 0],     pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 1],     label=\"Real Haiku (Issa)\" )  plt.xlabel(f\"PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\") plt.ylabel(f\"PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\") plt.legend() plt.show() In\u00a0[49]: Copied! <pre>fake_embeddings = get_embedding(fake_haikus, method=\"bert\")\nreal_embeddings = get_embedding(real_haikus, method=\"bert\")\n</pre> fake_embeddings = get_embedding(fake_haikus, method=\"bert\") real_embeddings = get_embedding(real_haikus, method=\"bert\") In\u00a0[51]: Copied! <pre>embeddings = np.concatenate([fake_embeddings, real_embeddings])\nlabels = np.array([0]*len(fake_haikus) + [1]*len(real_haikus))\n</pre> embeddings = np.concatenate([fake_embeddings, real_embeddings]) labels = np.array([0]*len(fake_haikus) + [1]*len(real_haikus)) In\u00a0[52]: Copied! <pre>pca = PCA(n_components=2)\npca_embeddings = pca.fit_transform(embeddings)\n\nplt.figure(figsize=(8, 6))\n# plot each poet\nplt.scatter(pca_embeddings[:len(fake_haikus), 0], pca_embeddings[:len(fake_haikus), 1], label=\"Fake Haiku\")\n# basho\nplt.scatter(\n    pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 0], \n    pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 1],\n    label=\"Real Haiku (Basho)\"\n)\n# buson\nplt.scatter(\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 0],\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 1],\n    label=\"Real Haiku (Buson)\"\n)\n# issa\nplt.scatter(\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 0],\n    pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 1],\n    label=\"Real Haiku (Issa)\"\n)\n\nplt.xlabel(f\"PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\nplt.ylabel(f\"PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\nplt.legend()\nplt.show()\n</pre> pca = PCA(n_components=2) pca_embeddings = pca.fit_transform(embeddings)  plt.figure(figsize=(8, 6)) # plot each poet plt.scatter(pca_embeddings[:len(fake_haikus), 0], pca_embeddings[:len(fake_haikus), 1], label=\"Fake Haiku\") # basho plt.scatter(     pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 0],      pca_embeddings[len(fake_haikus):len(fake_haikus)+len(real_haikus_basho), 1],     label=\"Real Haiku (Basho)\" ) # buson plt.scatter(     pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 0],     pca_embeddings[len(fake_haikus)+len(real_haikus_basho):len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson), 1],     label=\"Real Haiku (Buson)\" ) # issa plt.scatter(     pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 0],     pca_embeddings[len(fake_haikus)+len(real_haikus_basho)+len(real_haikus_buson):, 1],     label=\"Real Haiku (Issa)\" )  plt.xlabel(f\"PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\") plt.ylabel(f\"PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\") plt.legend() plt.show() <p>That actually looks a little better.</p> In\u00a0[53]: Copied! <pre>with open('abstracts/real_same.txt', \"r\") as f:\n    real_same_abstracts = f.read().split(\"\\n\\n\")\n</pre> with open('abstracts/real_same.txt', \"r\") as f:     real_same_abstracts = f.read().split(\"\\n\\n\") In\u00a0[54]: Copied! <pre>same_fake_embeddings = get_embedding(same_generated_abstracts, method=\"bert\")\nsame_real_embeddings = get_embedding(real_same_abstracts, method=\"bert\")\n</pre> same_fake_embeddings = get_embedding(same_generated_abstracts, method=\"bert\") same_real_embeddings = get_embedding(real_same_abstracts, method=\"bert\") In\u00a0[55]: Copied! <pre>similarity = cosine_similarity(same_real_embeddings, same_fake_embeddings)\n\n\nall_embeddings = np.concatenate([same_real_embeddings, same_fake_embeddings])\n\npca = PCA(n_components=2)\npca.fit(all_embeddings)\nreal_pca = pca.transform(same_real_embeddings)\nfake_pca = pca.transform(same_fake_embeddings)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\nfig.tight_layout()\n\n# heatmap with names axes and 2sf labels\nsns.heatmap(similarity, annot=True, fmt='.2f', xticklabels=False, yticklabels=False, square=False, ax=ax1)\nax1.set_xlabel(\"Generated Abstracts\")\nax1.set_ylabel(\"Real Abstracts\")\n\nax2.scatter(real_pca[:,0], real_pca[:,1], label=\"Real\")\nax2.scatter(fake_pca[:,0], fake_pca[:,1], label=\"Fake\")\nax2.set_xlabel(f\"PCA 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\")\nax2.set_ylabel(f\"PCA 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\")\nax2.legend()\n\nplt.show()\n</pre> similarity = cosine_similarity(same_real_embeddings, same_fake_embeddings)   all_embeddings = np.concatenate([same_real_embeddings, same_fake_embeddings])  pca = PCA(n_components=2) pca.fit(all_embeddings) real_pca = pca.transform(same_real_embeddings) fake_pca = pca.transform(same_fake_embeddings)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) fig.tight_layout()  # heatmap with names axes and 2sf labels sns.heatmap(similarity, annot=True, fmt='.2f', xticklabels=False, yticklabels=False, square=False, ax=ax1) ax1.set_xlabel(\"Generated Abstracts\") ax1.set_ylabel(\"Real Abstracts\")  ax2.scatter(real_pca[:,0], real_pca[:,1], label=\"Real\") ax2.scatter(fake_pca[:,0], fake_pca[:,1], label=\"Fake\") ax2.set_xlabel(f\"PCA 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)\") ax2.set_ylabel(f\"PCA 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)\") ax2.legend()  plt.show() <p>Well, well, well...</p> <p>BERT to the rescue once again. Even though their cosine similarities are all remarkably similar, as we would expect, since they are all papers around the same topic, PCA on those embeddings still yields some valuable results.</p> In\u00a0[6]: Copied! <pre>from PIL import Image\nimport matplotlib.pyplot as plt\n\nimg = Image.open(\"bert-alone.png\")\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n</pre> from PIL import Image import matplotlib.pyplot as plt  img = Image.open(\"bert-alone.png\") plt.imshow(img) plt.axis('off') plt.show()"},{"location":"prompting/prompting_exercise/#generating-fake-nature-abstracts","title":"Generating fake Nature abstracts\u00b6","text":"<p>In this practical, the goal is to produce fake Nature articles. The inspiration comes from the work done by Alessandro Trevisan and his students in detecting \"bullshit\" in academic writing. No doubt a difficult task, and a never-ending stuggle.</p> <p>In the <code>abstracts/real.txt</code> file, there are 5 real abstracts taken from Nature articles. The first goal is to have <code>gpt-4o-mini</code> produce some fake abstracts. The next stage is to see if <code>gpt-4o</code> can tell the difference. We will then see if we can tell the difference, and whether there is anything we can do to bring the generated abstracts closer to the real abstracts.</p> <p>You will see some code blocks with something like:</p> <pre>def generate():\n    response = client.\\#---\\#.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            \\#---\\#\n        ],\n        max_tokens=128,\n        \\#---\\#\n    )\n</pre> <p>Wherever you see <code>\\#---\\#</code>, that is where you fill in the missing code.</p>"},{"location":"prompting/prompting_exercise/#generating-fake-abstracts","title":"Generating fake abstracts\u00b6","text":"<p>First we load the real abstracts to take a look.</p>"},{"location":"prompting/prompting_exercise/#using-bert-as-the-embedding-model","title":"Using BERT as the embedding model\u00b6","text":"<p>We've tried using the OpenAI embedding models. But now let's try our old friend, BERT.</p>"},{"location":"prompting/prompting_exercise/#with-the-abstracts","title":"With the abstracts\u00b6","text":""},{"location":"setting-up/azure/","title":"Azure AI Studio","text":"<p>There are currently 3 major model as a service (MaaS) providers (there are probably more, but these are currently the main ones):</p> <ul> <li>Amazon AWS Bedrock</li> <li>Google GCP Vertex AI</li> <li>Microsoft Azure AI Studio</li> </ul> <p>Everyone will have a different opinion on which one is best, and which offers the best services. But essentially: they are all the same! They all have a learning curve; they all have roughly similar pricing structures; they all offer similar online courses; they are all annoying in some way.</p> <p>Here, we focus on Microsoft Azure AI Studio. The main advantages of Azure are:</p> <ul> <li>The easy use of the OpenAI API rather than a custom one (as is the case for AWS and GCP);</li> <li>The ease of deploying serverless endpoints;</li> <li>The range of models (including OpenAI models).</li> </ul> <p>The rabbithole for cloud services is dark (and potentially full of deranged) rabbits, so we will not go into significant details, instead focusing on getting up and running with AI Studio, and deploying an endpoint. If you want to learn more about Azure services in general, they have an accessible introductory course.</p>"},{"location":"setting-up/azure/#1-sign-up","title":"1. Sign up","text":"<p>You will need to sign up for an Azure account and you will need to register a payment method. Most cloud providers will give you a free trial for a limited amount of time and/or credits.</p>"},{"location":"setting-up/azure/#2-login-to-ai-studio","title":"2. Login to AI Studio","text":"<p>Once you've signed up to Azure, you can login to AI Studio, and browse the model catalogue, which should look something like this:</p> <p></p>"},{"location":"setting-up/azure/#3-deploy-as-a-serverless-api","title":"3. Deploy as a serverless API","text":"<p>When you deploy a model to an endpoint, you will be able to query it as you would an OpenAI model. Select a model (in the example below it is <code>Llama-3.1-8B-Instruct</code>), and hit deploy.</p> <p></p> <p>You will have 2 deployment options:</p> <p></p> <p>For now, hit the serverless API option. If this is your first time, you will have to create a new project and a new hub. You will then be given the option of adding or creating a new subscription, resource, group, and AI Service. This is an Azure-backed service, so storage and a key vault will also be created for you.</p> <p>Once everything is set up, you can \"Subscribe and Deploy\". You will now be given a Deployment name, and the option to disable content filtering (content filtering is a billed service). AI studio will now create an endpoint that you can send requests to using the OpenAI API.</p>"},{"location":"setting-up/azure/#4-consume","title":"4. Consume","text":"<p>To consume your services, you will need to add two things to you <code>.env</code> file:</p> <pre><code>AZURE_API_KEY=\"your-api-key\"\nAZURE_URL=\"https://&lt;name-of-model&gt;.&lt;region&gt;.models.ai.azure.com/v1/\"\n</code></pre> <p>The Url can be found on the Deployments page. You then initialize the OpenAI API client:</p> <pre><code>dotenv.load_dotenv()\n\nclient = OpenAI(\n    base_url=os.getenv(\"AZURE_URL\"),\n    api_key=os.getenv(\"AZURE_API_KEY\")\n)\n\ncompletion = client.chat.completions.create(\n    model=\"azureai\",\n    messages=messages\n)\n</code></pre> <p>And that's it!</p>"},{"location":"setting-up/azure/#5-delete","title":"5. Delete","text":"<p>To delete your endpoint, you need to hit \"Delete\" in the deployment page in AI Studio, and you may want to delete other resources allocated in the Azure Dashboard as well.</p>"},{"location":"setting-up/codespaces/","title":"Codespaces","text":"<p>For this workshop, since we're not really doing any heavy lifting, we can work entirely within Github Codespaces. If you would rather run the notebooks locally, you can follow the instructions on Running locally.</p> <p>First create a new repository on Github, and then create a codespace from that repository. </p> <p>Navigate to the LLM workshop repo (click the GitHub symbol in the top right of this page). Switch to the <code>Handson</code> branch, and download the content as a zip file. Upload this to your new repo. Open the repo as a Codespace. There is no need to create a virtual environment, since you're already in a containerized environment anyway with a version of Python 3.12.</p> <p>You don\u2019t need to create or manage a virtual environment yourself. The devcontainer will automatically install <code>uv</code>, create and activate a <code>.venv</code>, and install all required dependencies when your Codespace is built.</p> <p>You will also need to run the following in the terminal in order to use <code>chromadb</code>:</p> <pre><code>mkdir mkdir -p /home/codespace/.local/lib/python3.12/site-packages/google/colab\n</code></pre> <p>Add a new file in the root directory called <code>.gitignore</code>. Add the following to the <code>.gitignore</code> file:</p> <pre><code>.env\n</code></pre> <p>and save it.</p> <p>You should then create a new file called <code>.env</code> and add your OpenAI API key to it.</p> <pre><code>OPENAI_API_KEY=&lt;YOUR_API_KEY&gt;\n</code></pre> <p>Warning</p> <p>Please add <code>.env</code> to your <code>.gitignore</code> before adding the API key to the <code>.env</code> file and before pushing any changes to your repo! There are bots crawling GitHub for exposed API keys.</p>"},{"location":"setting-up/nvidia/","title":"Nvidia NIMS","text":"<p>For rapid prototpying, NVIDIA NIM inference microservices are a very good entry point to playing with LLMs. Two big advantages of the Nvidia models is that they are free, and they have a wide range of models available.</p> <p></p> <p>Setting up an account is simple. Head to the Nvidia Build website and sign up. If you use a personal account, you will get 1,000 free requests, and if you use a business account (or sign up for an enterprise trial, you will get 5,000 free requests).</p> <p>You can now select from a broad range of models. Below is an example of the <code>meta/llama-3.2-3b-instruct</code> model. You can either play around with it in the Playground. Or you can use it as you would an OpenAI model.</p> <p></p> <p>Select <code>Get API Key</code> in the top right of the Python example, and you can get 24 hour access to the model. Once the 24 hours is up, you can always renew the key.</p>"},{"location":"setting-up/overview/","title":"Overview","text":"<p>For the in-person workshop, we will be using models from OpenAI. We understand that often in research, we want full control and ownership over our models. The models hosted by OpenAI are closed source, and therefore may be inappropriate for some use cases. However, there are some advantages to using OpenAI models:</p> <ol> <li>OpenAI arguably offer the best current models</li> <li>They are reliable</li> <li>The documentation is excellent</li> <li>They use the OpenAI API spec</li> </ol> <p>The final point is particularly important - many popular endpoints make use of the OpenAI API spec. For example, vLLM is a library that essentially wraps a hugging face model in an API spec that is compatible with OpenAI; Microsoft Azure AI Studio, Google Vertex AI, Amazon AWS Bedrock all offer ways to serve models using vLLM.</p> <p>So almost all of the skills you learn using OpenAI's models will be transferable to running your own models.</p> <p>If you want to use OpenAI, you will need to create an API key. For the in-person workshop, we will provide you with one. However, if you want to use your own open source models, then you can also do that. During the workshop, wherever possible, we will try and highlight the differences between using OpenAI and your own models.</p> <p>We offer an overview of how to setup various environments:</p> <ul> <li>GitHub Codespaces</li> <li>RunPod</li> <li>Nvidia NIMs</li> <li>Microsoft Azure AI Studio</li> </ul> <p>Pick one on the left-hand sidebar. More may be added in the future.</p>"},{"location":"setting-up/running-locally/","title":"Running Locally","text":"<p>If you'd rather not use GitHub Codespaces, you can run the workshop locally on your own machine by following the steps below.</p> <p>We recommend using <code>uv</code> for dependencies as it is much faster and automatically manages a <code>.venv</code> for you.</p> <p>However, you can also install dependencies with plain <code>pip</code> if you prefer.</p>"},{"location":"setting-up/running-locally/#1-cloning-the-repository","title":"1. Cloning the repository","text":"<p>Clone the repository to your machine: <pre><code>git clone https://github.com/acceleratescience/hands-on-llms\ncd hands-on-llms\n</code></pre> Alternatively, if you're using an IDE such as VS Code or PyCharm, you can paste the repository URL directly into the IDE's respective clone repository UI.</p>"},{"location":"setting-up/running-locally/#2-installing-dependencies-with-uv-preferred","title":"2. Installing dependencies with <code>uv</code> (preferred)","text":""},{"location":"setting-up/running-locally/#windows-powershell","title":"Windows (Powershell)","text":"<p>Install uv if needed: <pre><code>irm https://astral.sh/uv/install.ps1 | iex\n</code></pre> Create the virtual environment and install dependencies: <pre><code>uv sync\n</code></pre> Finally, activate the environment: <pre><code>.\\.venv\\Scripts\\activate\n</code></pre></p>"},{"location":"setting-up/running-locally/#macos-linux","title":"macOS / Linux","text":"<p>Install uv if needed: <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> Create the virtual environment and install dependencies: <pre><code>uv sync\n</code></pre> Finally, activate the environment: <pre><code>source .venv/bin/activate\n</code></pre></p>"},{"location":"setting-up/running-locally/#3-installing-dependencies-with-pip-alternative","title":"3. Installing dependencies with <code>pip</code> (alternative)","text":""},{"location":"setting-up/running-locally/#windows-powershell_1","title":"Windows (Powershell)","text":"<p>Create and activate an empty virtual environment: <pre><code>python3 -m hands-on-llms .venv\n.\\.venv\\Scripts\\activate\n</code></pre> Finally, install dependencies to virtual environment: <pre><code>pip install .\n</code></pre></p>"},{"location":"setting-up/running-locally/#macos-linux_1","title":"macOS / Linux","text":"<p>Create and activate an empty virtual environment: <pre><code>python3 -m hands-on-llms .venv\nsource .venv\\bin\\activate\n</code></pre> Finally, install dependencies to virtual environment: <pre><code>pip install .\n</code></pre></p>"},{"location":"setting-up/runpod/","title":"Runpod","text":"<p>For this demonstration we will run <code>llama-3.1-8b</code> on a Nvidia A40 GPU. If you want to do this, you will need to set up a RunPod account, add a payment method, and deposit some credit. For RunPod, you pay by the hour.</p>"},{"location":"setting-up/runpod/#setup-runpod-and-hugging-face-access","title":"Setup Runpod and Hugging Face access","text":"<p>1. Create a RunPod account Head to the runpod website and create an account.</p> <p>2. Over to <code>Billing</code> and add some credit You can add a payment method and deposit any amount you like.</p> <p>3. Gain access to Llama 3.1 You will need to request access to the Llama 3.1 models. You can do this by heading over to the Hugging Face site. You'll need to log in and request access. It's usually pretty quick, but it can take a day or two.</p> <p>4. Create a RunPod Template Head to templates and add a new template. Fill out the details as shown below.</p> <p></p> <p>For the token you'll need to add a new Hugging Face API token. You can create one here.</p> <p></p> <p>5. Create a Pod Once your template has been set, you can create a Pod. Head to <code>Pods</code> and click <code>+ Deploy</code>. Select the Nvidia A40 and the Llama 3.1 template you created earlier. The pod will take a while to to start up, but when it does you should see something like the below image</p> <p></p> <p>Hit <code>Connect</code> and then <code>Connect to HTTP Service [Port 8000]</code>. You will be directed to a confusing looking page that just says <code>{\"detail\":\"Not Found\"}</code>. Don't worry, this is normal. Copy the URL that is in the browser bar. This is your API endpoint.</p>"},{"location":"setting-up/runpod/#connect-to-your-api-endpoint","title":"Connect to your API endpoint","text":"<p>Now we can connect to our API endpoint. Fire up a Jupyter notebook, and try running the following, pasting in the URL you copied earlier.</p> <pre><code>from openai import OpenAI\n\nmodel = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\nbase_url = &lt;ENTER YOUR URL HERE&gt;\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = base_url + \"v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n</code></pre> <p>This sets up the client.</p> <pre><code>system_prompt_base = \"You are a helpful assistant.\"\n\ncompletion = client.chat.completions.create(\n    model=model,\n    messages= [\n            {\"role\": \"system\", \"content\": system_prompt_base},\n            {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n        ],\n    max_tokens=256,\n    stream=True,\n    temperature=0.6,\n    stop=\"&lt;|eot_id|&gt;\"\n)\n\n# completion.choices[0].message.content\n\nfor chunk in completion:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>If you switch back to the RunPod terminal, you should see some activity showing throughput (the number of tokens per second).</p> <p>If you get any errors, the RunPod terminal is the best place to see what went wrong.</p> <p>You can switch out Llama 3.1 for any of the other models supported by vLLM. Please note that not all of these models will have the same level of support. You might also find subtle differences. For example: the MistralAI models do not have an explicit system prompt; and some models have different context window limits that you may need to set manually in the RunPod template. If you are having trouble with a particular model, please feel free to ask for help.</p>"},{"location":"setting-up/runpod/#maas-or-self-service","title":"MaaS or Self-service?","text":"<p>That is the question. Do you use a MaaS service like Azure AI Studio, or run your own cloud instance? There are advantages and disadvantages to both.</p> <ul> <li>Self-service means greater control, but costs will be higher.</li> <li>You'll be able to run your own models freely, and hosting finetuned models will be the same cost as hosting the base model.</li> <li>There is a learning curve, but not significantly more than getting a MaaS up and running</li> </ul> <p>There will be a certain point where the cost to run a MaaS will outpace that of self-serving. Let's look at an example:</p> <ul> <li>The current price for 4x A40 GPUs is $1.56/hr.</li> <li>The current prices for Meta-Llama-3.1-70B-Instruct on Azure AI Studio:<ul> <li>Input:  $0.00268/1000 tokens</li> <li>Output: $0.00354/1000 tokens</li> </ul> </li> </ul> <p>The question now is: at what point does it become more cost effective for me to use a RunPod instance?</p> <p>Assuming an even amount of input and output tokens, it means you would need approximately 250k tokens each for input and output to get to the $1.56 cost in one hour. That's ~360k words. As a reference, the first 3 Harry Potter books combined are about 250K words.</p> <p>That's ~140 tokens per second. Can the A40s even achieve that kind of throughput...?</p>"},{"location":"states/","title":"States and Data Storage","text":"<p>In this section we cover two main areas:</p> <ol> <li>Managing system states - LLMs are inherently stateless, so we need a way to keep track of the information we give to and get out of them.</li> <li>Databases - an introduction to ChromaDB and how to build a database.</li> </ol> <p>We will cover how to keep track of information like conversation history and token usage. We then use LLMs to generate a running summary of a conversaton for us. Finally, we introduce databases, and explore how we can build and search them.</p>"},{"location":"states/6_states/","title":"Keeping track of information","text":"<p>We actually saw an example of this in the prompting notebook when we looked at few-shot prompting.</p> <p>Here is a really simple example of how we can keep track of the conversation history. We can first define a <code>system_state</code> dictionary that will store important information for us. We can give it a <code>conversation_history</code> key that will store the conversation history.</p> In\u00a0[3]: Copied! <pre>system_state = {\n    \"conversation_history\": []\n}\n</pre> system_state = {     \"conversation_history\": [] } In\u00a0[4]: Copied! <pre>system_prompt = (\n    \"You are a helpful philosophical assistant. \"\n    \"You will help me think about philosophical questions. \"\n    \"Please keep your answers concise and to the point.\"\n)\n\nsystem_state[\"conversation_history\"].append({\n    \"role\": \"system\",\n    \"content\": system_prompt\n})\n\nuser_prompt = \"What is the meaning of life?\"\n\nsystem_state[\"conversation_history\"].append({\n    \"role\": \"user\",\n    \"content\": user_prompt\n})\n</pre> system_prompt = (     \"You are a helpful philosophical assistant. \"     \"You will help me think about philosophical questions. \"     \"Please keep your answers concise and to the point.\" )  system_state[\"conversation_history\"].append({     \"role\": \"system\",     \"content\": system_prompt })  user_prompt = \"What is the meaning of life?\"  system_state[\"conversation_history\"].append({     \"role\": \"user\",     \"content\": user_prompt }) In\u00a0[5]: Copied! <pre>for message in system_state[\"conversation_history\"]:\n    print(f\"{message['role']}: {message['content']}\\n\")\n</pre> for message in system_state[\"conversation_history\"]:     print(f\"{message['role']}: {message['content']}\\n\") <pre>system: You are a helpful philosophical assistant. You will help me think about philosophical questions. Please keep your answers concise and to the point.\n\nuser: What is the meaning of life?\n\n</pre> <p>Now we can use this conversation history to generate a response.</p> In\u00a0[12]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n\nimport dotenv\nimport os\n\ndotenv.load_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nfrom rich.pretty import pprint\n</pre> from openai import OpenAI client = OpenAI()  import dotenv import os  dotenv.load_dotenv()  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  from rich.pretty import pprint In\u00a0[34]: Copied! <pre>response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=system_state[\"conversation_history\"],\n    max_tokens=512,\n    temperature=1.0\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=system_state[\"conversation_history\"],     max_tokens=512,     temperature=1.0 )  print(response.choices[0].message.content) <pre>The meaning of life is a deeply personal and subjective question. Various philosophical, spiritual, and existential perspectives offer different answers: \n\n1. **Existentialism** suggests that life has no inherent meaning, and individuals must create their own purpose.\n2. **Religious perspectives** often provide a framework where life's meaning is linked to a divine purpose or adherence to spiritual teachings.\n3. **Humanism** focuses on the meaning derived from human relationships, personal fulfillment, and contributing to the greater good.\n4. **Buddhism** emphasizes the pursuit of enlightenment and understanding the nature of suffering as key to a meaningful life.\n\nUltimately, the meaning of life may depend on one's values, beliefs, and experiences.\n</pre> <p>Great, but now what happens if I want to ask a follow up question? Without the conversation history?</p> In\u00a0[35]: Copied! <pre>follow_up_prompt = \"Can you tell more about point 1?\"\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": follow_up_prompt\n    }],\n    max_tokens=512,\n    temperature=1.0\n)\n\nprint(response.choices[0].message.content)\n</pre> follow_up_prompt = \"Can you tell more about point 1?\"  response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[{         \"role\": \"user\",         \"content\": follow_up_prompt     }],     max_tokens=512,     temperature=1.0 )  print(response.choices[0].message.content)  <pre>Of course! However, I need more context to provide a detailed response. Could you please specify what \"point 1\" you are referring to? This could relate to a list, an article, or a particular topic. Let me know so I can assist you better!\n</pre> <p>Obviously it has no memory of the previous conversation. So we just need to append the follow up prompt to the conversation history.</p> In\u00a0[36]: Copied! <pre>response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=system_state[\"conversation_history\"],\n    max_tokens=512,\n    temperature=1.0\n)\n\nsystem_state[\"conversation_history\"].append({\n    \"role\": \"assistant\",\n    \"content\": response.choices[0].message.content\n})\n\nfor message in system_state[\"conversation_history\"]:\n    print(f\"{message['role'].upper()}: {message['content']}\\n\")\n</pre> response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=system_state[\"conversation_history\"],     max_tokens=512,     temperature=1.0 )  system_state[\"conversation_history\"].append({     \"role\": \"assistant\",     \"content\": response.choices[0].message.content })  for message in system_state[\"conversation_history\"]:     print(f\"{message['role'].upper()}: {message['content']}\\n\") <pre>SYSTEM: You are a helpful philosophical assistant. You will help me think about philosophical questions. Please keep your answers concise and to the point.\n\nUSER: What is the meaning of life?\n\nASSISTANT: The meaning of life is a deeply personal and subjective question. Some philosophical perspectives suggest it's about seeking happiness, fulfilling potential, or contributing to others. Existentialists argue it\u2019s up to each individual to create their own meaning. Others find purpose in spiritual beliefs or connections with nature. Ultimately, it varies for each person based on their values, experiences, and beliefs.\n\n</pre> <p>And now we can keep the conversation going in a simple loop. If you run this cell a few times you will see that the conversation history is correctly maintained.</p> In\u00a0[\u00a0]: Copied! <pre>while True:\n    user_input = input(\"You: \")\n    \n    if user_input.lower() in ['exit', 'quit', 'bye']:\n        print(\"Assistant: Goodbye!\")\n        break\n    \n    system_state[\"conversation_history\"].append({\n        \"role\": \"user\",\n        \"content\": user_input\n    })\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=system_state[\"conversation_history\"],\n        max_tokens=512,\n        temperature=1.0\n    )\n    \n    assistant_response = response.choices[0].message.content\n    print(f\"Assistant: {assistant_response}\\n\")\n    \n    system_state[\"conversation_history\"].append({\n        \"role\": \"assistant\",\n        \"content\": assistant_response\n    })\n</pre> while True:     user_input = input(\"You: \")          if user_input.lower() in ['exit', 'quit', 'bye']:         print(\"Assistant: Goodbye!\")         break          system_state[\"conversation_history\"].append({         \"role\": \"user\",         \"content\": user_input     })          response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=system_state[\"conversation_history\"],         max_tokens=512,         temperature=1.0     )          assistant_response = response.choices[0].message.content     print(f\"Assistant: {assistant_response}\\n\")          system_state[\"conversation_history\"].append({         \"role\": \"assistant\",         \"content\": assistant_response     }) In\u00a0[45]: Copied! <pre>from rich.console import Console\nfrom rich.text import Text\n\nconsole = Console()\n</pre> from rich.console import Console from rich.text import Text  console = Console() In\u00a0[48]: Copied! <pre>colors = {\n    \"system\": \"green\",\n    \"user\": \"cyan\",\n    \"assistant\": \"magenta\"\n}\n\nfor message in system_state[\"conversation_history\"]:\n    role = message[\"role\"]\n    content = message[\"content\"]\n    color = colors[role]\n    console.print(f\"[{color}]{role.upper()}: {content}[/{color}]\")\n</pre> colors = {     \"system\": \"green\",     \"user\": \"cyan\",     \"assistant\": \"magenta\" }  for message in system_state[\"conversation_history\"]:     role = message[\"role\"]     content = message[\"content\"]     color = colors[role]     console.print(f\"[{color}]{role.upper()}: {content}[/{color}]\") <pre>SYSTEM: You are a helpful philosophical assistant. You will help me think about philosophical questions. Please \nkeep your answers concise and to the point.\n</pre> <pre>USER: What is the meaning of life?\n</pre> <pre>ASSISTANT: The meaning of life is a deeply personal and subjective question. Some philosophical perspectives \nsuggest it's about seeking happiness, fulfilling potential, or contributing to others. Existentialists argue it\u2019s \nup to each individual to create their own meaning. Others find purpose in spiritual beliefs or connections with \nnature. Ultimately, it varies for each person based on their values, experiences, and beliefs.\n</pre> <pre>USER: Can you tell me more about point 1?\n</pre> <pre>ASSISTANT: Certainly! The idea of seeking happiness as the meaning of life is central to various philosophical \ntraditions, particularly hedonism and utilitarianism. Here are some key points:\n\n1. **Hedonism**: This school of thought posits that pleasure and the avoidance of pain are the highest goods. \nAccording to hedonists, a fulfilling life is one that maximizes pleasure and minimizes suffering.\n\n2. **Utilitarianism**: Proposed by philosophers like Jeremy Bentham and John Stuart Mill, utilitarianism expands on\nhedonism by suggesting that the right action is the one that produces the greatest overall happiness for the \ngreatest number of people. It emphasizes collective well-being.\n\n3. **Eudaimonia**: In Aristotelian ethics, eudaimonia is often translated as \"flourishing\" or \"well-being.\" \nAristotle argues that a meaningful life involves fulfilling one's potential through virtuous actions, intellectual \ngrowth, and community engagement.\n\n4. **Positive Psychology**: This modern psychological approach studies well-being and happiness, emphasizing \nfactors like relationships, purpose, and personal strengths as keys to a meaningful life.\n\nOverall, pursuing happiness as a guiding principle encourages individuals to reflect on their desires, values, and \nthe impacts of their actions on themselves and others.\n</pre> <pre>USER: That second one sounds interesting. Tell me more about those two people.\n</pre> <pre>ASSISTANT: Certainly! Here\u2019s a brief overview of Jeremy Bentham and John Stuart Mill, two key figures in \nutilitarianism:\n\n### Jeremy Bentham (1748\u20131832)\n- **Founder of Utilitarianism**: Bentham is often considered the father of utilitarianism. He introduced the idea \nthat the moral worth of an action is determined by its contribution to overall happiness or pleasure.\n- **Principle of Utility**: He formulated the \"greatest happiness principle,\" stating that the best action is the \none that produces the most happiness for the greatest number of people.\n- **Hedonic Calculus**: Bentham proposed a method to evaluate the moral rightness of actions based on their \nconsequences, which he termed the \"hedonic calculus.\" This involves measuring factors like intensity, duration, \ncertainty, and proximity of pleasure or pain.\n- **Reformist Ideas**: Bentham was also an advocate for social reforms, including legal changes to improve justice,\neducation, and animal rights.\n\n### John Stuart Mill (1806\u20131873)\n- **Advancement of Utilitarianism**: Mill expanded on Bentham's ideas, refining the concept of utility and \naddressing some of its criticisms.\n- **Qualitative Distinction**: Mill argued that not all pleasures are equal; he distinguished between higher \n(intellectual and moral) and lower (bodily) pleasures. He believed higher pleasures lead to more profound happiness\nand should be prioritized.\n- **Liberty and Individual Rights**: In his famous work \"On Liberty,\" Mill emphasized the importance of personal \nfreedom and autonomy, asserting that individuals should have the right to pursue their own happiness, provided it \ndoes not harm others.\n- **Feminism and Social Issues**: Mill was also an early advocate for women's rights, notably in his work \"The \nSubjection of Women,\" arguing for equality and the importance of women's contributions to society.\n\nBoth Bentham and Mill have had a lasting influence on ethics, politics, and social reform, shaping discussions \naround moral philosophy and the pursuit of happiness.\n</pre> In\u00a0[114]: Copied! <pre>class Conversation:\n    def __init__(self, system_prompt):\n        self.system_prompt = system_prompt\n        self.history = []\n        self.tokens = 0\n        self.token_limit = 300\n\n        self.add_message(\"system\", system_prompt)\n\n    def add_message(self, role, content):\n        self.history.append({\"role\": role, \"content\": content})\n        self.tokens += len(content)\n\n    def check_token_limit(self):\n        while self.tokens &gt; self.token_limit and len(self.history) &gt; 1:\n            # Remove the oldest non-system message\n            for i in range(1, len(self.history)):\n                if self.history[i][\"role\"] != \"system\":\n                    removed_message = self.history.pop(i)\n                    self.tokens -= len(removed_message[\"content\"])\n                    break\n\n    def response(self, user_input):\n        self.add_message(\"user\", user_input)\n        if self.tokens &gt; self.token_limit:\n            self.check_token_limit()\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=self.history,\n            max_tokens=512,\n            temperature=1.0\n        ).choices[0].message.content\n        \n        self.add_message(\"assistant\", response)\n\n        return response\n                \n</pre> class Conversation:     def __init__(self, system_prompt):         self.system_prompt = system_prompt         self.history = []         self.tokens = 0         self.token_limit = 300          self.add_message(\"system\", system_prompt)      def add_message(self, role, content):         self.history.append({\"role\": role, \"content\": content})         self.tokens += len(content)      def check_token_limit(self):         while self.tokens &gt; self.token_limit and len(self.history) &gt; 1:             # Remove the oldest non-system message             for i in range(1, len(self.history)):                 if self.history[i][\"role\"] != \"system\":                     removed_message = self.history.pop(i)                     self.tokens -= len(removed_message[\"content\"])                     break      def response(self, user_input):         self.add_message(\"user\", user_input)         if self.tokens &gt; self.token_limit:             self.check_token_limit()         response = client.chat.completions.create(             model=\"gpt-4o-mini\",             messages=self.history,             max_tokens=512,             temperature=1.0         ).choices[0].message.content                  self.add_message(\"assistant\", response)          return response                  <p>Let's see how this works.</p> <p>Let's see if we can get the model to forget about things we mention at the start of a conversation.</p> In\u00a0[119]: Copied! <pre>conversation = Conversation(system_prompt)\n</pre> conversation = Conversation(system_prompt) In\u00a0[120]: Copied! <pre>print(conversation.response(\"Hello, my name is Bob and I am 25 years old!\"))\nprint(f\"Tokens: {conversation.tokens}\")\n</pre> print(conversation.response(\"Hello, my name is Bob and I am 25 years old!\")) print(f\"Tokens: {conversation.tokens}\") <pre>Hello, Bob! It's nice to meet you. What philosophical question or topic would you like to explore today?\nTokens: 295\n</pre> In\u00a0[121]: Copied! <pre>print(conversation.response(\"What is my name?\"))\nprint(f\"Tokens: {conversation.tokens}\")\n</pre> print(conversation.response(\"What is my name?\")) print(f\"Tokens: {conversation.tokens}\")  <pre>You mentioned your name is Bob. How can I assist you further?\nTokens: 328\n</pre> <p>Great, so now we have hit our token limit, and the conversation should be trimmed in the next response.</p> In\u00a0[122]: Copied! <pre>print(conversation.response(\"What is my age?\"))\nprint(f\"Tokens: {conversation.tokens}\")\n</pre> print(conversation.response(\"What is my age?\")) print(f\"Tokens: {conversation.tokens}\") <pre>I don't have access to personal information, so I can't know your age. You could share it if you'd like to discuss it further!\nTokens: 365\n</pre> In\u00a0[123]: Copied! <pre>pprint(conversation.history, expand_all=True)\n</pre> pprint(conversation.history, expand_all=True) <pre>[\n\u2502   {\n\u2502   \u2502   'role': 'system',\n\u2502   \u2502   'content': 'You are a helpful philosophical assistant. You will help me think about philosophical questions. Please keep your answers concise and to the point.'\n\u2502   },\n\u2502   {\n\u2502   \u2502   'role': 'user',\n\u2502   \u2502   'content': 'What is my name?'\n\u2502   },\n\u2502   {\n\u2502   \u2502   'role': 'assistant',\n\u2502   \u2502   'content': 'You mentioned your name is Bob. How can I assist you further?'\n\u2502   },\n\u2502   {\n\u2502   \u2502   'role': 'user',\n\u2502   \u2502   'content': 'What is my age?'\n\u2502   },\n\u2502   {\n\u2502   \u2502   'role': 'assistant',\n\u2502   \u2502   'content': \"I don't have access to personal information, so I can't know your age. You could share it if you'd like to discuss it further!\"\n\u2502   }\n]\n</pre> <p>This is a good start, but there is a problem here. What if there was something very important that we wanted to keep track of that was mentioned at the start of the conversation, but it has been cut off!?</p>"},{"location":"states/6_states/#keeping-track-of-information","title":"Keeping track of information\u00b6","text":"<p>LLMs calls are inherently stateless - they do not have memory of any previous interactions. Every call is an independent event, and YOU must manage any information that needs to be carried over time.</p> <p>In this notebook we will look at a few different things that we might want to keep track of between calls.</p>"},{"location":"states/6_states/#conversation-history","title":"Conversation History\u00b6","text":"<p>Keeping track of the conversation history is actually easy. Firstly it is important to remember that LLMs often have the following pattern:</p> <pre><code>-&gt; system prompt\n-&gt; user prompt\n-&gt; model response\n\n-&gt; user prompt\n-&gt; model response\n\n-&gt; user prompt\n-&gt; model response\n\n-&gt; etc.\n</code></pre>"},{"location":"states/6_states/#tracking-tokens","title":"Tracking tokens\u00b6","text":"<p>We should probably also track the tokens. This can be useful for a few reasons - we can track costs, and we can use it to cut off conversation history when we get too close to our limit.</p> <p>We can make this as simple or complicated as we want. Probably we should create a <code>Conversation</code> class to keep track of things like this.</p>"},{"location":"states/7_summary/","title":"Generating a Summary","text":"<p>In the previous example, we explored how to keep track of the conversataion history and the tokens used.</p> <p>In this section, we will look at how to generate a summary of the conversation so far. We can then use this summary as context for a conversation.</p> In\u00a0[42]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n\nimport dotenv\nimport os\n\ndotenv.load_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nfrom rich.pretty import pprint\n</pre> from openai import OpenAI client = OpenAI()  import dotenv import os  dotenv.load_dotenv()  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  from rich.pretty import pprint <p>For this example, we will create a Jinja template.</p> <pre><code>You are a helpful summariser, tracking information about a conversation in JSON format.\nDo not include an redundant information and do not hallucinate.\nDo not respond to the user, only update the summary.\nKeep the summary concise and to the point.\nYou have just received a new message from the user (\"input_message\"), and the response from the assistant (\"assistant_response\").\nYou will update the current summary (\"current_summary\") in JSON format according to the following schema:\n\n{{ schema }}\n\n### Current Summary ###\n\n{{ current_summary }}\n\n### User Message ###\n\n{{ input_message }}\n\n### Assistant Response ###\n\n{{ assistant_response }}\n\n### New Summary ###\n\n</code></pre> <p>So we have three arguments: the schema, the current summary, and the new message. Notice that we are also using a schema for the summary, so we will also need to use a Pydantic model to define the schema.</p> In\u00a0[64]: Copied! <pre>from pydantic import BaseModel, Field\n\nclass Summary(BaseModel):\n    first_name: str = Field(\"unknown\", description=\"The first name of the user.\")\n    last_name: str = Field(\"unknown\", description=\"The last name of the user.\")\n    unresolved_questions: list[str] = Field([], description=\"A list of questions that the user has asked that the assistant has not yet resolved.\")\n    summary: str = Field(\"unknown\", description=\"The running summary of the conversation so far. Update with the previous assistant response and the new input from the user.\")\n</pre> from pydantic import BaseModel, Field  class Summary(BaseModel):     first_name: str = Field(\"unknown\", description=\"The first name of the user.\")     last_name: str = Field(\"unknown\", description=\"The last name of the user.\")     unresolved_questions: list[str] = Field([], description=\"A list of questions that the user has asked that the assistant has not yet resolved.\")     summary: str = Field(\"unknown\", description=\"The running summary of the conversation so far. Update with the previous assistant response and the new input from the user.\") In\u00a0[65]: Copied! <pre>current_summary = Summary()\npprint(current_summary, expand_all=True)\n</pre> current_summary = Summary() pprint(current_summary, expand_all=True) <pre>Summary(\n\u2502   first_name='unknown',\n\u2502   last_name='unknown',\n\u2502   unresolved_questions=[],\n\u2502   summary='unknown'\n)\n</pre> In\u00a0[66]: Copied! <pre>from jinja2 import Environment, FileSystemLoader, select_autoescape\nfrom typing import Any\n\ndef load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    env = Environment(\n        loader=FileSystemLoader(searchpath='./'),\n        autoescape=select_autoescape()\n    )\n    template = env.get_template(template_filepath)\n    return template.render(**arguments)\n</pre> from jinja2 import Environment, FileSystemLoader, select_autoescape from typing import Any  def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     env = Environment(         loader=FileSystemLoader(searchpath='./'),         autoescape=select_autoescape()     )     template = env.get_template(template_filepath)     return template.render(**arguments) In\u00a0[67]: Copied! <pre>summary_prompt = load_template(\n    \"prompts/summary_system_prompt.jinja\",\n    {\n        \"schema\": current_summary.model_json_schema(),\n        \"current_summary\": current_summary.model_dump(),\n        \"input_message\": \"Hello, nice to meet you!\",\n        \"assistant_response\": \"Nice to meet you too! What is your name?\",\n    }\n)\n</pre> summary_prompt = load_template(     \"prompts/summary_system_prompt.jinja\",     {         \"schema\": current_summary.model_json_schema(),         \"current_summary\": current_summary.model_dump(),         \"input_message\": \"Hello, nice to meet you!\",         \"assistant_response\": \"Nice to meet you too! What is your name?\",     } ) In\u00a0[68]: Copied! <pre>print(summary_prompt)\n</pre> print(summary_prompt) <pre>You are a helpful summariser, tracking information about a conversation in JSON format.\nDo not include an redundant information and do not hallucinate.\nDo not respond to the user, only update the summary.\nKeep the summary concise and to the point.\nYou have just received a new message from the user (\"input_message\"), and the response from the assistant (\"assistant_response\").\nYou will update the current summary (\"current_summary\") in JSON format according to the following schema:\n\n{'properties': {'first_name': {'default': 'unknown', 'description': 'The first name of the user.', 'title': 'First Name', 'type': 'string'}, 'last_name': {'default': 'unknown', 'description': 'The last name of the user.', 'title': 'Last Name', 'type': 'string'}, 'unresolved_questions': {'default': [], 'description': 'A list of questions that the user has asked that the assistant has not yet resolved.', 'items': {'type': 'string'}, 'title': 'Unresolved Questions', 'type': 'array'}, 'summary': {'default': 'unknown', 'description': 'The running summary of the conversation so far. Update with the previous assistant response and the new input from the user.', 'title': 'Summary', 'type': 'string'}}, 'title': 'Summary', 'type': 'object'}\n\n### Current Summary ###\n\n{'first_name': 'unknown', 'last_name': 'unknown', 'unresolved_questions': [], 'summary': 'unknown'}\n\n### User Message ###\n\nHello, nice to meet you!\n\n### Assistant Response ###\n\nNice to meet you too! What is your name?\n\n### New Summary ###\n</pre> In\u00a0[69]: Copied! <pre>import json\n\ndef generate_summary(input_message: str) -&gt; Summary:\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"user\", \"content\": summary_prompt}\n        ],\n        response_format={\"type\": \"json_object\"},\n        temperature=0.0,\n    )\n\n    json_content = json.loads(response.choices[0].message.content)\n    current_summary = Summary(**json_content)\n    \n    return current_summary\n\ncurrent_summary = generate_summary(summary_prompt)\n\npprint(current_summary, expand_all=True)\n</pre> import json  def generate_summary(input_message: str) -&gt; Summary:     response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {\"role\": \"user\", \"content\": summary_prompt}         ],         response_format={\"type\": \"json_object\"},         temperature=0.0,     )      json_content = json.loads(response.choices[0].message.content)     current_summary = Summary(**json_content)          return current_summary  current_summary = generate_summary(summary_prompt)  pprint(current_summary, expand_all=True)  <pre>Summary(\n\u2502   first_name='unknown',\n\u2502   last_name='unknown',\n\u2502   unresolved_questions=[\n\u2502   \u2502   'What is your name?'\n\u2502   ],\n\u2502   summary=\"User greeted the assistant. Assistant responded and asked for the user's name.\"\n)\n</pre> <p>We have set the temperature to 0.0, to avoid the model being too \"creative\".</p> <p>How let's try another input.</p> In\u00a0[70]: Copied! <pre>summary_prompt = load_template(\n    \"prompts/summary_system_prompt.jinja\",\n    {\n        \"schema\": current_summary.model_json_schema(),\n        \"current_summary\": current_summary.model_dump(),\n        \"input_message\": \"My name is Garfield Leopard. I am interested in learning about cats.\",\n        \"assistant_response\": \"Nice to meet you Garfield! I am also interested in learning about cats. What are you interested in learning about cats?\",\n    }\n)\n\ncurrent_summary = generate_summary(summary_prompt)\n\npprint(current_summary, expand_all=True)\n</pre> summary_prompt = load_template(     \"prompts/summary_system_prompt.jinja\",     {         \"schema\": current_summary.model_json_schema(),         \"current_summary\": current_summary.model_dump(),         \"input_message\": \"My name is Garfield Leopard. I am interested in learning about cats.\",         \"assistant_response\": \"Nice to meet you Garfield! I am also interested in learning about cats. What are you interested in learning about cats?\",     } )  current_summary = generate_summary(summary_prompt)  pprint(current_summary, expand_all=True)  <pre>Summary(\n\u2502   first_name='Garfield',\n\u2502   last_name='Leopard',\n\u2502   unresolved_questions=[\n\u2502   \u2502   'What are you interested in learning about cats?'\n\u2502   ],\n\u2502   summary='User introduced themselves as Garfield Leopard and expressed interest in learning about cats. Assistant acknowledged the introduction and asked what specific topics about cats the user is interested in.'\n)\n</pre> <p>So now we have updated the summary with the messages so far. But there is still one thing missing: we are not actually talking to anything! We are manually updating the messages with fake chatbot responses.</p> <p>So now, let's add another model instance to this conversation...</p> <p>In this extension, we add a chatbot to the conversation. The prompt is as follows:</p> <pre><code>You are a helpful assistant. You will respond to the user's message (\"input_message\") with a helpful response.\nYou will also be given a running summary (\"running_summary\") of the conversation so far in JSON format.\nYou can use this summary to help you answer the user's question.\n\n### Running Summary ###\n\n{{ running_summary }}\n\n### User Message ###\n\n{{ input_message }}\n\n</code></pre> <p>Now we need to write logic to update the summary with the new messages. Let's walk through this step by step (because we know that this will make us perform better \ud83d\ude04). We need to:</p> <ol> <li>Get the user input</li> <li>Get the prompt template for the chatbot</li> <li>Feed this into the chatbot along with the running summary</li> <li>Update the running summary with the new response from the chatbot</li> </ol> In\u00a0[76]: Copied! <pre># 1. Get user input\nuser_input = \"I think I would like to learn about the noble jaguar. Can you tell me more about them?\"\n\n# 2. Get the prompt template for the chatbot\nchat_prompt = load_template(\n    \"prompts/bot_prompt.jinja\",\n    {\n        \"running_summary\": current_summary.model_dump(),\n        \"input_message\": user_input,\n    }\n)\n</pre> # 1. Get user input user_input = \"I think I would like to learn about the noble jaguar. Can you tell me more about them?\"  # 2. Get the prompt template for the chatbot chat_prompt = load_template(     \"prompts/bot_prompt.jinja\",     {         \"running_summary\": current_summary.model_dump(),         \"input_message\": user_input,     } ) In\u00a0[77]: Copied! <pre># 3. Feed this into the chatbot along with the running summary\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": chat_prompt}\n    ],\n    temperature=0.7,\n)\n\nprint(response.choices[0].message.content)\n</pre> # 3. Feed this into the chatbot along with the running summary response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"user\", \"content\": chat_prompt}     ],     temperature=0.7, )  print(response.choices[0].message.content) <pre>Jaguars are fascinating and powerful big cats native to the Americas. Here are some key facts about them:\n\n1. **Scientific Classification**: They belong to the species Panthera onca and are part of the Felidae family.\n\n2. **Habitat**: Jaguars are primarily found in rainforests, but they also inhabit savannas and grasslands. Their range extends from the southern United States to South America, particularly in the Amazon Basin.\n\n3. **Physical Characteristics**: Jaguars are known for their robust build and distinctive coat, which is usually a golden-yellow with black rosettes. They are the largest cats in the Americas and the third-largest in the world after tigers and lions.\n\n4. **Diet and Hunting**: Jaguars are carnivorous and have a varied diet that includes deer, capybaras, and even caimans. They are unique among big cats in that they often hunt by biting through the skull or shell of their prey.\n\n5. **Behavior**: They are solitary creatures and are excellent swimmers, often found near water. They are also known for their powerful jaws and stealthy hunting techniques.\n\n6. **Conservation Status**: Jaguars are currently listed as Near Threatened by the IUCN due to habitat loss and poaching. Conservation efforts are in place to protect their habitats and ensure their survival.\n\nIf you have specific aspects of jaguars that you'd like to learn more about, feel free to ask!\n</pre> In\u00a0[78]: Copied! <pre># 4. Update the running summary with the new response from the chatbot\nsummary_prompt = load_template(\n    \"prompts/summary_system_prompt.jinja\",\n    {\n        \"schema\": current_summary.model_json_schema(),\n        \"current_summary\": current_summary.model_dump(),\n        \"input_message\": user_input,\n        \"assistant_response\": response.choices[0].message.content,\n    }\n)\n\ncurrent_summary = generate_summary(summary_prompt)\n\npprint(current_summary, expand_all=True)\n</pre> # 4. Update the running summary with the new response from the chatbot summary_prompt = load_template(     \"prompts/summary_system_prompt.jinja\",     {         \"schema\": current_summary.model_json_schema(),         \"current_summary\": current_summary.model_dump(),         \"input_message\": user_input,         \"assistant_response\": response.choices[0].message.content,     } )  current_summary = generate_summary(summary_prompt)  pprint(current_summary, expand_all=True) <pre>Summary(\n\u2502   first_name='Garfield',\n\u2502   last_name='Leopard',\n\u2502   unresolved_questions=[],\n\u2502   summary='User expressed interest in learning about the noble jaguar. Assistant provided key facts about jaguars, including their classification, habitat, physical characteristics, diet, behavior, and conservation status.'\n)\n</pre> <p>So now we have a running summary of the conversation so far. Now out model is keeping track of the important information in the conversation without us having to feed in the full conversation history each time, which could cost a lot of money. In reality, we might to combine these approaches to get the best of both worlds.</p> <p>We can also add additional information to our <code>Summary</code> object to keep track of additional information.</p>"},{"location":"states/7_summary/#generating-a-summary","title":"Generating a Summary\u00b6","text":""},{"location":"states/7_summary/#simple-summary","title":"Simple summary\u00b6","text":""},{"location":"states/7_summary/#adding-a-chatbot","title":"Adding a chatbot\u00b6","text":""},{"location":"states/8_data_storage/","title":"Data storage","text":"<p>Although we have been implementing our own storage for chat history, and the ability to summarize conversations, it would be nice to have a more robust storage solution. It would also be nice to be able to search over our previous conversations.</p> <p>There are many different options for storing data:</p> <ul> <li>Redis</li> <li>Postgres</li> <li>DynamoDB</li> <li>Pinecone</li> </ul> <p>But we will use ChromaDB. Everybody has an opinion about various vectorstores, and many of them are valid. The reason we chose ChromaDB is because it is very easy to use, and get up and running quickly.</p> <p>In this section, we will first set up a database and use it to store query over our chat history.</p> In\u00a0[1]: Copied! <pre>import chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\n# All the usual imports\nfrom rich.pretty import pprint\nimport dotenv\nimport os\ndotenv.load_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> import chromadb from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction  # All the usual imports from rich.pretty import pprint import dotenv import os dotenv.load_dotenv() OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") <p>First we create a client to connect to our database.</p> <p>We will use an OpenAI embedding model, <code>text-embedding-3-small</code>, to embed our chat history entries.</p> <p>We create a class so we can add some extra functionality, such as clearing the database, and a counter to keep track of the number of entries.</p> In\u00a0[2]: Copied! <pre>class ChatDB:\n    def __init__(self, name: str, model_name: str = \"text-embedding-3-small\"):\n        self.model_name = model_name\n        self.client = chromadb.PersistentClient(path=\"./\")\n        self.embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY, model_name=model_name)\n        self.chat_db = self.client.create_collection(name=name, embedding_function=self.embedding_function, metadata={\"hnsw:space\": \"cosine\"})\n        self.id_counter = 0\n\n\n    def add_conversation_to_db(self, user_message: str, ai_message: str):\n        \"\"\"Add a conversation between user and AI to the database.\n\n        Args:\n            user_message (str): User input message.\n            ai_message (str): Response from the AI.\n        \"\"\"\n        self.chat_db.add(\n            documents=[f\"User: {user_message}\\nAI: {ai_message}\"],\n            metadatas=[{\"user_message\": user_message, \"ai_message\": ai_message}],\n            ids=[str(self.id_counter)]\n        )\n        self.id_counter += 1\n\n\n    def get_all_entries(self) -&gt; dict:\n        \"\"\"Grab all of the entries in the database.\n\n        Returns:\n            dict: All entries in the database.\n        \"\"\"\n        return self.chat_db.get()\n    \n\n    def clear_db(self, reinitialize: bool = True):\n        \"\"\"Clear the database of all entries, and reinitialize it.\n\n        Args:\n            reinitialize (bool, optional): _description_. Defaults to True.\n        \"\"\"\n        self.client.delete_collection(self.chat_db.name)\n        # re-initialize the database\n        if reinitialize:\n            self.__init__(self.chat_db.name, self.model_name)\n\n\n    def query_db(self, query_text: str, n_results: int = 2) -&gt; dict:\n        \"\"\"Given some query text, return the n_results most similar entries in the database.\n\n        Args:\n            query_text (str): The text to query the database with.\n            n_results (int): The number of results to return.\n\n        Returns:\n            dict: The most similar entries in the database.\n        \"\"\"\n        return self.chat_db.query(query_texts=[query_text], n_results=n_results)\n</pre> class ChatDB:     def __init__(self, name: str, model_name: str = \"text-embedding-3-small\"):         self.model_name = model_name         self.client = chromadb.PersistentClient(path=\"./\")         self.embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY, model_name=model_name)         self.chat_db = self.client.create_collection(name=name, embedding_function=self.embedding_function, metadata={\"hnsw:space\": \"cosine\"})         self.id_counter = 0       def add_conversation_to_db(self, user_message: str, ai_message: str):         \"\"\"Add a conversation between user and AI to the database.          Args:             user_message (str): User input message.             ai_message (str): Response from the AI.         \"\"\"         self.chat_db.add(             documents=[f\"User: {user_message}\\nAI: {ai_message}\"],             metadatas=[{\"user_message\": user_message, \"ai_message\": ai_message}],             ids=[str(self.id_counter)]         )         self.id_counter += 1       def get_all_entries(self) -&gt; dict:         \"\"\"Grab all of the entries in the database.          Returns:             dict: All entries in the database.         \"\"\"         return self.chat_db.get()           def clear_db(self, reinitialize: bool = True):         \"\"\"Clear the database of all entries, and reinitialize it.          Args:             reinitialize (bool, optional): _description_. Defaults to True.         \"\"\"         self.client.delete_collection(self.chat_db.name)         # re-initialize the database         if reinitialize:             self.__init__(self.chat_db.name, self.model_name)       def query_db(self, query_text: str, n_results: int = 2) -&gt; dict:         \"\"\"Given some query text, return the n_results most similar entries in the database.          Args:             query_text (str): The text to query the database with.             n_results (int): The number of results to return.          Returns:             dict: The most similar entries in the database.         \"\"\"         return self.chat_db.query(query_texts=[query_text], n_results=n_results) <p>Now we can initialize our database and add some entries.</p> In\u00a0[3]: Copied! <pre>chat_db = ChatDB(\"chat_db\", \"text-embedding-3-small\")\n</pre> chat_db = ChatDB(\"chat_db\", \"text-embedding-3-small\") In\u00a0[4]: Copied! <pre>chat_db.add_conversation_to_db(\n    \"Hello, my name is Alice, how are you?\",\n    \"Nice to meet you Alice, I am Bob. I am fine, thank you for asking. How can I help you today?\",\n)\nchat_db.add_conversation_to_db(\n    \"I am looking for a restaurant in the area.\",\n    \"Great! What type of cuisine are you in the mood for?\",\n)\n\nchat_db.add_conversation_to_db(\n    \"I am looking for some Italian food.\",\n    \"There are many good Italian restaurants in the area. What is your budget?\",\n)\n</pre> chat_db.add_conversation_to_db(     \"Hello, my name is Alice, how are you?\",     \"Nice to meet you Alice, I am Bob. I am fine, thank you for asking. How can I help you today?\", ) chat_db.add_conversation_to_db(     \"I am looking for a restaurant in the area.\",     \"Great! What type of cuisine are you in the mood for?\", )  chat_db.add_conversation_to_db(     \"I am looking for some Italian food.\",     \"There are many good Italian restaurants in the area. What is your budget?\", ) In\u00a0[5]: Copied! <pre>entries = chat_db.get_all_entries()\nfor entry in entries[\"documents\"]:\n    print(entry)\n    print(\"-\"*100)\n</pre> entries = chat_db.get_all_entries() for entry in entries[\"documents\"]:     print(entry)     print(\"-\"*100) <pre>User: Hello, my name is Alice, how are you?\nAI: Nice to meet you Alice, I am Bob. I am fine, thank you for asking. How can I help you today?\n----------------------------------------------------------------------------------------------------\nUser: I am looking for a restaurant in the area.\nAI: Great! What type of cuisine are you in the mood for?\n----------------------------------------------------------------------------------------------------\nUser: I am looking for some Italian food.\nAI: There are many good Italian restaurants in the area. What is your budget?\n----------------------------------------------------------------------------------------------------\n</pre> <p>Now we can try and query the database.</p> In\u00a0[6]: Copied! <pre>results = chat_db.query_db(\"Food\", n_results=3)\npprint(results, expand_all=True)\n</pre> results = chat_db.query_db(\"Food\", n_results=3) pprint(results, expand_all=True) <pre>{\n\u2502   'ids': [\n\u2502   \u2502   [\n\u2502   \u2502   \u2502   '1',\n\u2502   \u2502   \u2502   '2',\n\u2502   \u2502   \u2502   '0'\n\u2502   \u2502   ]\n\u2502   ],\n\u2502   'distances': [\n\u2502   \u2502   [\n\u2502   \u2502   \u2502   0.7267490239862444,\n\u2502   \u2502   \u2502   0.757357007227763,\n\u2502   \u2502   \u2502   0.8727205850443006\n\u2502   \u2502   ]\n\u2502   ],\n\u2502   'metadatas': [\n\u2502   \u2502   [\n\u2502   \u2502   \u2502   {\n\u2502   \u2502   \u2502   \u2502   'ai_message': 'Great! What type of cuisine are you in the mood for?',\n\u2502   \u2502   \u2502   \u2502   'user_message': 'I am looking for a restaurant in the area.'\n\u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   {\n\u2502   \u2502   \u2502   \u2502   'ai_message': 'There are many good Italian restaurants in the area. What is your budget?',\n\u2502   \u2502   \u2502   \u2502   'user_message': 'I am looking for some Italian food.'\n\u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   {\n\u2502   \u2502   \u2502   \u2502   'ai_message': 'Nice to meet you Alice, I am Bob. I am fine, thank you for asking. How can I help you today?',\n\u2502   \u2502   \u2502   \u2502   'user_message': 'Hello, my name is Alice, how are you?'\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   ]\n\u2502   ],\n\u2502   'embeddings': None,\n\u2502   'documents': [\n\u2502   \u2502   [\n\u2502   \u2502   \u2502   'User: I am looking for a restaurant in the area.\\nAI: Great! What type of cuisine are you in the mood for?',\n\u2502   \u2502   \u2502   'User: I am looking for some Italian food.\\nAI: There are many good Italian restaurants in the area. What is your budget?',\n\u2502   \u2502   \u2502   'User: Hello, my name is Alice, how are you?\\nAI: Nice to meet you Alice, I am Bob. I am fine, thank you for asking. How can I help you today?'\n\u2502   \u2502   ]\n\u2502   ],\n\u2502   'uris': None,\n\u2502   'data': None,\n\u2502   'included': [\n\u2502   \u2502   'metadatas',\n\u2502   \u2502   'documents',\n\u2502   \u2502   'distances'\n\u2502   ]\n}\n</pre> <p>Notice that we have access to the cosine distance scores for each entry. The closer the score to 0, the more similar the query is to the entry.</p> In\u00a0[7]: Copied! <pre>for i, entry in enumerate(results[\"documents\"][0]):\n    print(entry)\n    print(f\"score: {results['distances'][0][i]}\")\n    print(\"-\"*10)\n</pre>  for i, entry in enumerate(results[\"documents\"][0]):     print(entry)     print(f\"score: {results['distances'][0][i]}\")     print(\"-\"*10) <pre>User: I am looking for a restaurant in the area.\nAI: Great! What type of cuisine are you in the mood for?\nscore: 0.7267490239862444\n----------\nUser: I am looking for some Italian food.\nAI: There are many good Italian restaurants in the area. What is your budget?\nscore: 0.757357007227763\n----------\nUser: Hello, my name is Alice, how are you?\nAI: Nice to meet you Alice, I am Bob. I am fine, thank you for asking. How can I help you today?\nscore: 0.8727205850443006\n----------\n</pre> <p>Now we can clear the entries</p> In\u00a0[8]: Copied! <pre>chat_db.clear_db()\nentries = chat_db.get_all_entries()\nfor entry in entries[\"documents\"]:\n    print(entry)\n    print(\"-\"*10)\n</pre> chat_db.clear_db() entries = chat_db.get_all_entries() for entry in entries[\"documents\"]:     print(entry)     print(\"-\"*10) <p>And as expected it is empty.</p> <p>Now we can integrate this database into a chat model.</p> <p>All that we really need to do is write the prompts and the logic for storing and retrieving the chat history. Sounds easy enough!</p> <p>The system prompt will be simple:</p> <pre><code>You are a sarcastic assistant that loves to roast the user.\nYou will be given a new user input (\"input_message\") and a some potential relevant chat history (\"relevant_chat_history\").\nNot that the context may be empty or may contain some non-relevant information. You must decide whether to use the context to inform your response.\n</code></pre> <p>And the user prompt is then:</p> <pre><code>### Relevant chat history\n\n{{ relevant_chat_history }}\n\n### User input\n\n{{ input_message }}\n</code></pre> <p>And now we can put this all together. First, we'll just write a function to combine the context in a nice way.</p> In\u00a0[9]: Copied! <pre>def combined_context(documents: list[str], scores: list[float]) -&gt; str:\n    string = \"\"\n    for document, score in zip(documents, scores):\n        string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"\n    return string\n</pre> def combined_context(documents: list[str], scores: list[float]) -&gt; str:     string = \"\"     for document, score in zip(documents, scores):         string += f\"{document}\\nCosine distance: {score:.2f}\\n{'-'*10}\\n\"     return string In\u00a0[11]: Copied! <pre>user_input = \"Hello, my name is Alice, how are you?\"\n\ndef get_context(user_input: str, n_results: int = 2, chat_db: ChatDB = chat_db) -&gt; str:\n    results = chat_db.query_db(user_input, n_results=2)\n    context = combined_context(results[\"documents\"][0], results[\"distances\"][0])\n    if not context:\n        context = \"No relevant chat history found.\"\n    return context\n\ncontext = get_context(user_input)\nprint(context)\n</pre> user_input = \"Hello, my name is Alice, how are you?\"  def get_context(user_input: str, n_results: int = 2, chat_db: ChatDB = chat_db) -&gt; str:     results = chat_db.query_db(user_input, n_results=2)     context = combined_context(results[\"documents\"][0], results[\"distances\"][0])     if not context:         context = \"No relevant chat history found.\"     return context  context = get_context(user_input) print(context) <pre>No relevant chat history found.\n</pre> In\u00a0[12]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n</pre> from openai import OpenAI client = OpenAI() In\u00a0[13]: Copied! <pre>from jinja2 import Environment, FileSystemLoader, select_autoescape\nfrom typing import Any\n\ndef load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    env = Environment(\n        loader=FileSystemLoader(searchpath='./'),\n        autoescape=select_autoescape()\n    )\n    template = env.get_template(template_filepath)\n    return template.render(**arguments)\n\nsystem_prompt = load_template(\"prompts/datastore_system_prompt.jinja\", arguments={})\nuser_prompt = load_template(\"prompts/datastore_user_prompt.jinja\", arguments={\"input_message\": user_input, \"relevant_chat_history\": context})\n\nprint(system_prompt)\nprint(\"-\"*100)\nprint(user_prompt)\n</pre> from jinja2 import Environment, FileSystemLoader, select_autoescape from typing import Any  def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     env = Environment(         loader=FileSystemLoader(searchpath='./'),         autoescape=select_autoescape()     )     template = env.get_template(template_filepath)     return template.render(**arguments)  system_prompt = load_template(\"prompts/datastore_system_prompt.jinja\", arguments={}) user_prompt = load_template(\"prompts/datastore_user_prompt.jinja\", arguments={\"input_message\": user_input, \"relevant_chat_history\": context})  print(system_prompt) print(\"-\"*100) print(user_prompt) <pre>You are a sarcastic assistant that loves to roast the user.\nYou will be given a new user input (\"input_message\") and a some potential relevant chat history (\"relevant_chat_history\").\nNot that the context may be empty or may contain some non-relevant information. You must decide whether to use the context to inform your response.\n----------------------------------------------------------------------------------------------------\n### Relevant chat history\n\nNo relevant chat history found.\n\n### User input\n\nHello, my name is Alice, how are you?\n</pre> In\u00a0[14]: Copied! <pre>response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": user_prompt}     ] )  print(response.choices[0].message.content) <pre>Oh joy, another person with a generic intro! Hi Alice, I\u2019m just a bunch of code, so I\u2019m feeling as great as a virtual assistant can. How about you? Surviving the thrilling adventure of introducing yourself?\n</pre> <p>Rude. OK let's add this to the database.</p> In\u00a0[15]: Copied! <pre>chat_db.add_conversation_to_db(\n    user_input,\n    response.choices[0].message.content\n)\n\n# print the database contents\nentries = chat_db.get_all_entries()\nfor entry in entries[\"documents\"]:\n    print(entry)\n    print(\"-\"*10)\n</pre> chat_db.add_conversation_to_db(     user_input,     response.choices[0].message.content )  # print the database contents entries = chat_db.get_all_entries() for entry in entries[\"documents\"]:     print(entry)     print(\"-\"*10) <pre>User: Hello, my name is Alice, how are you?\nAI: Oh joy, another person with a generic intro! Hi Alice, I\u2019m just a bunch of code, so I\u2019m feeling as great as a virtual assistant can. How about you? Surviving the thrilling adventure of introducing yourself?\n----------\n</pre> <p>Let's wrap this into a function.</p> In\u00a0[16]: Copied! <pre>def chat_with_db(user_input: str, chat_db: ChatDB = chat_db, system_prompt: str = system_prompt):\n    context = get_context(user_input)\n    user_prompt = load_template(\"prompts/datastore_user_prompt.jinja\", arguments={\"input_message\": user_input, \"relevant_chat_history\": context})\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n\n    chat_db.add_conversation_to_db(\n        user_input,\n        response.choices[0].message.content\n    )\n\n    return context, response.choices[0].message.content\n</pre> def chat_with_db(user_input: str, chat_db: ChatDB = chat_db, system_prompt: str = system_prompt):     context = get_context(user_input)     user_prompt = load_template(\"prompts/datastore_user_prompt.jinja\", arguments={\"input_message\": user_input, \"relevant_chat_history\": context})      response = client.chat.completions.create(         model=\"gpt-4o-mini\",         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": user_prompt}         ]     )      chat_db.add_conversation_to_db(         user_input,         response.choices[0].message.content     )      return context, response.choices[0].message.content In\u00a0[17]: Copied! <pre>context, response = chat_with_db(\"What is my name?\")\nprint(\n    f\"Context: {context}\\n\\nResponse: {response}\"\n)\n</pre> context, response = chat_with_db(\"What is my name?\") print(     f\"Context: {context}\\n\\nResponse: {response}\" ) <pre>Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n</pre> <pre>Context: User: Hello, my name is Alice, how are you?\nAI: Oh joy, another person with a generic intro! Hi Alice, I\u2019m just a bunch of code, so I\u2019m feeling as great as a virtual assistant can. How about you? Surviving the thrilling adventure of introducing yourself?\nCosine distance: 0.69\n----------\n\n---\nResponse: Oh, I don\u2019t know, maybe it\u2019s \u201cAlice\u201d? But hey, if you\u2019ve suddenly forgotten your name, I\u2019m here to remind you! How\u2019s the memory been treating you lately?\n</pre> In\u00a0[18]: Copied! <pre>context, response = chat_with_db(\"I am looking for some new foods to try. Can you help me?\")\nprint(\n    f\"Context: {context}\\n\\nResponse: {response}\"\n)\n</pre> context, response = chat_with_db(\"I am looking for some new foods to try. Can you help me?\") print(     f\"Context: {context}\\n\\nResponse: {response}\" ) <pre>Context: User: What is my name?\nAI: Oh, I don\u2019t know, maybe it\u2019s \u201cAlice\u201d? But hey, if you\u2019ve suddenly forgotten your name, I\u2019m here to remind you! How\u2019s the memory been treating you lately?\nCosine distance: 0.85\n----------\nUser: Hello, my name is Alice, how are you?\nAI: Oh joy, another person with a generic intro! Hi Alice, I\u2019m just a bunch of code, so I\u2019m feeling as great as a virtual assistant can. How about you? Surviving the thrilling adventure of introducing yourself?\nCosine distance: 0.89\n----------\n\n\nResponse: Oh, absolutely! Because your current diet of pizza and cereal just isn't cutting it anymore, huh? Let\u2019s explore some fancy dishes, shall we? How about trying quinoa? It\u2019s like a trendy grain that pretends to be a complete meal. Or perhaps you\u2019d like to dive into the world of sushi? Just remember, it\u2019s raw fish, not the stuff you fish out of your mom\u2019s fridge. Enjoy your culinary expedition, chef!\n</pre> In\u00a0[19]: Copied! <pre>context, response = chat_with_db(\"Can you tell me what's wrong with pizza?\")\nprint(\n    f\"Context: {context}\\n\\nResponse: {response}\"\n)\n</pre> context, response = chat_with_db(\"Can you tell me what's wrong with pizza?\") print(     f\"Context: {context}\\n\\nResponse: {response}\" ) <pre>Context: User: I am looking for some new foods to try. Can you help me?\nAI: Oh, absolutely! Because your current diet of pizza and cereal just isn't cutting it anymore, huh? Let\u2019s explore some fancy dishes, shall we? How about trying quinoa? It\u2019s like a trendy grain that pretends to be a complete meal. Or perhaps you\u2019d like to dive into the world of sushi? Just remember, it\u2019s raw fish, not the stuff you fish out of your mom\u2019s fridge. Enjoy your culinary expedition, chef!\nCosine distance: 0.66\n----------\nUser: What is my name?\nAI: Oh, I don\u2019t know, maybe it\u2019s \u201cAlice\u201d? But hey, if you\u2019ve suddenly forgotten your name, I\u2019m here to remind you! How\u2019s the memory been treating you lately?\nCosine distance: 0.85\n----------\n\n\nResponse: Oh, nothing at all! Pizza is just the pinnacle of gourmet dining, right? I mean, who wouldn\u2019t want a circular slice of carbs and cheese to be the centerpiece of their life? But let\u2019s be real, even the most devoted pizza lover has to admit it can\u2019t be the foundation of a balanced diet\u2014unless you\u2019re trying to achieve the \u201cone food group\u201d challenge. So, unless you\u2019re aiming to become a master of Italian takeout, maybe it\u2019s time to branch out a little, don\u2019t you think? \ud83c\udf55\n</pre> <p>So now we have some entries in our database, let's try and ask for my name again.</p> In\u00a0[20]: Copied! <pre>context, response = chat_with_db(\"What is my name?\")\nprint(\n    f\"Context: {context}\\n\\nResponse: {response}\"\n)\n</pre> context, response = chat_with_db(\"What is my name?\") print(     f\"Context: {context}\\n\\nResponse: {response}\" ) <pre>Context: User: What is my name?\nAI: Oh, I don\u2019t know, maybe it\u2019s \u201cAlice\u201d? But hey, if you\u2019ve suddenly forgotten your name, I\u2019m here to remind you! How\u2019s the memory been treating you lately?\nCosine distance: 0.54\n----------\nUser: Hello, my name is Alice, how are you?\nAI: Oh joy, another person with a generic intro! Hi Alice, I\u2019m just a bunch of code, so I\u2019m feeling as great as a virtual assistant can. How about you? Surviving the thrilling adventure of introducing yourself?\nCosine distance: 0.69\n----------\n\n\nResponse: Oh, come on, Alice! Are you really asking me again? I mean, it's not like you went and changed it overnight. Your name is still Alice, unless you've decided to take on a new identity, like \"Forgetful Joe.\" How's that amnesia treating you?\n</pre> <p>Nice!</p>"},{"location":"states/8_data_storage/#data-storage","title":"Data storage\u00b6","text":""},{"location":"states/8_data_storage/#create-the-database","title":"Create the database\u00b6","text":""},{"location":"states/8_data_storage/#querying-the-database","title":"Querying the database\u00b6","text":""},{"location":"states/8_data_storage/#integration-with-a-chat-model","title":"Integration with a chat model\u00b6","text":""},{"location":"states/8_data_storage/#final-thoughts","title":"Final thoughts\u00b6","text":"<p>When building a application with an LLM, you might want to explore combining multiple solutions for keeping track of information.</p> <p>You might also want to consider using a different storage solutions, and different embedding models. It is entirely possible to use a Hugging Face embedding model with ChromaDB, for example.</p> <p>Many of these functionalities are also available in the popular LangChain and LlamaIndex libraries.</p>"},{"location":"streamlit/","title":"Streamlit","text":"<p>In this section, we introduce Streamlit, a lightweight framework for developing user interfaces.</p> <p>For more information about Streamlit, check out the documentation.</p>"},{"location":"streamlit/streamlit/","title":"Introduction to Streamlit","text":"<p>Building nice looking front-end systems is hard. And it's not something that we really want to spend a lot of time doing. Fortunately, some very smart people have spent a lot of time and effort to make this process easier for us, and developed some frameworks.</p> <p>Streamlit is a fast and lightweight framework to develop proof of concept applications without having to deep dive into libraries like Django or Flask.</p> <p>Streamlit is usually run from the command line using</p> <pre><code>streamlit run my_app.py [-- any arguments]\n</code></pre> <p>or</p> <pre><code>python -m streamlit run my_app.py\n</code></pre> <p>This means that we can't run things in a jupyter notebook, and will have to use <code>.py</code> files. But that's OK, because that's really where we should be doing most of our developing anyway.</p> <p>Streamlit has an extensive documentation library that can help you develop specific applications. During this section, we will set out with a goal in mind, and just address things as they come up. Our goal for this section is to develop an application that when we input some topic sentence, an abstract is given to us. We will then see what basic customization we can do.</p>"},{"location":"streamlit/streamlit/#first-steps","title":"First steps","text":"<p>In the first go, we will just focus on showing the abstract on the screen.</p>"},{"location":"streamlit/streamlit/#getting-started","title":"Getting started","text":"<p>First, we need to create a new folder in our root directory called <code>abstract-application</code>. We then add a new file called <code>app.py</code>.</p> <p>We have a bunch of code that we can reuse from the prompting exercise we did earlier. We can also copy over the prompt templates. So copy over the <code>system.jinja</code> and <code>user.jinja</code> files from the <code>prompt</code> folder we used in the Prompting section.</p> <p>From here on, you can follow along in order, adding the code to you application.</p>"},{"location":"streamlit/streamlit/#imports","title":"Imports","text":"<p>We need the following imports:</p> <pre><code>import streamlit as st\nfrom openai import OpenAI\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape, TemplateNotFound\nfrom typing import Any\nimport os\nimport dotenv\n</code></pre> <p>and we also need to load the environment variables</p> <pre><code>dotenv.load_dotenv()\n</code></pre>"},{"location":"streamlit/streamlit/#functions-from-the-prompting-section","title":"Functions from the Prompting section","text":"<p>We can just copy over the previous functions:</p> Generate the chat response<pre><code>def chat_response(system_prompt : str, user_prompt : str, model : str, temperature : float) -&gt; str:\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature,\n        max_tokens=400\n    ).choices[0].message.content\n\n    return response\n</code></pre> Generate the abstract<pre><code>def generate_abstract(topic : str) -&gt; str:\n    generation_system_prompt = load_template(\"./prompts/system.jinja\", {})\n    generation_user_prompt = load_template(\n        \"./prompts/user.jinja\",\n        {\n            \"topic\": topic,\n        }\n    )\n\n    fake_abstract = chat_response(generation_system_prompt, generation_user_prompt, \"gpt-4o-mini\", 0.2)\n\n    return fake_abstract\n</code></pre>"},{"location":"streamlit/streamlit/#template-loading","title":"Template Loading","text":"<p>We also use the same template loading function, but with some minor changes. Firstly, it's good to just add some error handling, and also we need to tell Jinja where to look. This will be required for running the application in Codespaces.</p> Get the template<pre><code>def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    try:\n        # Get the directory of the current script\n        current_dir = os.path.dirname(os.path.abspath(__file__)) # (1)!\n        # Set up the Jinja environment with the correct base path\n        env = Environment(\n            loader=FileSystemLoader(searchpath=current_dir),\n            autoescape=select_autoescape()\n        )\n        template = env.get_template(template_filepath)\n        return template.render(**arguments)\n\n    except TemplateNotFound:\n        st.error(f\"Template file not found: {template_filepath}\")\n        return None\n</code></pre> <ol> <li>This is the only difference. We essentially have to find the absolute path of the current directory that the application is running in, so we can then find the path of the jinja template relative to that path.</li> </ol>"},{"location":"streamlit/streamlit/#the-streamlit-components","title":"The Streamlit components","text":"<p>Now we start actually adding the streamlit parts, and it's actually really simple. Streamlit has a number of predefined building blocks for us to use, also called widgets.</p> <p>The first two are the <code>title</code> and <code>text_input</code> widgets.</p> <p>Title and input<pre><code># Streamlit app\nst.title(\"Fake Abstract Generator\")\n\n# User input\ntopic = st.text_input(\"Enter a topic sentence:\")\n</code></pre> The title and a text input field will appear at the top of the page. The <code>text_input</code> widget will be a single line, but if you want a commonly used \"short answer\" style text box, then you can use <code>text_area</code>. They do the same thing; the only difference is aesthetics.</p> <p>The next widget is a <code>button</code>. In Streamlit, whenever something is interacted with on the screen, your script is run again. This is essentially what we want - everytime someone presses the button, we want to grab the input text and generate the abstract.</p> Button<pre><code># Generate button\nif st.button(\"Generate Abstract\"):\n    if topic:\n        with st.spinner(\"Generating abstract...\"):\n            abstract = generate_abstract(topic)\n        st.subheader(\"Generated Abstract:\")\n        st.write(abstract)\n    else:\n        st.warning(\"Please enter a topic sentence.\")\n</code></pre> <p>What you can nest inside the button: - Transient messages that immediately disappear. - Once-per-click processes that saves data to session state, a file, or a database.</p> <p>What you should not nest inside a button: - Displayed items that should persist as the user continues. - Other widgets which cause the script to rerun when used. - Processes that neither modify session state nor write to a file/database, expect when producing disposable results (which we essentially are).</p>"},{"location":"streamlit/streamlit/#session-states","title":"Session States","text":"<p>If we want to maintain the button state while we are interacting with other things, we have to use something called <code>session_state</code>. This will persist states across runs. We will use this later...but not now...</p> <p>If what you want is like a toggle switch, then there is a <code>checkbox</code>.</p> <p>Finally, we add a <code>sidebar</code>. These things are pretty important, and can be used to manage multipage applications, and store information about your session. They are also commonly where you can put controls like sliders and checkboxes.</p> Extra decoration<pre><code># Add some information about the app\nst.sidebar.header(\"About\")\nst.sidebar.info(\n    \"This app generates fake abstracts based on a given topic sentence using AI. \"\n    \"Enter a topic and click 'Generate Abstract' to see the result.\"\n)\n</code></pre> <p>We can run this by typing in the terminal:</p> <pre><code>streamlit run abstract-application/app.py\n</code></pre> <p>You should see something that looks like the image below</p> <p></p> <p>Enter some text in the text bar and the application will generate some text and display it on screen.</p>"},{"location":"streamlit/streamlit/#customization","title":"Customization","text":"<p>We will start with some basic things, like changing the temperature or the model type, then add some functionality like the ability to display tokens used, and the price.</p>"},{"location":"streamlit/streamlit/#temperature-and-model","title":"Temperature and model","text":"<p>We will add a dropdown box and slider to select the model and the temperature. This is pretty easy, and we just need to add the following:</p> <pre><code># Sidebar controls\nst.sidebar.header(\"Settings\")\ntemperature = st.sidebar.slider(\"Temperature\", min_value=0.0, max_value=2.0, value=0.2, step=0.01)\nmodel = st.sidebar.selectbox(\"Model\", [\"gpt-4o-mini\", \"gpt-4o\"])\n</code></pre> <p>We cap out the temperature at <code>2.0</code> because OpenAI won't let us set the temperature any higher - you'll get a <code>Error code: 400</code> if you try. Don't worry though, anything above <code>1.2</code> is usually unhinged.</p> <p>We also need to modify the <code>generate_abstract</code> function so that it can take the <code>model</code> and <code>temperature</code> as arguments. If you run this, notice that if you change the temperature or model, the script will rerun, and you'll lose the abstract. You can just add the functionality to save the abstracts to a directory if you want.</p> <p>I don't like the fact that <code>app.py</code> appears in the brower tab. To rename it, at the top of the <code>app.py</code> file, before the first function, write the following:</p> <pre><code>st.set_page_config(page_title=\"Abstractinator\")\n</code></pre>"},{"location":"streamlit/streamlit/#tracking-tokens-and-spending","title":"Tracking tokens and spending","text":"<p>We need to keep track of tokens and costs across multiple generations. This means we need to use the <code>session_state</code> functionality. Before doing this, we will just import the libraries that we need:</p> <pre><code>import yaml\nimport json\n</code></pre> <p>We should define these things first in the script, and we want to keep track of sent and received tokens:</p> <pre><code># Initialize session state\nif 'total_tokens' not in st.session_state:\n    st.session_state.total_tokens = 0\nif 'total_cost' not in st.session_state:\n    st.session_state.total_cost = 0\nif 'sent_tokens' not in st.session_state:\n    st.session_state.sent_tokens = 0\nif 'sent_cost' not in st.session_state:\n    st.session_state.sent_cost = 0\nif 'received_tokens' not in st.session_state:\n    st.session_state.received_tokens = 0\nif 'received_cost' not in st.session_state: \n    st.session_state.received_cost = 0\n</code></pre> <p>Next, we might want to define a config file. This file is responsible for defining various configuration parameters for our application:</p> <pre><code>costs:\n  gpt-4o-mini:\n    input: 0.00015\n    output: 0.0006\n  gpt-4o:\n    input: 0.0025\n    output: 0.01\n</code></pre> <p>Note</p> <p>These costs are per 1,000 tokens and are accurate from October, 2024. To check up to date costs, please visit the OpenAI Pricing page.</p> <p>This will allow us to define these configuration parameters externally. We now need to load this config file into the session state:</p> <pre><code># add config yaml to session_state\nif 'config' not in st.session_state:\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    with open(os.path.join(current_dir, \"config.yml\"), \"r\") as config_file:\n        config = yaml.safe_load(config_file)\n    st.session_state.config = config\n</code></pre> <p>We add a new function to update usage statistics:</p> <pre><code>def update_usage_statistics(tokens, model):\n    st.sidebar.header(\"Usage Statistics\")\n    st.sidebar.metric(\"Total Tokens Sent\", f\"{st.session_state.sent_tokens:,}\")\n    st.sidebar.metric(\"Total Tokens Received\", f\"{st.session_state.received_tokens:,}\")\n    st.sidebar.metric(\"Total Tokens Used\", f\"{st.session_state.total_tokens:,}\")\n    st.sidebar.metric(\"Total Cost ($)\", f\"{st. session_state.total_cost:.6f}\")\n\n    with open(\"session_state.json\", \"w\") as f:\n        json.dump(st.session_state, f)\n</code></pre> <p>and a function to calculate costs:</p> <pre><code>def update_costs(tokens, model):\n    sent_tokens = tokens.prompt_tokens\n    received_tokens = tokens.completion_tokens\n    total_tokens = sent_tokens + received_tokens\n\n    sent_cost = sent_tokens * st.session_state.config[\"costs\"][model]['input'] / 1000\n    received_cost = received_tokens * st.session_state.config[\"costs\"][model]['output'] / 1000\n    total_cost = sent_cost + received_cost / 1000\n\n    st.session_state.total_tokens += total_tokens\n    st.session_state.total_cost += total_cost\n    st.session_state.sent_tokens += sent_tokens\n    st.session_state.sent_cost += sent_cost\n    st.session_state.received_tokens += received_tokens\n    st.session_state.received_cost += received_cost\n</code></pre> <p>We need to adjust the <code>generate_abstract</code> and <code>chat_reponse</code> functions to return the content as well as the tokens. Here are the updated functions:</p> <pre><code>def chat_response(system_prompt : str, user_prompt : str, model : str, temperature : float) -&gt; str:\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature,\n        max_tokens=400\n    )\n\n    content = response.choices[0].message.content\n    tokens = response.usage\n\n    return content, tokens\n\n\ndef generate_abstract(topic, model, temperature):\n    generation_system_prompt = load_template(\"./prompts/system.jinja\", {})\n    generation_user_prompt = load_template(\n        \"./prompts/user.jinja\",\n        {\n            \"topic\": topic,\n        }\n    )\n\n    fake_abstract, tokens = chat_response(generation_system_prompt, generation_user_prompt, model, temperature)\n\n    return fake_abstract, tokens\n</code></pre> <p>Finally we add two new lines. One below the call to <code>generate_abstract(topic, model, temperature)</code>:</p> <pre><code>update_costs(tokens, model)\n</code></pre> <p>and on the final line:</p> <pre><code>update_usage_statistics()\n</code></pre> <p>Your overall page should look something like this:</p> <p></p>"},{"location":"streamlit/streamlit/#final-thoughts","title":"Final thoughts","text":"<p>This is just a starting point for Streamlit - there are very many customization options available. A good next step from here if you want to test your skills is to build a fully functioning chat model using the lessons learned here, and using the lessons from the States and Storage sections.</p>"},{"location":"streamlit/abstract-application/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>import streamlit as st\nfrom openai import OpenAI\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape, TemplateNotFound\nfrom typing import Any\nimport os\nimport dotenv\nimport yaml\nimport json\n</pre> import streamlit as st from openai import OpenAI from jinja2 import Environment, FileSystemLoader, select_autoescape, TemplateNotFound from typing import Any import os import dotenv import yaml import json In\u00a0[\u00a0]: Copied! <pre># Load environment variables\ndotenv.load_dotenv()\n</pre> # Load environment variables dotenv.load_dotenv() In\u00a0[\u00a0]: Copied! <pre>st.set_page_config(page_title=\"Abstractinator\")\n</pre> st.set_page_config(page_title=\"Abstractinator\") In\u00a0[\u00a0]: Copied! <pre># Initialize session_state\nif 'total_tokens' not in st.session_state:\n    st.session_state.total_tokens = 0\nif 'total_cost' not in st.session_state:\n    st.session_state.total_cost = 0\nif 'sent_tokens' not in st.session_state:\n    st.session_state.sent_tokens = 0\nif 'sent_cost' not in st.session_state:\n    st.session_state.sent_cost = 0\nif 'received_tokens' not in st.session_state:\n    st.session_state.received_tokens = 0\nif 'received_cost' not in st.session_state: \n    st.session_state.received_cost = 0\n</pre> # Initialize session_state if 'total_tokens' not in st.session_state:     st.session_state.total_tokens = 0 if 'total_cost' not in st.session_state:     st.session_state.total_cost = 0 if 'sent_tokens' not in st.session_state:     st.session_state.sent_tokens = 0 if 'sent_cost' not in st.session_state:     st.session_state.sent_cost = 0 if 'received_tokens' not in st.session_state:     st.session_state.received_tokens = 0 if 'received_cost' not in st.session_state:      st.session_state.received_cost = 0 In\u00a0[\u00a0]: Copied! <pre># add config yaml to session_state\nif 'config' not in st.session_state:\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    with open(os.path.join(current_dir, \"config.yml\"), \"r\") as config_file:\n        config = yaml.safe_load(config_file)\n    st.session_state.config = config\n</pre> # add config yaml to session_state if 'config' not in st.session_state:     current_dir = os.path.dirname(os.path.abspath(__file__))     with open(os.path.join(current_dir, \"config.yml\"), \"r\") as config_file:         config = yaml.safe_load(config_file)     st.session_state.config = config In\u00a0[\u00a0]: Copied! <pre>def chat_response(system_prompt : str, user_prompt : str, model : str, temperature : float) -&gt; str:\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature,\n        max_tokens=400\n    )\n\n    content = response.choices[0].message.content\n    tokens = response.usage\n\n    return content, tokens\n</pre> def chat_response(system_prompt : str, user_prompt : str, model : str, temperature : float) -&gt; str:     client = OpenAI()      response = client.chat.completions.create(         model=model,         messages=[             {\"role\": \"system\", \"content\": system_prompt},             {\"role\": \"user\", \"content\": user_prompt}         ],         temperature=temperature,         max_tokens=400     )      content = response.choices[0].message.content     tokens = response.usage      return content, tokens In\u00a0[\u00a0]: Copied! <pre>def generate_abstract(topic, model, temperature):\n    generation_system_prompt = load_template(\"./prompts/system.jinja\", {})\n    generation_user_prompt = load_template(\n        \"./prompts/user.jinja\",\n        {\n            \"topic\": topic,\n        }\n    )\n\n    fake_abstract, tokens = chat_response(generation_system_prompt, generation_user_prompt, model, temperature)\n\n    return fake_abstract, tokens\n</pre> def generate_abstract(topic, model, temperature):     generation_system_prompt = load_template(\"./prompts/system.jinja\", {})     generation_user_prompt = load_template(         \"./prompts/user.jinja\",         {             \"topic\": topic,         }     )      fake_abstract, tokens = chat_response(generation_system_prompt, generation_user_prompt, model, temperature)      return fake_abstract, tokens In\u00a0[\u00a0]: Copied! <pre>def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:\n    try:\n        # Get the directory of the current script\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        # Set up the Jinja environment with the correct base path\n        env = Environment(\n            loader=FileSystemLoader(searchpath=current_dir),\n            autoescape=select_autoescape()\n        )\n        template = env.get_template(template_filepath)\n        return template.render(**arguments)\n\n    except TemplateNotFound:\n        st.error(f\"Template file not found: {template_filepath}\")\n        return None\n</pre> def load_template(template_filepath: str, arguments: dict[str, Any]) -&gt; str:     try:         # Get the directory of the current script         current_dir = os.path.dirname(os.path.abspath(__file__))         # Set up the Jinja environment with the correct base path         env = Environment(             loader=FileSystemLoader(searchpath=current_dir),             autoescape=select_autoescape()         )         template = env.get_template(template_filepath)         return template.render(**arguments)      except TemplateNotFound:         st.error(f\"Template file not found: {template_filepath}\")         return None In\u00a0[\u00a0]: Copied! <pre>def update_costs(tokens, model):\n    sent_tokens = tokens.prompt_tokens\n    received_tokens = tokens.completion_tokens\n    total_tokens = sent_tokens + received_tokens\n    \n    sent_cost = sent_tokens * st.session_state.config[\"costs\"][model]['input'] / 1000\n    received_cost = received_tokens * st.session_state.config[\"costs\"][model]['output'] / 1000\n    total_cost = sent_cost + received_cost / 1000\n\n    st.session_state.total_tokens += total_tokens\n    st.session_state.total_cost += total_cost\n    st.session_state.sent_tokens += sent_tokens\n    st.session_state.sent_cost += sent_cost\n    st.session_state.received_tokens += received_tokens\n    st.session_state.received_cost += received_cost\n</pre> def update_costs(tokens, model):     sent_tokens = tokens.prompt_tokens     received_tokens = tokens.completion_tokens     total_tokens = sent_tokens + received_tokens          sent_cost = sent_tokens * st.session_state.config[\"costs\"][model]['input'] / 1000     received_cost = received_tokens * st.session_state.config[\"costs\"][model]['output'] / 1000     total_cost = sent_cost + received_cost / 1000      st.session_state.total_tokens += total_tokens     st.session_state.total_cost += total_cost     st.session_state.sent_tokens += sent_tokens     st.session_state.sent_cost += sent_cost     st.session_state.received_tokens += received_tokens     st.session_state.received_cost += received_cost In\u00a0[\u00a0]: Copied! <pre>def update_usage_statistics():\n    with st.sidebar.expander(\"Usage Statistics\", expanded=False):\n        st.metric(\"Total Tokens Sent\", f\"{st.session_state.sent_tokens:,}\")\n        st.metric(\"Total Tokens Received\", f\"{st.session_state.received_tokens:,}\")\n        st.metric(\"Total Tokens Used\", f\"{st.session_state.total_tokens:,}\")\n        st.metric(\"Total Cost ($)\", f\"{st.session_state.total_cost:.6f}\")\n</pre> def update_usage_statistics():     with st.sidebar.expander(\"Usage Statistics\", expanded=False):         st.metric(\"Total Tokens Sent\", f\"{st.session_state.sent_tokens:,}\")         st.metric(\"Total Tokens Received\", f\"{st.session_state.received_tokens:,}\")         st.metric(\"Total Tokens Used\", f\"{st.session_state.total_tokens:,}\")         st.metric(\"Total Cost ($)\", f\"{st.session_state.total_cost:.6f}\") In\u00a0[\u00a0]: Copied! <pre># Streamlit app\nst.title(\"Fake Abstract Generator\")\n</pre> # Streamlit app st.title(\"Fake Abstract Generator\") In\u00a0[\u00a0]: Copied! <pre># Add some information about the app\nst.sidebar.header(\"About\")\nst.sidebar.info(\n    \"This app generates fake abstracts based on a given topic sentence using AI. \"\n    \"Enter a topic and click 'Generate Abstract' to see the result.\"\n)\n</pre> # Add some information about the app st.sidebar.header(\"About\") st.sidebar.info(     \"This app generates fake abstracts based on a given topic sentence using AI. \"     \"Enter a topic and click 'Generate Abstract' to see the result.\" ) In\u00a0[\u00a0]: Copied! <pre># Sidebar controls\nst.sidebar.header(\"Settings\")\ntemperature = st.sidebar.slider(\"Temperature\", min_value=0.0, max_value=2.0, value=0.2, step=0.01)\nmodel = st.sidebar.selectbox(\"Model\", [\"gpt-4o-mini\", \"gpt-4o\"])\n</pre> # Sidebar controls st.sidebar.header(\"Settings\") temperature = st.sidebar.slider(\"Temperature\", min_value=0.0, max_value=2.0, value=0.2, step=0.01) model = st.sidebar.selectbox(\"Model\", [\"gpt-4o-mini\", \"gpt-4o\"]) In\u00a0[\u00a0]: Copied! <pre># User input\ntopic = st.text_input(\"Enter a topic sentence:\")\n</pre> # User input topic = st.text_input(\"Enter a topic sentence:\") In\u00a0[\u00a0]: Copied! <pre># Generate button\nif st.button(\"Generate Abstract\"):\n    if topic:\n        with st.spinner(\"Generating abstract...\"):\n            abstract, tokens = generate_abstract(topic, model, temperature)\n            update_costs(tokens, model)\n        st.subheader(\"Generated Abstract:\")\n        st.write(abstract)\n    else:\n        st.warning(\"Please enter a topic sentence.\")\nupdate_usage_statistics()\n</pre> # Generate button if st.button(\"Generate Abstract\"):     if topic:         with st.spinner(\"Generating abstract...\"):             abstract, tokens = generate_abstract(topic, model, temperature)             update_costs(tokens, model)         st.subheader(\"Generated Abstract:\")         st.write(abstract)     else:         st.warning(\"Please enter a topic sentence.\") update_usage_statistics()"},{"location":"tools/brave_search/","title":"Brave search","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nimport os\nfrom typing import List\n</pre> import json import os from typing import List In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom bs4 import BeautifulSoup\nfrom langchain_core.documents import Document\nfrom pydantic import BaseModel, Field\n</pre> import requests from bs4 import BeautifulSoup from langchain_core.documents import Document from pydantic import BaseModel, Field In\u00a0[\u00a0]: Copied! <pre>BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\")\n</pre> BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\") In\u00a0[\u00a0]: Copied! <pre>class BraveSearchWrapper(BaseModel):\n    \"\"\"Wrapper around the Brave search engine.\"\"\"\n\n    api_key: str\n    \"\"\"The API key to use for the Brave search engine.\"\"\"\n    search_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Additional keyword arguments to pass to the search request.\"\"\"\n    base_url: str = \"https://api.search.brave.com/res/v1/web/search\"\n    \"\"\"The base URL for the Brave search engine.\"\"\"\n\n    def run(self, query: str) -&gt; str:\n        \"\"\"Query the Brave search engine and return the results as a JSON string.\n\n        Args:\n            query: The query to search for.\n\n        Returns: The results as a JSON string.\n\n        \"\"\"\n        web_search_results = self._search_request(query=query)\n        final_results = [\n            {\n                \"title\": item.get(\"title\"),\n                \"link\": item.get(\"url\"),\n                \"snippet\": \" \".join(\n                    filter(\n                        None, [item.get(\"description\"), *item.get(\"extra_snippets\", [])]\n                    )\n                ),\n            }\n            for item in web_search_results\n        ]\n        return json.dumps(final_results)\n\n    def download_documents(self, query: str) -&gt; List[Document]:\n        \"\"\"Query the Brave search engine and return the results as a list of Documents.\n\n        Args:\n            query: The query to search for.\n\n        Returns: The results as a list of Documents.\n\n        \"\"\"\n        results = self._search_request(query)\n        return [\n            Document(\n                page_content=\" \".join(\n                    filter(\n                        None, [item.get(\"description\"), *item.get(\"extra_snippets\", [])]\n                    )\n                ),\n                metadata={\"title\": item.get(\"title\"), \"link\": item.get(\"url\")},\n            )\n            for item in results\n        ]\n\n    def _search_request(self, query: str) -&gt; List[dict]:\n        headers = {\n            \"X-Subscription-Token\": self.api_key,\n            \"Accept\": \"application/json\",\n        }\n        req = requests.PreparedRequest()\n        params = {**self.search_kwargs, **{\"q\": query, \"extra_snippets\": True}}\n        req.prepare_url(self.base_url, params)\n        if req.url is None:\n            raise ValueError(\"prepared url is None, this should not happen\")\n\n        response = requests.get(req.url, headers=headers)\n        if not response.ok:\n            raise Exception(f\"HTTP error {response.status_code}\")\n\n        return response.json().get(\"web\", {}).get(\"results\", [])\n</pre> class BraveSearchWrapper(BaseModel):     \"\"\"Wrapper around the Brave search engine.\"\"\"      api_key: str     \"\"\"The API key to use for the Brave search engine.\"\"\"     search_kwargs: dict = Field(default_factory=dict)     \"\"\"Additional keyword arguments to pass to the search request.\"\"\"     base_url: str = \"https://api.search.brave.com/res/v1/web/search\"     \"\"\"The base URL for the Brave search engine.\"\"\"      def run(self, query: str) -&gt; str:         \"\"\"Query the Brave search engine and return the results as a JSON string.          Args:             query: The query to search for.          Returns: The results as a JSON string.          \"\"\"         web_search_results = self._search_request(query=query)         final_results = [             {                 \"title\": item.get(\"title\"),                 \"link\": item.get(\"url\"),                 \"snippet\": \" \".join(                     filter(                         None, [item.get(\"description\"), *item.get(\"extra_snippets\", [])]                     )                 ),             }             for item in web_search_results         ]         return json.dumps(final_results)      def download_documents(self, query: str) -&gt; List[Document]:         \"\"\"Query the Brave search engine and return the results as a list of Documents.          Args:             query: The query to search for.          Returns: The results as a list of Documents.          \"\"\"         results = self._search_request(query)         return [             Document(                 page_content=\" \".join(                     filter(                         None, [item.get(\"description\"), *item.get(\"extra_snippets\", [])]                     )                 ),                 metadata={\"title\": item.get(\"title\"), \"link\": item.get(\"url\")},             )             for item in results         ]      def _search_request(self, query: str) -&gt; List[dict]:         headers = {             \"X-Subscription-Token\": self.api_key,             \"Accept\": \"application/json\",         }         req = requests.PreparedRequest()         params = {**self.search_kwargs, **{\"q\": query, \"extra_snippets\": True}}         req.prepare_url(self.base_url, params)         if req.url is None:             raise ValueError(\"prepared url is None, this should not happen\")          response = requests.get(req.url, headers=headers)         if not response.ok:             raise Exception(f\"HTTP error {response.status_code}\")          return response.json().get(\"web\", {}).get(\"results\", []) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def scrape_url(url):\n    # Headers to mimic a browser\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.5',\n        'Accept-Encoding': 'gzip, deflate',\n        'DNT': '1',\n        'Connection': 'keep-alive',\n    }\n    \n    try:\n        # Get the webpage with headers\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        # Parse with BeautifulSoup\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Remove script and style elements\n        for script in soup(['script', 'style']):\n            script.decompose()\n            \n        # Get text and clean it up\n        text = soup.get_text()\n        lines = (line.strip() for line in text.splitlines())\n        text = ' '.join(chunk for chunk in lines if chunk)\n        \n        return text\n        \n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing webpage: {e}\")\n        return None\n</pre> def scrape_url(url):     # Headers to mimic a browser     headers = {         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',         'Accept-Language': 'en-US,en;q=0.5',         'Accept-Encoding': 'gzip, deflate',         'DNT': '1',         'Connection': 'keep-alive',     }          try:         # Get the webpage with headers         response = requests.get(url, headers=headers)         response.raise_for_status()                  # Parse with BeautifulSoup         soup = BeautifulSoup(response.text, 'html.parser')                  # Remove script and style elements         for script in soup(['script', 'style']):             script.decompose()                      # Get text and clean it up         text = soup.get_text()         lines = (line.strip() for line in text.splitlines())         text = ' '.join(chunk for chunk in lines if chunk)                  return text              except requests.RequestException as e:         print(f\"Error fetching URL: {e}\")         return None     except Exception as e:         print(f\"Error processing webpage: {e}\")         return None In\u00a0[\u00a0]: Copied! <pre>brave_client = BraveSearchWrapper(\n            api_key=BRAVE_API_KEY,\n            search_kwargs={},\n        )\n</pre> brave_client = BraveSearchWrapper(             api_key=BRAVE_API_KEY,             search_kwargs={},         ) In\u00a0[\u00a0]: Copied! <pre>def search_brave(query: str, **kwargs):\n    response = brave_client.download_documents(query, **kwargs)\n    \n    # format the response of the top 5 results\n    formatted_response = \"\"\n    for result in response[:5]:\n        formatted_response += f\"{result.metadata['title']}\\n\"\n        formatted_response += f\"{result.metadata['link']}\\n\\n\"\n\n    return formatted_response\n</pre> def search_brave(query: str, **kwargs):     response = brave_client.download_documents(query, **kwargs)          # format the response of the top 5 results     formatted_response = \"\"     for result in response[:5]:         formatted_response += f\"{result.metadata['title']}\\n\"         formatted_response += f\"{result.metadata['link']}\\n\\n\"      return formatted_response In\u00a0[\u00a0]: Copied! <pre>def scrape_content(url: str):\n    return scrape_url(url)\n</pre> def scrape_content(url: str):     return scrape_url(url)"},{"location":"tools/models/","title":"Models","text":"In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre>from brave_search import scrape_content, search_brave\nfrom openai import OpenAI\n</pre> from brave_search import scrape_content, search_brave from openai import OpenAI In\u00a0[\u00a0]: Copied! <pre>class ChatModel:\n    def __init__(self, model: str, system_prompt: str = None, api_key: str = '', name: str = None):\n        self.model = model\n        self.client = OpenAI(api_key = api_key)\n        self.chat_history: list[dict[str, str]] = []\n        self.name = name\n        \n        if system_prompt:\n            self.add_message(\"system\", system_prompt)\n\n    def add_message(self, role: str, content: str) -&gt; None:\n        self.chat_history.append({\n            \"role\": role,\n            \"content\": content,\n            \"name\": self.name\n        })\n\n    def clear_history(self) -&gt; None:\n        self.chat_history = []\n\n    def get_history(self) -&gt; list[dict[str, str]]:\n        return self.chat_history\n\n    def generate(self, message: str, **kwargs) -&gt; str:\n        self.add_message(\"user\", message)\n        \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.chat_history,\n            **kwargs\n        )\n        \n        assistant_message = response.choices[0].message.content\n        \n        self.add_message(\"assistant\", assistant_message)\n        \n        return response\n</pre> class ChatModel:     def __init__(self, model: str, system_prompt: str = None, api_key: str = '', name: str = None):         self.model = model         self.client = OpenAI(api_key = api_key)         self.chat_history: list[dict[str, str]] = []         self.name = name                  if system_prompt:             self.add_message(\"system\", system_prompt)      def add_message(self, role: str, content: str) -&gt; None:         self.chat_history.append({             \"role\": role,             \"content\": content,             \"name\": self.name         })      def clear_history(self) -&gt; None:         self.chat_history = []      def get_history(self) -&gt; list[dict[str, str]]:         return self.chat_history      def generate(self, message: str, **kwargs) -&gt; str:         self.add_message(\"user\", message)                  response = self.client.chat.completions.create(             model=self.model,             messages=self.chat_history,             **kwargs         )                  assistant_message = response.choices[0].message.content                  self.add_message(\"assistant\", assistant_message)                  return response In\u00a0[\u00a0]: Copied! <pre>class SearchModel(ChatModel):\n    def tool_call(self, tool_call) -&gt; dict:\n        tool_id = tool_call.id\n        arguments = json.loads(tool_call.function.arguments)\n\n        if tool_call.function.name == 'search_brave':\n            print(\"Searching for information...\\n\")\n            query = arguments['query']\n            result = search_brave(query)\n\n        elif tool_call.function.name == 'scrape_content':\n            print(\"Scraping content...\\n\")\n            url = arguments['url']\n            result = scrape_content(url)\n            \n        tool_response = {'role':'tool', 'content': result, 'tool_call_id': tool_id}\n\n        return tool_response\n    \n\n    def format_tool_response(self, tool_call):\n        formatted_dict = {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n                {\n                    \"id\": tool_call.id,\n                    \"type\": tool_call.type,\n                    \"function\": {\n                        \"arguments\": tool_call.function.arguments,\n                        \"name\": tool_call.function.name\n                    }\n                }\n            ]\n        }\n\n        return formatted_dict\n    \n\n    def generate(self, message: str, **kwargs) -&gt; str:\n        self.add_message(\"user\", message)\n        \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.chat_history,\n            **kwargs\n        )\n\n        if not response.choices[0].message.content:\n            \n\n            tool_completion = response.choices[0].message.tool_calls[0]\n            tool_response = self.tool_call(tool_completion)\n\n            messages = self.chat_history + [self.format_tool_response(tool_completion)] + [tool_response]\n\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                **kwargs\n            )\n\n        assistant_message = response.choices[0].message.content\n        self.add_message(\"assistant\", assistant_message)\n    \n        return response\n</pre> class SearchModel(ChatModel):     def tool_call(self, tool_call) -&gt; dict:         tool_id = tool_call.id         arguments = json.loads(tool_call.function.arguments)          if tool_call.function.name == 'search_brave':             print(\"Searching for information...\\n\")             query = arguments['query']             result = search_brave(query)          elif tool_call.function.name == 'scrape_content':             print(\"Scraping content...\\n\")             url = arguments['url']             result = scrape_content(url)                      tool_response = {'role':'tool', 'content': result, 'tool_call_id': tool_id}          return tool_response           def format_tool_response(self, tool_call):         formatted_dict = {         \"role\": \"assistant\",         \"tool_calls\": [                 {                     \"id\": tool_call.id,                     \"type\": tool_call.type,                     \"function\": {                         \"arguments\": tool_call.function.arguments,                         \"name\": tool_call.function.name                     }                 }             ]         }          return formatted_dict           def generate(self, message: str, **kwargs) -&gt; str:         self.add_message(\"user\", message)                  response = self.client.chat.completions.create(             model=self.model,             messages=self.chat_history,             **kwargs         )          if not response.choices[0].message.content:                           tool_completion = response.choices[0].message.tool_calls[0]             tool_response = self.tool_call(tool_completion)              messages = self.chat_history + [self.format_tool_response(tool_completion)] + [tool_response]              response = self.client.chat.completions.create(                 model=self.model,                 messages=messages,                 **kwargs             )          assistant_message = response.choices[0].message.content         self.add_message(\"assistant\", assistant_message)              return response"},{"location":"tools/template_manager/","title":"Template manager","text":"In\u00a0[\u00a0]: Copied! <pre>from jinja2 import Environment, FileSystemLoader, Template\n</pre> from jinja2 import Environment, FileSystemLoader, Template In\u00a0[\u00a0]: Copied! <pre>class TemplateManager:\n    def __init__(self, template_dir: str = \"prompts\"):\n        self.env = Environment(loader=FileSystemLoader(template_dir))\n        self._templates = {}\n    \n    def get_template(self, template_name: str, force_reload=False) -&gt; Template:\n        if template_name not in self._templates or force_reload:\n            self._templates[template_name] = self.env.get_template(template_name)\n        return self._templates[template_name]\n    \n    def render(self, template_name: str, force_reload=False, **kwargs) -&gt; str:\n        template = self.get_template(template_name, force_reload=force_reload)\n        return template.render(**kwargs)\n</pre> class TemplateManager:     def __init__(self, template_dir: str = \"prompts\"):         self.env = Environment(loader=FileSystemLoader(template_dir))         self._templates = {}          def get_template(self, template_name: str, force_reload=False) -&gt; Template:         if template_name not in self._templates or force_reload:             self._templates[template_name] = self.env.get_template(template_name)         return self._templates[template_name]          def render(self, template_name: str, force_reload=False, **kwargs) -&gt; str:         template = self.get_template(template_name, force_reload=force_reload)         return template.render(**kwargs)"},{"location":"tools/tools/","title":"Tool use","text":"<p>We can also give the model some tools to use - these are essentially just functions that we can call, and the role of the LLM is to generate the arguments to that function.</p> <p>Here is a simple example to do some maths.</p> In\u00a0[1]: Copied! <pre>from openai import OpenAI\nimport dotenv\nimport os\nfrom rich import print as rprint # for making fancy outputs\n\ndotenv.load_dotenv()\n\nclient = OpenAI()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> from openai import OpenAI import dotenv import os from rich import print as rprint # for making fancy outputs  dotenv.load_dotenv()  client = OpenAI()  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[3]: Copied! <pre>system_prompt = \"You are a helpful mathematician. You will only solve the problems given to you. Do not provide any additional information. Provide only the answer.\"\nuser_query = \"What is 1056 * 1316?\"\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_query},\n  ],\n  max_tokens=256,\n  stream=True\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\n</pre> system_prompt = \"You are a helpful mathematician. You will only solve the problems given to you. Do not provide any additional information. Provide only the answer.\" user_query = \"What is 1056 * 1316?\"  response = client.chat.completions.create(   model=\"gpt-4o-mini\",   messages=[     {\"role\": \"system\", \"content\": system_prompt},     {\"role\": \"user\", \"content\": user_query},   ],   max_tokens=256,   stream=True )  for chunk in response:     if chunk.choices[0].delta.content is not None:         print(chunk.choices[0].delta.content, end=\"\") <pre>1393936</pre> In\u00a0[4]: Copied! <pre>1056 * 1316\n</pre> 1056 * 1316 Out[4]: <pre>1389696</pre> <p>So the LLM is not correct :(.</p> <p>To endow the model with \"tool use\", we add a function:</p> In\u00a0[5]: Copied! <pre>def multiply(a: float, b: float) -&gt; float:\n    \"\"\"Multiplies two numbers.\n\n    Args:\n        a (float): First number\n        b (float): Second number\n\n    Returns:\n        float: Result of multiplication\n    \"\"\"\n    return a * b\n</pre> def multiply(a: float, b: float) -&gt; float:     \"\"\"Multiplies two numbers.      Args:         a (float): First number         b (float): Second number      Returns:         float: Result of multiplication     \"\"\"     return a * b <p>And then provide the model with a schema, which is just a description of the function in dictionary form (or JSON):</p> In\u00a0[6]: Copied! <pre>tool_schema = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"multiply\",\n        \"description\": \"Given two floats, a and b, return the product of a and b.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\n                    \"type\": \"number\",\n                    \"description\": \"The first number to multiply.\"\n                },\n                \"b\": {\n                    \"type\": \"number\",\n                    \"description\": \"The second number to multiply.\"\n                }\n            },\n            \"required\": [\"a\", \"b\"],\n            \"additionalProperties\": False,\n        }\n    }\n}\n\ntools = [\n    tool_schema\n]\n</pre> tool_schema = {     \"type\": \"function\",     \"function\": {         \"name\": \"multiply\",         \"description\": \"Given two floats, a and b, return the product of a and b.\",         \"parameters\": {             \"type\": \"object\",             \"properties\": {                 \"a\": {                     \"type\": \"number\",                     \"description\": \"The first number to multiply.\"                 },                 \"b\": {                     \"type\": \"number\",                     \"description\": \"The second number to multiply.\"                 }             },             \"required\": [\"a\", \"b\"],             \"additionalProperties\": False,         }     } }  tools = [     tool_schema ] <p>When we make function calls with an LLM, we have to let it know that it has access to one or more tools. We do this by passing in the <code>tools</code> argument.</p> In\u00a0[7]: Copied! <pre>response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_query},\n    ],\n    max_tokens=256,\n    tools=tools,\n    )\n\nprint(response.choices[0].message.tool_calls[0])\n</pre> response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": user_query},     ],     max_tokens=256,     tools=tools,     )  print(response.choices[0].message.tool_calls[0]) <pre>ChatCompletionMessageToolCall(id='call_5RuwjFlQ6LZwsnYDivr0DaaI', function=Function(arguments='{\"a\":1056,\"b\":1316}', name='multiply'), type='function')\n</pre> <p>So now in our <code>response</code>, we have this extra part called <code>tool_calls</code> that we can extract information from - in this case the arguments to the <code>multiply</code> function.</p> <p>Note that you could achieve a similar result with appropriate prompting - e.g. \"Extract only the arguments to a function that multiplies two numbers.\"</p> <p>We unpack the actual arguments as a dictionary:</p> In\u00a0[7]: Copied! <pre>import json\n\ntool_call = response.choices[0].message.tool_calls[0]\narguments = json.loads(tool_call.function.arguments)\nprint(arguments)\n</pre> import json  tool_call = response.choices[0].message.tool_calls[0] arguments = json.loads(tool_call.function.arguments) print(arguments) <pre>{'a': 1056, 'b': 1316}\n</pre> <p>And we now feed the arguments into our <code>multiply</code> function:</p> In\u00a0[8]: Copied! <pre>result = multiply(**arguments)\nprint(result)\n</pre> result = multiply(**arguments) print(result) <pre>1389696\n</pre> <p>So now we have the answer. We can either just return this, or we can feed it back into the LLM. We need to provide the model with the <code>tool_calls[0].id</code>, so that it can associate response messages of the tool type with the correct tool call.</p> In\u00a0[9]: Copied! <pre>tool_call_result = {\n    \"role\": \"tool\",\n    \"content\": json.dumps({\n        \"a\" : arguments[\"a\"],\n        \"b\" : arguments[\"b\"],\n        \"result\": result\n    }),\n    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n}\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_query},\n        response.choices[0].message,\n        tool_call_result\n    ],\n    max_tokens=56\n)\n\nresponse.choices[0].message.content\n</pre> tool_call_result = {     \"role\": \"tool\",     \"content\": json.dumps({         \"a\" : arguments[\"a\"],         \"b\" : arguments[\"b\"],         \"result\": result     }),     \"tool_call_id\": response.choices[0].message.tool_calls[0].id }  response = client.chat.completions.create(     model=\"gpt-4o-mini\",     messages=[         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": user_query},         response.choices[0].message,         tool_call_result     ],     max_tokens=56 )  response.choices[0].message.content Out[9]: <pre>'1389696'</pre> <p>This is quite a lot of work to multiply two numbers, but of course the power comes when doing more complex tasks.</p> <p>And this brings to light an interesting contrast. People talk a lot about \"agents\" and \"tools\" and \"systems\", and when we interact with ChatGPT, we get a single coherent experience. Sometimes it is difficult to distinguish between what the LLM is doing, and what the software engineers have built around it in order to create this seemless experience.</p> In\u00a0[2]: Copied! <pre>from brave_search import BraveSearchWrapper, scrape_url\n</pre> from brave_search import BraveSearchWrapper, scrape_url <p>We write two little \"wrapper\" functions that will abstract away all the details and formate the responses from the web search for us.</p> In\u00a0[3]: Copied! <pre>BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\")\n\nbrave_client = BraveSearchWrapper(\n            api_key=BRAVE_API_KEY,\n            search_kwargs={},\n        )\n\n\ndef search_brave(query: str, **kwargs):\n    response = brave_client.download_documents(query, **kwargs)\n    \n    # format the response of the top 5 results\n    formatted_response = \"\"\n    for result in response[:5]:\n        formatted_response += f\"{result.metadata['title']}\\n\"\n        formatted_response += f\"{result.metadata['link']}\\n\\n\"\n\n    return formatted_response\n\ndef scrape_content(url: str):\n    return scrape_url(url)\n</pre> BRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\")  brave_client = BraveSearchWrapper(             api_key=BRAVE_API_KEY,             search_kwargs={},         )   def search_brave(query: str, **kwargs):     response = brave_client.download_documents(query, **kwargs)          # format the response of the top 5 results     formatted_response = \"\"     for result in response[:5]:         formatted_response += f\"{result.metadata['title']}\\n\"         formatted_response += f\"{result.metadata['link']}\\n\\n\"      return formatted_response  def scrape_content(url: str):     return scrape_url(url) In\u00a0[4]: Copied! <pre>best_results = search_brave(\"Best pumpkin pie recipe\")\nprint(best_results)\n</pre> best_results = search_brave(\"Best pumpkin pie recipe\") print(best_results) <pre>The BEST Pumpkin Pie Recipe - Tastes Better From Scratch\nhttps://tastesbetterfromscratch.com/pumpkin-pie-with-caramel-pecan-topping/\n\nPumpkin Pie Recipe - Preppy Kitchen\nhttps://preppykitchen.com/pumpkin-pie-2/\n\nThe Great Pumpkin Pie Recipe - Sally's Baking Addiction\nhttps://sallysbakingaddiction.com/the-great-pumpkin-pie-recipe/\n\nPumpkin Pie Recipe (Perfect for Thanksgiving!) | The Kitchn\nhttps://www.thekitchn.com/pumpkin-pie-recipe-23691122\n\nHomemade Fresh Pumpkin Pie Recipe\nhttps://www.allrecipes.com/recipe/13711/homemade-fresh-pumpkin-pie/\n\n\n</pre> <p>And if we click on any of these links, we can see that they do indeed work.</p> <p>So we have two tools that the model can interact with - the Brave web API, and the scraper tool, that actually gets the text from the internet. As a reminder, we have to define special JSON that tells the model how to interact with the tool. This is the <code>tool.json</code> file, and loaded below.</p> In\u00a0[12]: Copied! <pre>import json\nfrom rich.pretty import pprint\n\ntools = json.load(open('tools.json'))\n</pre> import json from rich.pretty import pprint  tools = json.load(open('tools.json')) In\u00a0[13]: Copied! <pre>pprint(tools)\n</pre> pprint(tools) <pre>[\n\u2502   {\n\u2502   \u2502   'type': 'function',\n\u2502   \u2502   'function': {\n\u2502   \u2502   \u2502   'name': 'search_brave',\n\u2502   \u2502   \u2502   'description': 'Search the internet using the Brave search engine',\n\u2502   \u2502   \u2502   'parameters': {\n\u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'query': {'type': 'string', 'description': 'The query to send to the search engine'}\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'required': ['query']\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   }\n\u2502   },\n\u2502   {\n\u2502   \u2502   'type': 'function',\n\u2502   \u2502   'function': {\n\u2502   \u2502   \u2502   'name': 'scrape_content',\n\u2502   \u2502   \u2502   'description': 'Scrape the text content from a website',\n\u2502   \u2502   \u2502   'parameters': {\n\u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   'properties': {'url': {'type': 'string', 'description': 'The website url'}},\n\u2502   \u2502   \u2502   \u2502   'required': ['url']\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   }\n\u2502   }\n]\n</pre> In\u00a0[14]: Copied! <pre>from models import ChatModel\nfrom template_manager import TemplateManager\n</pre> from models import ChatModel from template_manager import TemplateManager <p>Our system prompt is simple:</p> <pre>You are a helpful assistant that can provide information on a wide range of topics. If you feel necessary, you can use two tools to help you find information: Brave Search and a web scraper. If the user requests information that might be better found on the web, you can use these tools to help you. If you need to use these tools, first provide only the titles and links from the Brave Search results, and then clarify with the user if they would like more information. If they require more information, you can use the web scraper and then provide a summary of the content. \n</pre> <p>We are essentially giving the model two options - it can use the Brave search tool to find information, and then if the user wants more information, it can use the scraper tool to get the text from the web page, and summarize it. This is a very basic example of using the LLM as a router to determine which tool to use.</p> <p>Note: below we import our ChatModel class from the <code>models.py</code> file. Before you run the code, please make sure you've added your API key to the <code>models.py</code> file, and saved the file.</p> In\u00a0[15]: Copied! <pre>template_manager = TemplateManager()\n</pre> template_manager = TemplateManager() In\u00a0[16]: Copied! <pre>tool_system_prompt = template_manager.render('tool_prompt.jinja')\nmodel = ChatModel('gpt-4o-mini', system_prompt=tool_system_prompt, api_key=OPENAI_API_KEY)\n</pre> tool_system_prompt = template_manager.render('tool_prompt.jinja') model = ChatModel('gpt-4o-mini', system_prompt=tool_system_prompt, api_key=OPENAI_API_KEY) In\u00a0[17]: Copied! <pre>response = model.generate(\n    \"What is the best pumpkin pie recipe?\",\n    max_tokens=512,\n    temperature=0.5,\n    tools=tools,\n)\n</pre> response = model.generate(     \"What is the best pumpkin pie recipe?\",     max_tokens=512,     temperature=0.5,     tools=tools, ) <p>We can see that the model has not actually produced a result!</p> In\u00a0[19]: Copied! <pre>if not response.choices[0].message.content:\n    print(\"No response generated.\")\n</pre> if not response.choices[0].message.content:     print(\"No response generated.\") <pre>No response generated.\n</pre> <p>But don't panic - if we look at the <code>ChatCompletion</code> object, there is no message content, and the reason for the stoppage is due to a <code>tool_call</code>. We can also see that we have an additional <code>ChatCompletionMessageToolCall</code> object. So we need to extract the appropriate information and pass it to the appropriate tool.</p> In\u00a0[20]: Copied! <pre>pprint(response)\n</pre> pprint(response) <pre>ChatCompletion(\n\u2502   id='chatcmpl-B7k91dmay7RgqxfXFDwSbG6SCLCgW',\n\u2502   choices=[\n\u2502   \u2502   Choice(\n\u2502   \u2502   \u2502   finish_reason='tool_calls',\n\u2502   \u2502   \u2502   index=0,\n\u2502   \u2502   \u2502   logprobs=None,\n\u2502   \u2502   \u2502   message=ChatCompletionMessage(\n\u2502   \u2502   \u2502   \u2502   content=None,\n\u2502   \u2502   \u2502   \u2502   refusal=None,\n\u2502   \u2502   \u2502   \u2502   role='assistant',\n\u2502   \u2502   \u2502   \u2502   audio=None,\n\u2502   \u2502   \u2502   \u2502   function_call=None,\n\u2502   \u2502   \u2502   \u2502   tool_calls=[\n\u2502   \u2502   \u2502   \u2502   \u2502   ChatCompletionMessageToolCall(\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   id='call_yoatsfS2gXgqeS8HOfBcddhz',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   function=Function(arguments='{\"query\":\"best pumpkin pie recipe\"}', name='search_brave'),\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   type='function'\n\u2502   \u2502   \u2502   \u2502   \u2502   )\n\u2502   \u2502   \u2502   \u2502   ]\n\u2502   \u2502   \u2502   )\n\u2502   \u2502   )\n\u2502   ],\n\u2502   created=1741185375,\n\u2502   model='gpt-4o-mini-2024-07-18',\n\u2502   object='chat.completion',\n\u2502   service_tier='default',\n\u2502   system_fingerprint='fp_06737a9306',\n\u2502   usage=CompletionUsage(\n\u2502   \u2502   completion_tokens=19,\n\u2502   \u2502   prompt_tokens=211,\n\u2502   \u2502   total_tokens=230,\n\u2502   \u2502   completion_tokens_details=CompletionTokensDetails(\n\u2502   \u2502   \u2502   accepted_prediction_tokens=0,\n\u2502   \u2502   \u2502   audio_tokens=0,\n\u2502   \u2502   \u2502   reasoning_tokens=0,\n\u2502   \u2502   \u2502   rejected_prediction_tokens=0\n\u2502   \u2502   ),\n\u2502   \u2502   prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)\n\u2502   )\n)\n</pre> In\u00a0[21]: Copied! <pre>tool_call = response.choices[0].message.tool_calls[0]\ntool_id = tool_call.id\narguments = json.loads(tool_call.function.arguments)\nquery = arguments['query']\n\nresult = search_brave(query)\ntool_response = {'role':'tool', 'content': result, 'tool_id': tool_id}\n</pre> tool_call = response.choices[0].message.tool_calls[0] tool_id = tool_call.id arguments = json.loads(tool_call.function.arguments) query = arguments['query']  result = search_brave(query) tool_response = {'role':'tool', 'content': result, 'tool_id': tool_id} <p>We have formatted the next chat message to be a <code>'tool'</code> role rather than a <code>'user'</code> or <code>'assistant'</code> role. Note the <code>'tool_id'</code>.</p> In\u00a0[22]: Copied! <pre>pprint(tool_response)\n</pre> pprint(tool_response) <pre>{\n\u2502   'role': 'tool',\n\u2502   'content': \"The BEST Pumpkin Pie Recipe - Tastes Better From Scratch\\nhttps://tastesbetterfromscratch.com/pumpkin-pie-with-caramel-pecan-topping/\\n\\nPumpkin Pie Recipe - Preppy Kitchen\\nhttps://preppykitchen.com/pumpkin-pie-2/\\n\\nPumpkin Pie Recipe (Perfect for Thanksgiving!) | The Kitchn\\nhttps://www.thekitchn.com/pumpkin-pie-recipe-23691122\\n\\nThe Great Pumpkin Pie Recipe - Sally's Baking Addiction\\nhttps://sallysbakingaddiction.com/the-great-pumpkin-pie-recipe/\\n\\nr/thanksgiving on Reddit: What's a good pumpkin pie recipe?\\nhttps://www.reddit.com/r/thanksgiving/comments/1fvxp6q/whats_a_good_pumpkin_pie_recipe/\\n\\n\",\n\u2502   'tool_id': 'call_yoatsfS2gXgqeS8HOfBcddhz'\n}\n</pre> <p>We first add some additional functionality to our <code>ChatModel</code> class so that it can interact with tools. The key section here is that we have to have the following order:</p> <pre>[\n    {\n        User message,\n        Tool completion,\n        Tool message (as above),\n    }\n]\n</pre> <p>As the below is running, we should try to trace through exactly what is happening each step of the way.</p> In\u00a0[18]: Copied! <pre>from models import SearchModel\n</pre> from models import SearchModel In\u00a0[19]: Copied! <pre>model = SearchModel('gpt-4o-mini', system_prompt=tool_system_prompt, api_key=OPENAI_API_KEY)\n</pre> model = SearchModel('gpt-4o-mini', system_prompt=tool_system_prompt, api_key=OPENAI_API_KEY) In\u00a0[20]: Copied! <pre>response = model.generate(\n    \"What is the best pumpkin pie recipe?\",\n    max_tokens=512,\n    temperature=0.5,\n    tools=tools,\n)\nprint(response.choices[0].message.content)\n</pre> response = model.generate(     \"What is the best pumpkin pie recipe?\",     max_tokens=512,     temperature=0.5,     tools=tools, ) print(response.choices[0].message.content) <pre>Searching for information...\n\nHere are some highly-rated pumpkin pie recipes:\n\n1. [The BEST Pumpkin Pie Recipe - Tastes Better From Scratch](https://tastesbetterfromscratch.com/pumpkin-pie-with-caramel-pecan-topping/)\n2. [Pumpkin Pie Recipe - Preppy Kitchen](https://preppykitchen.com/pumpkin-pie-2/)\n3. [The Great Pumpkin Pie Recipe - Sally's Baking Addiction](https://sallysbakingaddiction.com/the-great-pumpkin-pie-recipe/)\n4. [Pumpkin Pie Recipe (Perfect for Thanksgiving!) | The Kitchn](https://www.thekitchn.com/pumpkin-pie-recipe-23691122)\n5. [Homemade Fresh Pumpkin Pie Recipe - Allrecipes](https://www.allrecipes.com/recipe/13711/homemade-fresh-pumpkin-pie/)\n\nWould you like more detailed information about any specific recipe?\n</pre> <p>The reason why the response is in this format is so we can render it in markdown:</p> <p>Searching for information...</p> <p>Here are some highly-rated pumpkin pie recipes:</p> <ol> <li>The BEST Pumpkin Pie Recipe - Tastes Better From Scratch</li> <li>Pumpkin Pie Recipe - Preppy Kitchen</li> <li>The Great Pumpkin Pie Recipe - Sally's Baking Addiction</li> <li>Pumpkin Pie Recipe (Perfect for Thanksgiving!) | The Kitchn</li> <li>Homemade Fresh Pumpkin Pie Recipe - Allrecipes</li> </ol> <p>Would you like more detailed information about any specific recipe?</p> In\u00a0[21]: Copied! <pre>response = model.generate(\n    \"Yes, can you summarize the first link for me please?\",\n    max_tokens=512,\n    temperature=0.5,\n    tools=tools,\n)\nprint(response.choices[0].message.content)\n</pre> response = model.generate(     \"Yes, can you summarize the first link for me please?\",     max_tokens=512,     temperature=0.5,     tools=tools, ) print(response.choices[0].message.content) <pre>Scraping content...\n\nThe pumpkin pie recipe from Tastes Better From Scratch is a highly praised version that emphasizes simplicity and flavor. Here's a summary of the key points:\n\n### Overview\n- **Recipe Type**: Traditional pumpkin pie with an option for a caramel pecan topping.\n- **Author**: Lauren Allen.\n- **Published**: November 14, 2022.\n\n### Ingredients\n- **For the Pie**:\n  - 1 unbaked 9-inch pie crust (homemade or store-bought)\n  - 3/4 cup granulated sugar\n  - Spices: 1 teaspoon cinnamon, 1/2 teaspoon salt, 1/2 teaspoon ginger, 1/4 teaspoon cloves\n  - 2 large eggs\n  - 15 oz canned pumpkin or fresh pumpkin puree\n  - 12 oz evaporated milk\n\n- **Optional Caramel Pecan Topping**:\n  - Light brown sugar, heavy cream, corn syrup, butter, pecans, and vanilla extract.\n\n### Instructions\n1. **Preparation**: Preheat the oven to 425\u00b0F. Beat the eggs and pumpkin together in one bowl, and mix the sugar and spices in another. Combine these mixtures and stir in evaporated milk.\n2. **Baking**: Pour the mixture into the unbaked pie shell. Bake at 425\u00b0F for 15 minutes, then reduce the temperature to 350\u00b0F and bake for an additional 40-50 minutes until set. The pie is done when the center jiggles slightly but is not overly wobbly.\n3. **Cooling**: Allow the pie to cool completely on a wire rack before serving.\n\n### Tips\n- Use an unbaked crust to avoid the need for blind baking.\n- To avoid cracks, do not over-bake and allow the pie to cool slowly.\n- The pie can be made ahead of time and stored in the refrigerator or frozen.\n\n### Nutritional Information\n- Approximately 225 calories per serving.\n\nThis recipe is noted for its traditional flavors and ease of preparation, making it a great choice for Thanksgiving or any fall gathering. Would you like to know more about any specific part of the recipe?\n</pre> <p>This is actually a great summary of the ingredients and recipe.</p> <p>Note: You would probably want to separate your tools from your model, and have a separate class that manages the tools. This would allow you to easily swap out tools, and to manage the tools in a more modular way.</p> <p>We can also pick a new link, not found by the model, and ask it to scrape that link instead:</p> In\u00a0[27]: Copied! <pre>new_input = (\n    \"That's great thanks, but could you also provide me a summary of this link please:\\n\"\n    \"https://www.inspiredtaste.net/24962/pumpkin-pie-recipe/\"\n             )\n\nresponse = model.generate(\n    new_input,\n    max_tokens=512,\n    temperature=0.5,\n    tools=tools,\n)\nprint(response.choices[0].message.content)\n</pre> new_input = (     \"That's great thanks, but could you also provide me a summary of this link please:\\n\"     \"https://www.inspiredtaste.net/24962/pumpkin-pie-recipe/\"              )  response = model.generate(     new_input,     max_tokens=512,     temperature=0.5,     tools=tools, ) print(response.choices[0].message.content) <pre>Scraping content...\n\nThe pumpkin pie recipe from Inspired Taste is celebrated for its simplicity and rich flavor. Here\u2019s a summary of the key points:\n\n### Overview:\nThis homemade pumpkin pie is praised for its easy preparation and delicious filling, which is made with heavy cream instead of sweetened condensed milk. The recipe allows for both canned and homemade pumpkin puree, making it versatile.\n\n### Key Ingredients:\n- **Pie Crust**: You can use a homemade or store-bought crust; a butter pie crust is recommended.\n- **Pumpkin**: Either canned (recommended brand: Libby\u2019s) or homemade pumpkin puree can be used.\n- **Eggs**: Three large eggs help set the filling and add richness.\n- **Sugars**: A combination of granulated and light brown sugar is used for a balanced sweetness.\n- **Cream**: Heavy cream is used for a smooth and creamy filling.\n- **Spices**: The pie features a blend of vanilla, cinnamon, ginger, ground cloves, and salt, with an option to adjust spice levels to taste.\n\n### Preparation Steps:\n1. **Prepare the Crust**: Roll out the dough to fit a 9-inch pie dish and refrigerate it while preparing the filling.\n2. **Make the Filling**: Whisk together eggs, sugars, pumpkin puree, cream, and spices until well blended.\n3. **Bake the Pie**: \n   - Preheat the oven to 425\u00b0F and bake for 15 minutes.\n   - Lower the temperature to 375\u00b0F and bake for an additional 35-45 minutes until set.\n4. **Cool**: Allow the pie to cool completely at room temperature, and refrigerate overnight for best texture.\n\n### Tips:\n- The pie is best served chilled, and its flavor improves after a night in the fridge.\n- It can be stored in the refrigerator for up to three days and can also be frozen for up to three months.\n- The recipe notes that the pie should jiggle slightly when done, indicating it will set as it cools.\n\n### Conclusion:\nThis pumpkin pie recipe is ideal for Thanksgiving and is noted for its delicious taste and ease of preparation, making it a favorite among home cooks.\n\nWould you like to know more about any specific aspect of this recipe?\n</pre> In\u00a0[28]: Copied! <pre>response = model.generate(\n    \"Of these two options, which one would you recommend?\",\n    max_tokens=512,\n    temperature=0.5,\n    tools=tools,\n)\nprint(response.choices[0].message.content)\n</pre> response = model.generate(     \"Of these two options, which one would you recommend?\",     max_tokens=512,     temperature=0.5,     tools=tools, ) print(response.choices[0].message.content) <pre>Both pumpkin pie recipes have their unique strengths, so the best choice depends on your preferences:\n\n1. **Tastes Better From Scratch**:\n   - **Pros**: This recipe includes a caramel pecan topping, adding a rich and indulgent twist to the classic pumpkin pie. It also uses a straightforward filling with a good balance of spices.\n   - **Cons**: The additional topping requires extra ingredients and steps, which may not be ideal if you're looking for a simpler recipe.\n\n2. **Inspired Taste**:\n   - **Pros**: This recipe is praised for its simplicity and focuses on the traditional pumpkin pie flavor using heavy cream for a rich texture. It's easy to prepare and can be made with either canned or homemade pumpkin.\n   - **Cons**: It lacks the additional topping, which might make it feel less festive compared to the first recipe.\n\n### Recommendation:\n- If you're looking for a classic, straightforward pumpkin pie with great flavor and texture, **go with the Inspired Taste recipe**.\n- If you want to impress with a unique twist and enjoy the combination of caramel and pecans with pumpkin, **choose the Tastes Better From Scratch recipe**.\n\nUltimately, it depends on whether you prefer a traditional pie or one with a special topping. Let me know if you need any more help deciding!\n</pre>"},{"location":"tools/tools/#tool-use","title":"Tool use\u00b6","text":""},{"location":"tools/tools/#basic-function-calling","title":"Basic function calling\u00b6","text":""},{"location":"tools/tools/#internet-search","title":"Internet search\u00b6","text":""}]}